<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/bigdata-lecture/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>로지스틱 회귀 알고리즘 | TensorFlow로 시작하는 머신러닝 &amp; 딥러닝</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="로지스틱 회귀 알고리즘" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="가장 기초적인 분류 알고리즘인 로지스틱회귀에 대해 배워봅시다." />
<meta property="og:description" content="가장 기초적인 분류 알고리즘인 로지스틱회귀에 대해 배워봅시다." />
<link rel="canonical" href="https://gusdnd852.github.io/bigdata-lecture/logistic_regression" />
<meta property="og:url" content="https://gusdnd852.github.io/bigdata-lecture/logistic_regression" />
<meta property="og:site_name" content="TensorFlow로 시작하는 머신러닝 &amp; 딥러닝" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-20T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gusdnd852.github.io/bigdata-lecture/logistic_regression","@type":"BlogPosting","headline":"로지스틱 회귀 알고리즘","dateModified":"2020-07-20T00:00:00-05:00","datePublished":"2020-07-20T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gusdnd852.github.io/bigdata-lecture/logistic_regression"},"description":"가장 기초적인 분류 알고리즘인 로지스틱회귀에 대해 배워봅시다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="/bigdata-lecture/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://gusdnd852.github.io/bigdata-lecture/feed.xml" title="TensorFlow로 시작하는 머신러닝 & 딥러닝" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>

     @media screen and (max-width : 511px) {
         body { 
		font-size: 0.84rem; 
	}pre, code, blockquote {
		font-size: 0.84rem !important;
	}
	.katex, .mord {
		font: normal 1.02em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.90em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 0.9rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}
	.mobile_hide{
		display: none;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 

     @media screen and (min-width : 512px) and (max-width : 767px) {
         body { 
		font-size: 0.92rem; 
	}pre, code, blockquote {
		font-size: 0.92rem !important;
	}
	.katex, .mord {
		font: normal 1.04em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.92em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.0rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}    
	.mobile_hide{
		display: none !important;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 
     @media screen and (min-width : 768px) and (max-width : 1024px) {
         body {
		 font-size: 1.0rem; 
	}pre, code, blockquote {
		font-size: 1.0rem !important;
	}
	.katex, .mord {
		font: normal 1.05em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.93em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.1rem !important;
	}
	.contents-right, .contents-left {
		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}  
	.contents-left{
		margin-left: 20px !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

     @media screen and (min-width : 1025px) {
         body { 
		font-size: 1.08rem; 
	}pre, code, blockquote {
		font-size: 1.08rem !important;
	}
     	.katex, .mord {
		font: normal 1.06em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.94em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.2rem !important;
	}
	.contents-right, .contents-left {
    		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}
	.contents-right > h2, .contents-left > h2 {
    		margin-botton: 2px
	}  
	.contents-left{
    		margin-left: 0 !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

    </style>
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" style="font-size:1.2rem;" href="/bigdata-lecture/">TensorFlow로 시작하는 머신러닝 &amp; 딥러닝 </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bigdata-lecture/lecture">Lecture</a><a class="page-link" href="/bigdata-lecture/orientation">Orientation</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content" style="flex: none;" >
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h3 class="post-title p-name" itemprop="name headline">05. 로지스틱 회귀 알고리즘</h3><p class="page-description">가장 기초적인 분류 알고리즘인 로지스틱회귀에 대해 배워봅시다.</p
      <i class="fas fa-tags category-tags-icon"></i><p class="category-tags"> 
      
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
	
	<div class="px-1">
    <a href="https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/03_05_로지스틱_회귀.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bigdata-lecture/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
	
	<div class="px-1">
<a href="https://github.com/gusdnd852/bigdata-lecture/tree/master/_notebooks/03_05_로지스틱_회귀.ipynb" role="button">
    <img class="notebook-badge-image" src="https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d">
</a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-로지스틱-회귀(logistic-regression)-알고리즘-이란?">1. 로지스틱 회귀(logistic regression) 알고리즘 이란? </a></li>
<li class="toc-entry toc-h3"><a href="#2.-회귀를-해봤으니,-이제-분류를-해보자">2. 회귀를 해봤으니, 이제 분류를 해보자 </a></li>
<li class="toc-entry toc-h3"><a href="#3.-Hello!-시그모이드(Sigmoid)">3. Hello! 시그모이드(Sigmoid) </a></li>
<li class="toc-entry toc-h3"><a href="#4.-Cross-Entropy-Loss-함수">4. Cross Entropy Loss 함수 </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/03_05_로지스틱_회귀.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>가장 기초적인 머신러닝 분류 알고리즘인 로지스틱 회귀에 대해 배워보고, 실습을 진행해보겠습니다.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-로지스틱-회귀(logistic-regression)-알고리즘-이란?">
<a class="anchor" href="#1.-%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80(logistic-regression)-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. 로지스틱 회귀(logistic regression) 알고리즘 이란?<a class="anchor-link" href="#1.-%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80(logistic-regression)-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%EB%9E%80?"> </a>
</h3>
<p><img src="https://t1.daumcdn.net/cfile/tistory/99F325485C7B76BC2B" alt=""></p>
<p>이번에 알려드릴 로지스틱 회귀(logistic regression) 알고리즘은 선형 회귀 다음으로 간단한 분류, 회귀 알고리즘입니다. 로지스틱 회귀 알고리즘은 데이터 샘플에 맞는 최적의 로지스틱 함수를 구하고 이를 통해 (데이터 특성으로) 예측값을 추출하는 알고리즘입니다. 선형 회귀와 함수 모양이 다를 뿐 원리는 비슷하다고 볼 수 있습니다. <strong>이름은 회귀이지만, 본 강의에서는 분류 알고리즘으로 설명</strong>하겠습니다. (원래는 오직 분류목적으로 쓰이는 것이 아니라서 이름이 그런것입니다.)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="2.-회귀를-해봤으니,-이제-분류를-해보자">
<a class="anchor" href="#2.-%ED%9A%8C%EA%B7%80%EB%A5%BC-%ED%95%B4%EB%B4%A4%EC%9C%BC%EB%8B%88,-%EC%9D%B4%EC%A0%9C-%EB%B6%84%EB%A5%98%EB%A5%BC-%ED%95%B4%EB%B3%B4%EC%9E%90" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. 회귀를 해봤으니, 이제 분류를 해보자<a class="anchor-link" href="#2.-%ED%9A%8C%EA%B7%80%EB%A5%BC-%ED%95%B4%EB%B4%A4%EC%9C%BC%EB%8B%88,-%EC%9D%B4%EC%A0%9C-%EB%B6%84%EB%A5%98%EB%A5%BC-%ED%95%B4%EB%B3%B4%EC%9E%90"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/15.png?raw=true" alt=""></p>
<p>이전 시간에 선형회귀에 대해 배웠으니 이번엔 분류를 해봅시다. 선형회귀 모델의 출력은 연속형입니다. 선형회귀의 출력값이 전체의 절반을 넘었으면 그냥 반을림해서 출력을 1로, 절반을 못넘겼으면 출력을 0으로 생각해서 분류 할 수도 있어보입니다. 예를 들자면, 위 그림(x:공부시간, y:합격여부)처럼 시험공부를 1시간, 2시간, 3시간한 친구들은 시험에서 떨어졌고, 시험공부를 4시간, 5시간, 6시간 한 친구들은 시험에 붙었다고 합시다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/16.png?raw=true" alt=""></p>
<p>그러면 이전에 했던것과 비슷하게 선형회귀 모델을 사용해서 직선을 하나 찾고, 직선의 중간쯤을 넘어가면 불합격, 직선의 중간쯤을 넘어가면 합격이라고 예상할 수도 있지 않을까요? 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/17.png?raw=true" alt=""></p>
<p>뭔가 그럴싸 해보입니다. 출력이 0.5를 넘어가면 1(합격), 0.5를 넘지 못하면 0(불합격)이라고 생각할 수 있을 것 같습니다. 그러나 여기에는 커다란 문제가 있습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/18.png?raw=true" alt=""></p>
<p>만약 빨간 네모칸에 있는 데이터가 입력되었다고 해봅시다. 이 학생은 공부를 상당히 오래했기 때문에 x축의 가장 끝쪽에 위치합니다. 하지만 여기에서 문제가 발생합니다. 이 데이터가 들어오면 선형회귀 직선이 크게 영향을 받게 됩니다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/20.png?raw=true" alt=""></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/19.png?raw=true" alt=""></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/21.png?raw=true" alt=""></p>
<p>선형회귀 모델은 데이터($y$)와 직선($\hat{y}$)간의 차이의 <strong>평균</strong>을 최소화 하게 학습했습니다. 때문에 가장 끝쪽에 들어온 데이터는 직선과 차이가 크기 때문에 직선이 직접 옆으로 움직여서 이 데이터와 차이를 좁히려고 할 것입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/22.png?raw=true" alt=""></p>
<p>직선이 변했기 때문에, 함수값이 0.5를 지나는 구간도 변경되었습니다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/23.png?raw=true" alt=""></p>
<p>함수값이 0.5를 지나는 구간이 변경되었기 때문에 이에 다른 데이터들의 정답 여부가 바뀝니다.
빨간색 동그라미 친 데이터는 원래 정답으로 맞췄지만 기준선이 지나치게 오른쪽으로 움직여버려서
오답처리되었습니다. 즉, 선형회귀의 경우 기본적으로 회귀모델이기 때문에 분류에는 적합하지 않습니다.
그러면 어떻게 해야할까요? 우리는 <strong>선형회귀 모델의 출력에 베이즈 정리를 적용</strong>하므로써 
같은 모델을 분류모델로 탈바꿈 시킬 수 있습니다.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Hello!-시그모이드(Sigmoid)">
<a class="anchor" href="#3.-Hello!-%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C(Sigmoid)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Hello! 시그모이드(Sigmoid)<a class="anchor-link" href="#3.-Hello!-%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C(Sigmoid)"> </a>
</h3>
<p><img src="https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&amp;fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F275BAD4F577B669920" alt=""></p>
<p>통계시간에 시그모이드 함수는 곧 베이즈 정리라고 배웠습니다. 베이즈 정리는 $P(D|M) = \frac{P(M|D) P(D)}{P(M)}$입니다. 즉, 모델이 주어졌을때, 해당 데이터를 맞출 확률입니다. 기존에 우리가 배웠던 회귀는 데이터를 맞추는 것이 아니라 데이터의 분포를 흉내냅니다. 그러나 분류는 데이터가 구분되는 시점을 분리내서 데이터가 무엇인지 맞춥니다. 시그모이드를 사용하면 아까와 같이 멀리 떨어진 데이터들도 잘 맞춰낼 수 있습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/24.png?raw=true" alt=""></p>
<p>기존에 사용했던 선형회귀 모델의 출력을 그대로 시그모이드 함수의 입력으로 넣으면 0 혹은 1의 값을 출력하게 되고, 이 값으로 예측을 수행하면 됩니다. 기존의 출력은 $y = wx + b$였습니다. 이 출력을 시그모이드 함수 $sigmoid(z)$의 입력으로 넣으면 y = $\frac{1}{1 + e^{-(wx + b)}}$가 되고, 이 값은 0 또는 1에 가까운 분류값이 됩니다.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Cross-Entropy-Loss-함수">
<a class="anchor" href="#4.-Cross-Entropy-Loss-%ED%95%A8%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Cross Entropy Loss 함수<a class="anchor-link" href="#4.-Cross-Entropy-Loss-%ED%95%A8%EC%88%98"> </a>
</h3>
<p>선형회귀의 경우는 MSE(Mean Squared Error)라는 Loss 함수를 사용했습니다. MSE는 직선과 데이터의 차이를 제곱한 값의 평균, 즉 분산을 최소화시키는 방향으로 학습했습니다. 그런데 분류모델을 과연 MSE로 학습해도 될까요? 
<br><br></p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*_P2Y7wi_B7o8MERuabnE9Q@2x.png" alt=""></p>
<p>위 그림을 봅시다. 우리가 이전에 했던 Regression(회귀)의 경우에는 직선이 데이터를 따라가야합니다. 따라서 직선과 데이터 사이의 차이가 줄어들어야합니다. 따라서 MSE는 이런 경우 꽤나 괜찮은 Loss함수처럼 보입니다. 그런데 분류의 경우는 두 데이터의 사이를 가로질러야합니다. 위 그림만 봐도 직선의 방향이 반대 방향인데 만약 Classification(분류)를 위한 Loss함수로 MSE를 쓰면 아래처럼 수행될 것입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/25.png?raw=true" alt=""></p>
<p>인공지능이 이렇게 작업을 수행하면 느낌이 맞는 것이 아니라 깡통을 한대 맞아야합니다. 위 그림을 보면 알 수 있듯이 MSE를 분류작업에 쓰게 되면 전혀 분류작업을 할 수 없습니다. 따라서 여기에서는 이전에 정보이론 시간에 배웠던 Cross Entropy를 Loss함수로 사용하여 KLD를 최소화시키는 방향으로 모델을 학습시킵니다. (이전 강의 때 KLD의 예시를 설명할 때는 간략하게 설명하기 위해서 Log부분을 자세하게 설명하지 않았지만, 원래 Entropy와 CrossEntropy는 Log함수를 기반으로 되어있습니다.)
<br><br></p>
$$
CrossEntropyLoss =  - \sum_{i}^{N} P_i \cdot \log Q_i
$$<p><br></p>
<p>정보이론 시간에 위와 같이 CrossEntropy를 정의했습니다. $Q_i$는 모델의 예측분포를 의미합니다.
그런데 로지스틱 회귀의 경우 2개의 클래스를 분류하는 이진분류 모델입니다.
이진분류란 2개의 클래스 중 하나로 분류하는 작업을 말합니다.</p>
$$
CrossEntropyLoss =  - \sum_{i}^{2} P_i \cdot \log Q_i
$$<p><br><br>
로지스틱 회귀는 이진분류 모델이기 때문에 $\sum_{i} P_i = P_i + (1 - P_i)$로 쓸 수 있고, $\sum_{i} Q_i = Q_i + (1 - Q_i)$로 쓸 수 있습니다. 이게 무슨말이냐면 확률은 모두 합쳐서 1이여야 하는데 이진분류라면 한쪽이 일어날 확률 $P_i$를 알면 나머지는 무조건 $1 - P_i$이 된다는 것입니다. 따라서 수식을 아래처럼 변경할 수 있습니다. (e.g. 이진분류에서 한쪽의 확률이 0.7이라면 다른쪽은 반드시 0.3임) 
<br><br></p>
$$
CrossEntropyLoss =  [- P_i \cdot \log Q_i] + [-(1-P_j) \cdot \log (1-Q_j))]
$$<p><br><br></p>
<p>로지스틱 모델의 예측분포 $Q_i$는 $sigmoid(wx + b)$이고, $y=0$일땐 앞쪽 term을 계산하고, $y=1$일땐, 뒷쪽 term을 계산하게 됩니다. 따라서 $P_i$와 $Q_i$를 대체해서 아래같이 적을 수 있습니다.
<br><br></p>
$$
CrossEntropyLoss =
\begin{cases}
-log(sigmoid(wx+b)), &amp; \mbox{if }\mbox{y = 1} \\
-log(1-sigmoid(wx+b)), &amp; \mbox{if }\mbox{y = 0}
\end{cases}
$$<p><br><br></p>
<p>지금까지 로지스틱 회귀모델에 대한 이론적인 배경에 대해 알게 되었습니다.
이제 아래 예제와 함께 분류를 수행해봅시다.</p>

</div>
</div>
</div>
</div>





    <br><br>
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = “https://gusdnd852.github.io/logistic_regression“;
this.page.identifier = 05. 로지스틱 회귀 알고리즘;
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://gusdnd852.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<a class="u-url" href="/bigdata-lecture/logistic_regression" hidden></a>
</article>

<script>
	function run_exec(){
		if("colab" === "binder"){
			if(confirm("이 노트북은 CPU환경인 Binder에서 소스코드를 실행합니다.")){
				var binder_path = "https://mybinder.org/v2/gh/gusdnd852/bigdata-lecture/master?filepath=_notebooks%2F03_05_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.ipynb";
				alert("Binder는 Docker기반의 컨테이너를 직접 빌드하기 때문에 시간이 다소 소요됩니다. Build logs의 show버튼을 눌러서 진행상황을 확인하세요");
				window.open(binder_path, "_blank");
							
			}
			
		}else if("colab" === "colab"){
			if(confirm("이 노트북은 GPU환경인 Google Colab에서 소스코드를 실행합니다.")){
				var colab_path = "https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/03_05_로지스틱_회귀.ipynb";
				alert("GPU 설정을 위해 Colab 상단 메뉴중, 런타임 → 런타임 유형변경에서 GPU를 선택해주세요. (미선택시 CPU에서 실행됨)");
				window.open(colab_path, "_blank");
			}			
		}else{
			alert("해당 실행환경은 구동이 불가능합니다. Binder와 Google Colab 중 한가지를 선택해주세요.");
		}
	}

    function code_listener() {
	var input_area = document.getElementsByClassName("input_area");

	for(var i = 0 ; i < input_area.length; i++){
		input_area[i].onclick = function(){
			run_exec();
    		}
	}

    } 

    code_listener();
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bigdata-lecture/"></data>

  <div class="wrapper">
      <div class="footer-col">
	<footer-elem class=px-2>
		<a href="https://github.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#github"></use>
			</svg>
			<span class="username">Github</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://youtube.com/channel/UCb7DJAuj1LulbY8WEr2EDUw">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#youtube"></use>
			</svg>
			<span class="username">Youtube</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://www.facebook.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#facebook"></use>
			</svg>
			<span class="username">Facebook</span>
		</a>
	</footer-elem>
	<br>
	<div class="copyright">
		© Copyright 2020 Hyunwoong Go. All rights reserved.
	</div>
      </div>
  </div>
</footer>
</body>
</html>
