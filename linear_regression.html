<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/bigdata-lecture/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>선형 회귀 알고리즘 | TensorFlow로 시작하는 머신러닝 &amp; 딥러닝</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="선형 회귀 알고리즘" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워봅시다." />
<meta property="og:description" content="가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워봅시다." />
<link rel="canonical" href="https://gusdnd852.github.io/bigdata-lecture/linear_regression" />
<meta property="og:url" content="https://gusdnd852.github.io/bigdata-lecture/linear_regression" />
<meta property="og:site_name" content="TensorFlow로 시작하는 머신러닝 &amp; 딥러닝" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-19T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gusdnd852.github.io/bigdata-lecture/linear_regression","@type":"BlogPosting","headline":"선형 회귀 알고리즘","dateModified":"2020-07-19T00:00:00-05:00","datePublished":"2020-07-19T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gusdnd852.github.io/bigdata-lecture/linear_regression"},"description":"가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워봅시다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="/bigdata-lecture/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://gusdnd852.github.io/bigdata-lecture/feed.xml" title="TensorFlow로 시작하는 머신러닝 & 딥러닝" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>

     @media screen and (max-width : 511px) {
         body { 
		font-size: 0.84rem; 
	}pre, code, blockquote {
		font-size: 0.84rem !important;
	}
	.katex, .mord {
		font: normal 1.02em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.90em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 0.9rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}
	.mobile_hide{
		display: none;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 

     @media screen and (min-width : 512px) and (max-width : 767px) {
         body { 
		font-size: 0.92rem; 
	}pre, code, blockquote {
		font-size: 0.92rem !important;
	}
	.katex, .mord {
		font: normal 1.04em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.92em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.0rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}    
	.mobile_hide{
		display: none !important;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 
     @media screen and (min-width : 768px) and (max-width : 1024px) {
         body {
		 font-size: 1.0rem; 
	}pre, code, blockquote {
		font-size: 1.0rem !important;
	}
	.katex, .mord {
		font: normal 1.05em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.93em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.1rem !important;
	}
	.contents-right, .contents-left {
		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}  
	.contents-left{
		margin-left: 20px !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

     @media screen and (min-width : 1025px) {
         body { 
		font-size: 1.08rem; 
	}pre, code, blockquote {
		font-size: 1.08rem !important;
	}
     	.katex, .mord {
		font: normal 1.06em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.94em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.2rem !important;
	}
	.contents-right, .contents-left {
    		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}
	.contents-right > h2, .contents-left > h2 {
    		margin-botton: 2px
	}  
	.contents-left{
    		margin-left: 0 !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

    </style>
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" style="font-size:1.2rem;" href="/bigdata-lecture/">TensorFlow로 시작하는 머신러닝 &amp; 딥러닝 </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bigdata-lecture/lecture">Lecture</a><a class="page-link" href="/bigdata-lecture/orientation">Orientation</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content" style="flex: none;" >
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h3 class="post-title p-name" itemprop="name headline">04. 선형 회귀 알고리즘</h3><p class="page-description">가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워봅시다.</p
      <i class="fas fa-tags category-tags-icon"></i><p class="category-tags"> 
      
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
	
	<div class="px-1">
    <a href="https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/03_04_선형_회귀.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bigdata-lecture/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
	
	<div class="px-1">
<a href="https://github.com/gusdnd852/bigdata-lecture/tree/master/_notebooks/03_04_선형_회귀.ipynb" role="button">
    <img class="notebook-badge-image" src="https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d">
</a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-선형-회귀(linear-regression)-알고리즘-이란?">1. 선형 회귀(linear regression) 알고리즘 이란? </a></li>
<li class="toc-entry toc-h3"><a href="#2.-오차-함수-(Loss-Function)-정의하기">2. 오차 함수 (Loss Function) 정의하기 </a></li>
<li class="toc-entry toc-h3"><a href="#3.-학습-알고리즘-:-경사-하강-알고리즘-(Gradient-Descent)">3. 학습 알고리즘 : 경사 하강 알고리즘 (Gradient Descent) </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/03_04_선형_회귀.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워보고, 실습을 진행해보겠습니다.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-선형-회귀(linear-regression)-알고리즘-이란?">
<a class="anchor" href="#1.-%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80(linear-regression)-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. 선형 회귀(linear regression) 알고리즘 이란?<a class="anchor-link" href="#1.-%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80(linear-regression)-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%EB%9E%80?"> </a>
</h3>
<p><img src="https://mlfromscratch.com/content/images/2020/01/linearRegression2-3.png" alt=""></p>
<p>이번에 알려드릴 선형 회귀(linear regression) 알고리즘은 머신 러닝에 기초가 되는 <strong>회귀 알고리즘</strong>입니다. 선형 회귀 알고리즘은 데이터 샘플에 맞는 <strong>최적의 직선</strong>을 구하고 이를 통해 예측값을 산출하는 회귀 알고리즘입니다. 회귀알고리즘이기 때문에 정답열의 데이터가 연속형일때 사용합니다.
<br><br></p>
<p>그러나 모든 경우에 사용할 수는 없고, 선형 회귀는 <strong>"우리가 예측하고자 하는 데이터는 선형적인 특성을 갖고 있다. 즉, $y = wx + b$에 적합하다."</strong> 라는 가정이 있을 때만 사용할 수 있습니다. (그렇지 않은 경우 좋은 성능을 얻기 어렵습니다.)
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-오차-함수-(Loss-Function)-정의하기">
<a class="anchor" href="#2.-%EC%98%A4%EC%B0%A8-%ED%95%A8%EC%88%98-(Loss-Function)-%EC%A0%95%EC%9D%98%ED%95%98%EA%B8%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. 오차 함수 (Loss Function) 정의하기<a class="anchor-link" href="#2.-%EC%98%A4%EC%B0%A8-%ED%95%A8%EC%88%98-(Loss-Function)-%EC%A0%95%EC%9D%98%ED%95%98%EA%B8%B0"> </a>
</h3>
<p>오차를 구하는 방법을 함께 생각해보면서 오차함수라는 것을 우리가 직접 만들어봅시다. 먼저 직선 $y = wx + b$의 $w$와 $b$는 랜덤하게 초기화 되어있습니다. 그렇다면 아래처럼 형편없는 직선이 만들어질 것입니다. 아래 직선은 데이터를 잘 설명한다고 보기 어렵습니다. (너무 낮게 위치하고 있음)</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/01.jpg?raw=true" alt=""></p>
<p>이 상태에서 오차를 측정해봅시다. 오차라는 건 무엇일까요? 매우 간단하게 생각해서 오차는 아래처럼 우리의 예측 직선 $\hat{y} = wx + b$ 과 실제 데이터 $y$ 사이의 차이라고 생각할 수 있습니다. (앞으로 정답은 $y$, 예측은 $\hat{y}$라고 하겠습니다)</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/02.jpg?raw=true" alt=""></p>
<p>우리는 데이터셋에 $x$좌표를 가지고 있습니다. 이 $x$좌표를 우리가 현재 추정하고 있는 직선인 $\hat{y} = wx + b$에 대입하면 녹색 직선의 $\hat{y}$좌표가 나옵니다. 그리고 데이터셋에 있는 실제 $y$좌표(파란 점)와 비교하여 차이를 계산할 수 있습니다. <br><br></p>
<p>그런데 생각해보면, 오차의 값은 1개가 아닙니다. 만약 데이터 샘플이 100개가 있으면 100개의 빨간 선($\hat{y} - y$)이 있을 것입니다. 때문에 전체적인 오차를 평가하기 위해 이 오차의 평균을 구합니다. 데이터 샘플의 수가 $N$개 있다면 평균 오차는 $\frac{1}{N}\sum_{i = 1}^N (\hat{y_i}-y_i)$가 됩니다.</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/03.jpg?raw=true" alt=""></p>
<p>음.. 꽤나 괜찮아보입니다. 그런데 아직 몇가지 문제가 있습니다. 몇몇 오차 점들은 직선보다 아래에 있어서 음수 오차를 만들어냅니다. 직선보다 아래에 있는 점들도 오차이기 때문에 오차로서 더해져야하는데, 이러면 오히려 평균 오차를 줄여주게 되기 때문에 학습에 방해가 됩니다.
<br><br></p>
<p>이 문제를 해결하기 위해 오차 $\hat{y} - y$를 제곱해서 $(\hat{y} - y)^2$로 만듭니다. 그러면 모든 음수가 사라지기 때문이죠. 그러면 이제부터는 평균 오차는 $\frac{1}{N}\sum_{i = 1}^N (\hat{y_i}-y_i)^2$가 됩니다.
<br><br></p>
<p>
$$Variance = \frac{1}{N} \sum_{i = 1}^N (x_i - mean)^2$$


$$LossFunction = \frac{1}{N}\sum_{i = 1}^N (\hat{y_i}-y_i)^2$$

<br></p>
<p>음... 그런데 이거 어디서 많이 본 것 같은 식 아닌가요? 맞습니다. 통계시간에 배운 분산과 동일합니다. 분산은 샘플과 평균의 차이의 제곱입니다. 오차함수는 샘플과 직선과의 차이의 제곱입니다. 즉, 오차함수라는건, 직선을 평균으로 봤을 때, 직선과 데이터들 사이의 분산을 의미합니다. 
<br><br></p>
<ul>
<li>오차함수의 진짜 학습 원리 (읽어볼 사람만 읽어보세요)</li>
</ul>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/04.jpg?raw=true" alt=""></p>
<p>표준편차는 오차(분산)의 제곱근이기 때문에, 오차가 작아지면 표준편차도 작아집니다. 표준편차가 작아지면 정규분포 형태를 띄는 모델의 확률분포가 뾰족해지면서 함수값이 높아집니다. 확률분포의 함수값은 통계시간에 배운 Likelihood를 의미합니다. 때문에 함수값이 올라간다는건 곧 모델이 예측하는 확률분포 함수의 Likelihood가 높아진다는 것이 되고 그러면 Likelihood와 비례하는 사후확률 (정확도)는 자연스레 올라가게 됩니다.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-학습-알고리즘-:-경사-하강-알고리즘-(Gradient-Descent)">
<a class="anchor" href="#3.-%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-:-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-(Gradient-Descent)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. 학습 알고리즘 : 경사 하강 알고리즘 (Gradient Descent)<a class="anchor-link" href="#3.-%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-:-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-(Gradient-Descent)"> </a>
</h3>
<p>이제 오차함수를 정의했습니다. 오차함수는 직선과 데이터 사이의 분산이였습니다. 그러면 이 분산을 어떻게 줄일 수 있을까요?</p>
<p><br>

$$LossFunction = \frac{1}{N}\sum_{i = 1}^N (\hat{y_i}-y_i)^2$$

<br></p>
<p>오차는 위와 같이 계산됩니다. 우리가 찾고자 하는 것은 $w$와 $b$이므로, $LossFunction$을 $LossFunction(w, b)$처럼 함수로 표현하겠습니다. 또한, $\hat{y} = wx + b$이기 때문에 이들을 그대로 대입해서 아래처럼 다시 쓸 수 있습니다.</p>
<p><br>

$$LossFunction(w, b) = \frac{1}{N}\sum_{i = 1}^N ((wx+b)-y_i)^2$$

<br></p>
<p>미지수가 $w$와 $b$이기 때문에 그외의 것들은 모두 상수입니다. 그러면 오차함수라는 것은 $w$와 $b$의 2차함수로 설명됩니다. 식만 봐서는 와닿지 않으니 아래의 그림을 볼까요? 우리가 직접 만들어낸 오차함수는 2차함수이기 때문에 공간상에서 아래와 같은 모습을 하고 있습니다. <br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/05.jpg?raw=true" alt="">
<br><br></p>
<p>현재 랜덤으로 w와 b를 초기화 했기 때문에 현재 w와 b의 위치에서 Loss함수의 함수값이 굉장히 높습니다.</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/06.jpg?raw=true" alt="">
<br><br></p>
<p>그러나 만약 w와 b가 움직여서 저 지점에 있다면 Loss가 최소값이 되겠죠?</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/07.jpg?raw=true" alt="">
<br><br></p>
<p>실제로 경사하강법 알고리즘은 경사를 타고 쭉 내려가서 w와 b를 조정하고 Loss가 최소인곳을 찾아냅니다.</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/08.gif?raw=true" alt="">
<br><br></p>
<p>그런데 3차원에서 생각하는건 너무 복잡하고 머리아프니 2차원이라고 생각하고 수식으로 풀어봅시다.</p>

</div>
</div>
</div>
</div>





    <br><br>
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = “https://gusdnd852.github.io/linear_regression“;
this.page.identifier = 04. 선형 회귀 알고리즘;
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://gusdnd852.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<a class="u-url" href="/bigdata-lecture/linear_regression" hidden></a>
</article>

<script>
	function run_exec(){
		if("colab" === "binder"){
			if(confirm("이 노트북은 CPU환경인 Binder에서 소스코드를 실행합니다.")){
				var binder_path = "https://mybinder.org/v2/gh/gusdnd852/bigdata-lecture/master?filepath=_notebooks%2F03_04_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80.ipynb";
				alert("Binder는 Docker기반의 컨테이너를 직접 빌드하기 때문에 시간이 다소 소요됩니다. Build logs의 show버튼을 눌러서 진행상황을 확인하세요");
				window.open(binder_path, "_blank");
							
			}
			
		}else if("colab" === "colab"){
			if(confirm("이 노트북은 GPU환경인 Google Colab에서 소스코드를 실행합니다.")){
				var colab_path = "https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/03_04_선형_회귀.ipynb";
				alert("GPU 설정을 위해 Colab 상단 메뉴중, 런타임 → 런타임 유형변경에서 GPU를 선택해주세요. (미선택시 CPU에서 실행됨)");
				window.open(colab_path, "_blank");
			}			
		}else{
			alert("해당 실행환경은 구동이 불가능합니다. Binder와 Google Colab 중 한가지를 선택해주세요.");
		}
	}

    function code_listener() {
	var input_area = document.getElementsByClassName("input_area");

	for(var i = 0 ; i < input_area.length; i++){
		input_area[i].onclick = function(){
			run_exec();
    		}
	}

    } 

    code_listener();
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bigdata-lecture/"></data>

  <div class="wrapper">
      <div class="footer-col">
	<footer-elem class=px-2>
		<a href="https://github.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#github"></use>
			</svg>
			<span class="username">Github</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://youtube.com/channel/UCb7DJAuj1LulbY8WEr2EDUw">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#youtube"></use>
			</svg>
			<span class="username">Youtube</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://www.facebook.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#facebook"></use>
			</svg>
			<span class="username">Facebook</span>
		</a>
	</footer-elem>
	<br>
	<div class="copyright">
		© Copyright 2020 Hyunwoong Go. All rights reserved.
	</div>
      </div>
  </div>
</footer>
</body>
</html>
