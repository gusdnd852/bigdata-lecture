<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/bigdata-lecture/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>정보이론 기초 | TensorFlow로 시작하는 머신러닝 &amp; 딥러닝</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="정보이론 기초" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다." />
<meta property="og:description" content="머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다." />
<link rel="canonical" href="https://gusdnd852.github.io/bigdata-lecture/information_theory" />
<meta property="og:url" content="https://gusdnd852.github.io/bigdata-lecture/information_theory" />
<meta property="og:site_name" content="TensorFlow로 시작하는 머신러닝 &amp; 딥러닝" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-19T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gusdnd852.github.io/bigdata-lecture/information_theory","@type":"BlogPosting","headline":"정보이론 기초","dateModified":"2020-07-19T00:00:00-05:00","datePublished":"2020-07-19T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gusdnd852.github.io/bigdata-lecture/information_theory"},"description":"머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="/bigdata-lecture/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://gusdnd852.github.io/bigdata-lecture/feed.xml" title="TensorFlow로 시작하는 머신러닝 & 딥러닝" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>

     @media screen and (max-width : 511px) {
         body { 
		font-size: 0.84rem; 
	}pre, code, blockquote {
		font-size: 0.84rem !important;
	}
	.katex, .mord {
		font: normal 1.02em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.90em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 0.9rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}
	.mobile_hide{
		display: none;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 

     @media screen and (min-width : 512px) and (max-width : 767px) {
         body { 
		font-size: 0.92rem; 
	}pre, code, blockquote {
		font-size: 0.92rem !important;
	}
	.katex, .mord {
		font: normal 1.04em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.92em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.0rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}    
	.mobile_hide{
		display: none !important;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 
     @media screen and (min-width : 768px) and (max-width : 1024px) {
         body {
		 font-size: 1.0rem; 
	}pre, code, blockquote {
		font-size: 1.0rem !important;
	}
	.katex, .mord {
		font: normal 1.05em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.93em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.1rem !important;
	}
	.contents-right, .contents-left {
		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}  
	.contents-left{
		margin-left: 20px !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

     @media screen and (min-width : 1025px) {
         body { 
		font-size: 1.08rem; 
	}pre, code, blockquote {
		font-size: 1.08rem !important;
	}
     	.katex, .mord {
		font: normal 1.06em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.94em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.2rem !important;
	}
	.contents-right, .contents-left {
    		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}
	.contents-right > h2, .contents-left > h2 {
    		margin-botton: 2px
	}  
	.contents-left{
    		margin-left: 0 !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

    </style>
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" style="font-size:1.2rem;" href="/bigdata-lecture/">TensorFlow로 시작하는 머신러닝 &amp; 딥러닝 </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bigdata-lecture/lecture">Lecture</a><a class="page-link" href="/bigdata-lecture/orientation">Orientation</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content" style="flex: none;" >
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h3 class="post-title p-name" itemprop="name headline">04. 정보이론 기초</h3><p class="page-description">머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다.</p
      <i class="fas fa-tags category-tags-icon"></i><p class="category-tags"> 
      
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
	
	<div class="px-1">
    <a href="https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_04_정보이론_기초.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bigdata-lecture/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
	
	<div class="px-1">
<a href="https://github.com/gusdnd852/bigdata-lecture/tree/master/_notebooks/02_04_정보이론_기초.ipynb" role="button">
    <img class="notebook-badge-image" src="https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d">
</a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-엔트로피?-무질서?">1. 엔트로피? 무질서? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#1.1.-열역학-엔트로피와-정보이론-엔트로피">1.1. 열역학 엔트로피와 정보이론 엔트로피 </a></li>
<li class="toc-entry toc-h4"><a href="#1.2.-정보의-단위-:-Bit">1.2. 정보의 단위 : Bit </a></li>
<li class="toc-entry toc-h4"><a href="#1.2.-카톡-대화를-Bit로-변환하기">1.2. 카톡 대화를 Bit로 변환하기 </a></li>
<li class="toc-entry toc-h4"><a href="#1.3.-정보이론에서-엔트로피란?">1.3. 정보이론에서 엔트로피란? </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/02_04_정보이론_기초.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>머신러닝의 학습의핵심 아이디어 중 한가지인 정보이론의 Entropy, Cross Entropy, KL-Divergence 등의 개념에 대해 배우고, One-hot 인코딩 등의 몇가지 팁을 배워봅시다. 정보이론은 원래는 정보를 효율적으로 인코딩하기 위해 고안되었으나 머신러닝/딥러닝 모델을 학습시킬 때, 정답과 예측사이의 차이를 계산할 때 매우 유용한 Tool로 사용됩니다. 매우 간단한 예제로 알아봅시다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h3 id="1.-엔트로피?-무질서?">
<a class="anchor" href="#1.-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC?-%EB%AC%B4%EC%A7%88%EC%84%9C?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. 엔트로피? 무질서?<a class="anchor-link" href="#1.-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC?-%EB%AC%B4%EC%A7%88%EC%84%9C?"> </a>
</h3>
<p>보통 엔트로피라고 하면 열역학에 나오는 엔트로피를 생각합니다. 열역학에서 엔트로피는 무질서도입니다. 정보이론에서도 마찬가지로 엔트로피는 무질서라는 의미를 가지고 있긴합니다만 그 쓰임이 조금 다릅니다. 그 차이에 대해 알아봅시다.
<br><br></p>
<h4 id="1.1.-열역학-엔트로피와-정보이론-엔트로피">
<a class="anchor" href="#1.1.-%EC%97%B4%EC%97%AD%ED%95%99-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1. 열역학 엔트로피와 정보이론 엔트로피<a class="anchor-link" href="#1.1.-%EC%97%B4%EC%97%AD%ED%95%99-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC"> </a>
</h4>
<p><img src="http://study.zumst.com/upload/00-d33-00-22-05/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80%20%ED%99%95%EB%A5%A01.png" alt=""></p>
<p>잘 아시다시피 열역학에서 <strong>엔트로피</strong>는 <strong>무질서</strong>를 의미하고, 계는 시간이 지나면서 자연스레 무질서해집니다. 집에서 청소를 안하면 집이 계속 더러워지는 것 처럼요. 이 것이 바로 열역학 제 2법칙이고, 이에 의해 자연계에서 엔트로피 증가량은 항상 0보다 큽니다. 만약 엔트로피를 감소시켜서 다시 질서있게 만들려면 그만큼의 힘이 듭니다. 우리가 어지럽힐땐 힘들지 않아도 청소할때는 힘든 것 처럼 말이에요.
<br><br></p>
<p>그렇다면 이것과 정보와 과연 무슨 관련이 있을까요? <strong>정보이론</strong>에서 엔트로피의 정의는 <strong>정보량의 기댓값(평균)</strong> 을 의미합니다. 정보이론에서도 엔트로피가 높으면 정보가 많다는 것이고, 그러면 정보들이 더 많이 뒤섞이기 때문에 <strong>무질서</strong>한 것입니다. 반면에, 엔트로피가 낮으면 정보의 양이 적다는 것이고 정보들은 덜 뒤섞이기 때문에 덜 무질서 한 것입니다. 또한 정보의 양은 항상 증가하기 때문에 열역학 제 2법칙과도 어느정도 잘 맞습니다.<br><br></p>
<p>가령 단어가 4개를 말할 수 있는 아이와 단어 1000개를 말할 수 있는 어른이 있다고 합시다. 어른이 훨씬 많은 단어를 알고 있기 때문에 훨씬 다양한 대화가 가능할 것이고, 훨씬 다양한 대화를 만들어 낼 수 있습니다. 따라서 정보 입장에서 보면 어른의 대화가 훨씬 무질서하고, 아이의 대화는 매우 단순합니다. <br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.2.-정보의-단위-:-Bit">
<a class="anchor" href="#1.2.-%EC%A0%95%EB%B3%B4%EC%9D%98-%EB%8B%A8%EC%9C%84-:-Bit" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2. 정보의 단위 : Bit<a class="anchor-link" href="#1.2.-%EC%A0%95%EB%B3%B4%EC%9D%98-%EB%8B%A8%EC%9C%84-:-Bit"> </a>
</h4>
<p>그렇다면 정보의 양을 어떻게 측정할까요? 한국어와 영어와 독일어 등 모든 언어가 다른데 말이죠. 정보이론을 연구하던 학자들은 여러 언어의 말을 <strong>0 or 1의 단위인 Bit로 변환해서 언어등과 무관하게 정보의 양을 측정</strong>할 수 있었습니다. 그런데 정보를 Bit로 변환할 때 중요한 것이 있습니다. 최소한의 통신으로 최대한 많은 정보를 전송해야하기 때문에 단어를 Bit로 변환할 때, <strong>자주나오는 말은 짧게 변환하고 드물게 나오는 말은 그것보다는 길게 변환</strong>해야했습니다.<br><br></p>
<h4 id="1.2.-카톡-대화를-Bit로-변환하기">
<a class="anchor" href="#1.2.-%EC%B9%B4%ED%86%A1-%EB%8C%80%ED%99%94%EB%A5%BC-Bit%EB%A1%9C-%EB%B3%80%ED%99%98%ED%95%98%EA%B8%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2. 카톡 대화를 Bit로 변환하기<a class="anchor-link" href="#1.2.-%EC%B9%B4%ED%86%A1-%EB%8C%80%ED%99%94%EB%A5%BC-Bit%EB%A1%9C-%EB%B3%80%ED%99%98%ED%95%98%EA%B8%B0"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/118.png?raw=true" alt=""></p>
<p>흔한 연인들의 대화입니다. 대화의 절반이 하트로 이루어진 것을 볼 수 있습니다. 그러다가 실수로 친구에게 보낼 ㅗㅗ을 연인에게 보내게 되었습니다. 대화의 절반정도가 하트이기 때문에 단어를 쓸 때 하트가 나올 확률 $P(♥) = 0.5$이고, 잘못보낸 ㅗㅗ를 보낼 확률은 $P(ㅗㅗ) = 0.001$ 정도 된다고 해봅시다. <br><br></p>
<p>정보의 양을 측정하기 위해 이 두 단어(♥, ㅗㅗ)를 2진수로 변환해야 한다고 해보겠습니다. 예를 들면 최대 4비트 패턴까지 할당할 수 있는데 하는데 만약 자주 나오는 단어인 ♥에 0000을 할당하고, 자주 나오지 않는 단어인 ㅗㅗ에 00을 할당했다고 해봅시다. 그러면 단어 전체를 전송하기 위해 훨씬 많은 비트가 필요해져서 매우 비효율적입니다. 반대로 ♥에 00을 ㅗㅗ에 0000을 할당하는 편이 훨씬 적은 비트로 많은 데이터를 보낼 수 있겠죠.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.3.-정보이론에서-엔트로피란?">
<a class="anchor" href="#1.3.-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%97%90%EC%84%9C-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3. 정보이론에서 엔트로피란?<a class="anchor-link" href="#1.3.-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%97%90%EC%84%9C-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EB%9E%80?"> </a>
</h4>
<p>엔트로피는 위에서 말했듯이 정보량의 기대값입니다. 그리고 그 정보의 양은 Bit로 측정합니다. 우리는 앞선 통계시간에 기대값에 대해 배웠습니다. 기대값은 아래와 같습니다.</p>
<p><br>

$$E[x] = \sum_{i} x_i \cdot P(x_i)$$

<br></p>
<p>평균이 아니라 기대값이여야하는 이유는 <strong>각 단어가 등장할 확률이 모두 다르기 때문</strong>입니다. 만약 단어가 균등분포의 형태로 등장한다면, 평균으로 측정해도 상관이 없지만, 위의 예시처럼 어떤 단어는 자주 등장하고, 어떤 단어는 잘 등장하지 않기 때문에 단어의 등장 확률을 고려하여 기대값으로 평가하는 것이 정확합니다.
<br><br></p>
<p>그런데 이 때, 단어는 단순한 숫자 (0, 01, 010 등)이므로 의미가 없습니다. 따라서 <strong>정보의 양을 측정하기 위해서는 몇 비트가 사용되었는지로 평가</strong>해야합니다. 여기에서는 아래와 같은 그래프를 사용합니다.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/ko/thumb/7/71/Entropy_kor.jpg/330px-Entropy_kor.jpg" alt=""></p>
<p>위 그래프는 $y= -\log P(x)$ 그래프입니다. 단어가 등장할 확률(y축)이 높다면, 길이(x축)가 짧아지고 단어가 등장할 확률(y축)이 낮다면 길이(x축)이 길어집니다. 그래서 이 $-\log P(x)$의 기대값을 구할 것이기 때문에 엔트로피는 아래와 같아집니다.</p>
<p><br>

$$E[-\log P(x)] = \sum_{i} (-\log P(x_i)) \cdot P(x_i)$$

<br></p>

</div>
</div>
</div>
</div>





    <br><br>
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = “https://gusdnd852.github.io/information_theory“;
this.page.identifier = 04. 정보이론 기초;
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://gusdnd852.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<a class="u-url" href="/bigdata-lecture/information_theory" hidden></a>
</article>

<script>
	function run_exec(){
		if("colab" === "binder"){
			if(confirm("이 노트북은 CPU환경인 Binder에서 소스코드를 실행합니다.")){
				var binder_path = "https://mybinder.org/v2/gh/gusdnd852/bigdata-lecture/master?filepath=_notebooks%2F02_04_%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0_%EA%B8%B0%EC%B4%88.ipynb";
				alert("Binder는 Docker기반의 컨테이너를 직접 빌드하기 때문에 시간이 다소 소요됩니다. Build logs의 show버튼을 눌러서 진행상황을 확인하세요");
				window.open(binder_path, "_blank");
							
			}
			
		}else if("colab" === "colab"){
			if(confirm("이 노트북은 GPU환경인 Google Colab에서 소스코드를 실행합니다.")){
				var colab_path = "https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_04_정보이론_기초.ipynb";
				alert("GPU 설정을 위해 Colab 상단 메뉴중, 런타임 → 런타임 유형변경에서 GPU를 선택해주세요. (미선택시 CPU에서 실행됨)");
				window.open(colab_path, "_blank");
			}			
		}else{
			alert("해당 실행환경은 구동이 불가능합니다. Binder와 Google Colab 중 한가지를 선택해주세요.");
		}
	}

    function code_listener() {
	var input_area = document.getElementsByClassName("input_area");

	for(var i = 0 ; i < input_area.length; i++){
		input_area[i].onclick = function(){
			run_exec();
    		}
	}

    } 

    code_listener();
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bigdata-lecture/"></data>

  <div class="wrapper">
      <div class="footer-col">
	<footer-elem class=px-2>
		<a href="https://github.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#github"></use>
			</svg>
			<span class="username">Github</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://youtube.com/channel/UCb7DJAuj1LulbY8WEr2EDUw">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#youtube"></use>
			</svg>
			<span class="username">Youtube</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://www.facebook.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#facebook"></use>
			</svg>
			<span class="username">Facebook</span>
		</a>
	</footer-elem>
	<br>
	<div class="copyright">
		© Copyright 2020 Hyunwoong Go. All rights reserved.
	</div>
      </div>
  </div>
</footer>
</body>
</html>
