<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/bigdata-lecture/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>정보이론 기초 | TensorFlow로 시작하는 머신러닝 &amp; 딥러닝</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="정보이론 기초" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다." />
<meta property="og:description" content="머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다." />
<link rel="canonical" href="https://gusdnd852.github.io/bigdata-lecture/information_theory" />
<meta property="og:url" content="https://gusdnd852.github.io/bigdata-lecture/information_theory" />
<meta property="og:site_name" content="TensorFlow로 시작하는 머신러닝 &amp; 딥러닝" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-23T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gusdnd852.github.io/bigdata-lecture/information_theory","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://gusdnd852.github.io/bigdata-lecture/information_theory"},"headline":"정보이론 기초","dateModified":"2020-07-23T00:00:00-05:00","datePublished":"2020-07-23T00:00:00-05:00","description":"머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="/bigdata-lecture/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://gusdnd852.github.io/bigdata-lecture/feed.xml" title="TensorFlow로 시작하는 머신러닝 & 딥러닝" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>

     @media screen and (max-width : 511px) {
         body { 
		font-size: 0.84rem; 
	}pre, code, blockquote {
		font-size: 0.84rem !important;
	}
	.katex, .mord {
		font: normal 1.02em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.90em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 0.9rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}
	.mobile_hide{
		display: none;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 

     @media screen and (min-width : 512px) and (max-width : 767px) {
         body { 
		font-size: 0.92rem; 
	}pre, code, blockquote {
		font-size: 0.92rem !important;
	}
	.katex, .mord {
		font: normal 1.04em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.92em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.0rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}    
	.mobile_hide{
		display: none !important;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 
     @media screen and (min-width : 768px) and (max-width : 1024px) {
         body {
		 font-size: 1.0rem; 
	}pre, code, blockquote {
		font-size: 1.0rem !important;
	}
	.katex, .mord {
		font: normal 1.05em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.93em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.1rem !important;
	}
	.contents-right, .contents-left {
		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}  
	.contents-left{
		margin-left: 20px !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

     @media screen and (min-width : 1025px) {
         body { 
		font-size: 1.08rem; 
	}pre, code, blockquote {
		font-size: 1.08rem !important;
	}
     	.katex, .mord {
		font: normal 1.06em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.94em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.2rem !important;
	}
	.contents-right, .contents-left {
    		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}
	.contents-right > h2, .contents-left > h2 {
    		margin-botton: 2px
	}  
	.contents-left{
    		margin-left: 0 !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

    </style>
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" style="font-size:1.2rem;" href="/bigdata-lecture/">TensorFlow로 시작하는 머신러닝 &amp; 딥러닝 </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bigdata-lecture/lecture">Lecture</a><a class="page-link" href="/bigdata-lecture/orientation">Orientation</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content" style="flex: none;" >
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h3 class="post-title p-name" itemprop="name headline">04. 정보이론 기초</h3><p class="page-description">머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다.</p
      <i class="fas fa-tags category-tags-icon"></i><p class="category-tags"> 
      
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
	
	<div class="px-1">
    <a href="https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_04_정보이론_기초.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bigdata-lecture/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
	
	<div class="px-1">
<a href="https://github.com/gusdnd852/bigdata-lecture/tree/master/_notebooks/02_04_정보이론_기초.ipynb" role="button">
    <img class="notebook-badge-image" src="https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d">
</a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-엔트로피?-무질서?">1. 엔트로피? 무질서? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#1.1.-열역학-엔트로피와-정보이론-엔트로피">1.1. 열역학 엔트로피와 정보이론 엔트로피 </a></li>
<li class="toc-entry toc-h4"><a href="#1.2.-정보의-단위-:-Bit">1.2. 정보의 단위 : Bit </a></li>
<li class="toc-entry toc-h4"><a href="#1.2.-예제-:-카톡-대화를-Bit로-변환하기">1.2. 예제 : 카톡 대화를 Bit로 변환하기 </a></li>
<li class="toc-entry toc-h4"><a href="#1.3.-정보이론에서-엔트로피란?">1.3. 정보이론에서 엔트로피란? </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#2.-Cross-Entropy">2. Cross Entropy </a></li>
<li class="toc-entry toc-h3"><a href="#3.-KL-(Kullback–Leibler)-Divergence">3. KL (Kullback–Leibler) Divergence </a></li>
<li class="toc-entry toc-h3"><a href="#4.-딥러닝에-적용하기">4. 딥러닝에 적용하기 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#4.1.-One---Hot-인코딩">4.1. One - Hot 인코딩 </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/02_04_정보이론_기초.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>머신러닝의 학습의핵심 아이디어 중 한가지인 정보이론의 Entropy, Cross Entropy, KL-Divergence 등의 개념에 대해 배우고, One-hot 인코딩 등의 몇가지 팁을 배워봅시다. 정보이론은 원래는 정보를 효율적으로 인코딩하기 위해 고안되었으나 머신러닝/딥러닝 모델을 학습시킬 때, 정답과 예측사이의 차이를 계산할 때 매우 유용한 Tool로 사용됩니다. 매우 간단한 예제로 알아봅시다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h3 id="1.-엔트로피?-무질서?">
<a class="anchor" href="#1.-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC?-%EB%AC%B4%EC%A7%88%EC%84%9C?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. 엔트로피? 무질서?<a class="anchor-link" href="#1.-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC?-%EB%AC%B4%EC%A7%88%EC%84%9C?"> </a>
</h3>
<p>보통 엔트로피라고 하면 열역학에 나오는 엔트로피를 생각합니다. 열역학에서 엔트로피는 무질서도입니다. 정보이론에서도 마찬가지로 엔트로피는 무질서라는 의미를 가지고 있긴합니다만 그 쓰임이 조금 다릅니다. 그 차이에 대해 알아봅시다.
<br><br></p>
<h4 id="1.1.-열역학-엔트로피와-정보이론-엔트로피">
<a class="anchor" href="#1.1.-%EC%97%B4%EC%97%AD%ED%95%99-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1. 열역학 엔트로피와 정보이론 엔트로피<a class="anchor-link" href="#1.1.-%EC%97%B4%EC%97%AD%ED%95%99-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC"> </a>
</h4>
<p><img src="http://study.zumst.com/upload/00-d33-00-22-05/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80%20%ED%99%95%EB%A5%A01.png" alt=""></p>
<p>잘 아시다시피 열역학에서 <strong>엔트로피</strong>는 <strong>무질서</strong>를 의미하고, 계는 시간이 지나면서 자연스레 무질서해집니다. 집에서 청소를 안하면 집이 계속 더러워지는 것 처럼요. 이 것이 바로 열역학 제 2법칙이고, 이에 의해 자연계에서 엔트로피 증가량은 항상 0보다 큽니다. 만약 엔트로피를 감소시켜서 다시 질서있게 만들려면 그만큼의 힘이 듭니다. 우리가 어지럽힐땐 힘들지 않아도 청소할때는 힘든 것 처럼 말이에요.
<br><br></p>
<p>그렇다면 이것과 정보와 과연 무슨 관련이 있을까요? <strong>정보이론</strong>에서 엔트로피의 정의는 <strong>전체 정보를 표현하는데 필요한 최소 자원량(기대값)</strong>을 의미합니다. 정보이론에서도 엔트로피가 높으면 정보가 많다는 것이고, 그러면 정보들이 더 많이 뒤섞이기 때문에 <strong>무질서</strong>한 것입니다. 반면에, 엔트로피가 낮으면 정보의 양이 적다는 것이고 정보들은 덜 뒤섞이기 때문에 덜 무질서 한 것입니다. 또한 정보의 양은 항상 증가하기 때문에 열역학 제 2법칙과도 어느정도 잘 맞습니다.<br><br></p>
<p>가령 총 4개의 단어를 말할 수 있는 아이와 총 1000개의 단어를 말할 수 있는 어른이 있다고 합시다. 어른이 훨씬 많은 단어를 알고 있기 때문에 훨씬 다양한 대화가 가능할 것이고, 훨씬 다양한 대화를 만들어 낼 수 있습니다. 따라서 정보 입장에서 보면 어른의 대화가 훨씬 무질서하고, 아이의 대화는 매우 단순합니다. <br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.2.-정보의-단위-:-Bit">
<a class="anchor" href="#1.2.-%EC%A0%95%EB%B3%B4%EC%9D%98-%EB%8B%A8%EC%9C%84-:-Bit" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2. 정보의 단위 : Bit<a class="anchor-link" href="#1.2.-%EC%A0%95%EB%B3%B4%EC%9D%98-%EB%8B%A8%EC%9C%84-:-Bit"> </a>
</h4>
<p>그렇다면 전체 정보를 표현하기 위한 최소 자원량을 어떻게 측정할까요? 한국어와 영어와 독일어 등 모든 언어가 다른데 말이죠. 정보이론을 연구하던 학자들은 여러 언어의 말을 <strong>0 or 1의 단위인 Bit로 인코딩해서 언어등과 무관하게 정보의 양을 측정</strong>할 수 있었습니다. 그런데 정보를 Bit로 변환할 때 중요한 것이 있습니다. 최소한의 통신으로 최대한 많은 정보를 전송해야하기 때문에 단어를 Bit로 인코딩할 때, <strong>자주나오는 말은 짧게 인코딩하고 드물게 나오는 말은 그것보다는 길게 인코딩</strong>해야했습니다.<br><br></p>
<h4 id="1.2.-예제-:-카톡-대화를-Bit로-변환하기">
<a class="anchor" href="#1.2.-%EC%98%88%EC%A0%9C-:-%EC%B9%B4%ED%86%A1-%EB%8C%80%ED%99%94%EB%A5%BC-Bit%EB%A1%9C-%EB%B3%80%ED%99%98%ED%95%98%EA%B8%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2. 예제 : 카톡 대화를 Bit로 변환하기<a class="anchor-link" href="#1.2.-%EC%98%88%EC%A0%9C-:-%EC%B9%B4%ED%86%A1-%EB%8C%80%ED%99%94%EB%A5%BC-Bit%EB%A1%9C-%EB%B3%80%ED%99%98%ED%95%98%EA%B8%B0"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/118.png?raw=true" alt=""></p>
<p>흔한 연인들의 대화입니다. 대화의 절반이 하트로 이루어진 것을 볼 수 있습니다. 대화의 절반정도가 하트이기 때문에 단어를 쓸 때 하트가 나올 확률 $P(♥) = 0.5$이고, ㅗ를 보내면 싸우게 되므로, 자주 안보내기 때문에 $P(ㅗ) = 0.1$ 정도 된다고 해봅시다. <br><br></p>
<p>정보의 양을 측정하기 위해 이 두 단어(♥와 ㅗ)를 2진수로 인코딩해야 한다고 해보겠습니다. 예를 들면 단어 하나를 최소 2비트 ~ 최대 4비트까지 할당할 수 있는데 하는데 만약 자주 나오는 단어인 ♥를 0000로 인코딩하고, 자주 나오지 않는 단어인 ㅗ를 00로 인코딩했다고 해봅시다. 이 커플은 ㅗ보다 ♥를 훨씬 자주보냅니다. ♥를 5배정도 많이 보내기 때문에 ♥를 4비트로 인코딩하면 보낼때 마다 4비트씩 보내야하므로 너무 데이터의 낭비가 심합니다. 그에 비해 자주 쓰이지 않는 ㅗ는 2비트에 할당되어있으니 이 둘을 바꿔서 ㅗ를 4비트로, ♥를 2비트로 인코딩해주는 것이 훨씬 효율적일 것입니다.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.3.-정보이론에서-엔트로피란?">
<a class="anchor" href="#1.3.-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%97%90%EC%84%9C-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3. 정보이론에서 엔트로피란?<a class="anchor-link" href="#1.3.-%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%97%90%EC%84%9C-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EB%9E%80?"> </a>
</h4>
<p>엔트로피는 위에서 말했듯이 <strong>전체 정보를 표현하는데 필요한 최소 자원량(기대값)</strong>입니다. 정말 모든 단어들이 최소의 자원량, 최소의 비트만큼만 할당되게 인코딩한다면 정말 좋겠죠. 
<br><br></p>
<p>단어들이 매우 많으므로 평균적으로 몇비트 정도로 인코딩 되었는지 알기 위해 기대값을 계산합니다. 물론 그 값은 적으면 적을수록 좋겠죠? (평균적으로 단어들의 비트가 짧다는 것) 우리는 앞선 통계시간에 기대값에 대해 배웠습니다. 기대값은 아래와 같습니다.</p>
<p><br>

$$E[x] = \sum_{i} x_i \cdot P(x_i)$$

<br></p>
<p>평균이 아니라 기대값이여야하는 이유는 <strong>각 단어가 등장할 확률이 모두 다르기 때문</strong>입니다. 만약 단어가 균등분포의 형태로 등장한다면, 평균으로 측정해도 상관이 없지만, 위의 예시처럼 어떤 단어는 자주 등장하고, 어떤 단어는 잘 등장하지 않기 때문에 단어의 등장 확률을 고려하여 기대값으로 평가하는 것이 정확합니다.
<br><br></p>
<p>그렇다면 <strong>전체 정보를 표현하는데 필요한 최소의 비트수(기대값)</strong>을 갖게끔 단어들을 인코딩하려면 각 단어가 얼마의 길이를 갖게끔 인코딩 해야할까요? 정보이론의 아버지인 클라우드 섀넌는 아래와 같은 그래프를 사용하면 각 단어를 이론상으로 최소 비트 수(단어 길이)로 인코딩 할 수 있다고 했습니다.</p>
<p><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/119.png?raw=true" alt=""></p>
<p>위 그래프는 $y= -\log_2 P_i$, 즉 등장 확률에 따른 비트 수의 그래프입니다. 여기에서 $P_i$는 $i$번째 단어의 등장 확률을 의미합니다. 단어가 등장할 확률(x축)이 높다면, 길이(y축)를 짧게 인코딩하고, 단어가 등장할 확률(x축)이 낮다면 길이(y축)를 길게 인코딩합니다. 우리는 단어 $x_i$가 $P_i$의 확률로 나타날 때, 비트의 길이인 $-\log_2 P_i$의 기대값을 계산할 것이기 때문에 엔트로피는 아래와 같아집니다.</p>
<p><br>

$$Entropy = \sum_{i} (-\log_2 P_i) \cdot P_i$$

<br></p>
<p>이는 기대값 $E[x] = \sum_{i} x_i \cdot P_i$에서 $x_i$를 $(-\log_2 P_i)$로 치환한 것입니다. 식을 조금 더 깔끔하게 정리해서 쓰면 아래와 동일합니다.</p>
<p><br>

$$Entropy = - \sum_{i} P_i \cdot \log_2 P_i $$

<br></p>
<p>이 것이 정보이론에서의 엔트로피에 대한 정의입니다. 즉, 엔트로피는 <strong>전체 정보를 표현하는데 필요한 최소 자원량(기대값)</strong> 을 의미하며 이론상으로 가장 최소의 비트만을 사용해서 데이터를 전송하는 인코딩 방식입니다. 이러한 엔트로피의 정의는 <strong>우리가 일정량의 데이터를 가지고 있을 때, 어떤 방식으로 인코딩한다고 해도 절대 이 것보다 적은 비트로 인코딩 할수는 없다는 것을 의미</strong>하기도 합니다. 결론적으로 다시 한번 말하자면, 일정한 만큼의 데이터가 있을 때, 각 단어를 가능한 최소의 비트만 써서 인코딩하는 길이가 $ -\log_2 P_i $라는 것입니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="2.-Cross-Entropy">
<a class="anchor" href="#2.-Cross-Entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Cross Entropy<a class="anchor-link" href="#2.-Cross-Entropy"> </a>
</h3>
<p>크로스 엔트로피란 각 단어의 인코딩 (비트)길이를 섀넌의 방식에서 우리의 방식으로 바꿨을 때의 엔트로피를 이야기합니다. 섀넌이 제안한 엔트로피는 말 그대로 <strong>이론상의 최소 비트 수</strong>입니다. 어떻게 인코딩해도 섀넌의 인코딩보다는 비효율적일 수 밖에 없습니다. 기존에 섀넌의 이론에서 제안된 엔트로피는 다음과 같습니다.</p>
<p><br>

$$Entropy = - \sum_{i} P_i \cdot \log_2 P_i$$

<br></p>
<p>여기에서 우리는 $\log_2 P_i$가 아니라 $\log_2 Q_i$를 쓰게 됩니다. ($Q_i$는 실제 단어의 등장 확률이 아니라 우리가 직접 예측한 단어 등장 확률입니다.) 위에서 말했다시피 $\log_2 P_i$를 사용하여 인코딩 하는 것은 정말 이론적으로 최상의 경우지만, 우리가 실제로 인코딩 할때는 현실적인 제약 등을 고려하여 $Q_i$로 생각하고 인코딩합니다. 그래서 우리가 실질적으로 적용하는 각 단어별 인코딩 길이는 $\log_2 Q_i$입니다. 따라서, 즉, 섀넌의 방법보다는 비효율적인, 그러나 현실적인 <strong>우리의 가설에 의해 인코딩한 뒤 계산한 Entropy가 바로 CrossEntropy</strong>입니다.</p>
<p><br>

$$CrossEntropy = - \sum_{i} P_i \cdot \log_2 Q_i$$

<br></p>
<p>이론상의 최소 비트수인 Entropy와 현실적으로 적용한 우리의 비트 할당량은 Cross Entropy. 이 두 개념 확실히 이해 가셨나요?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="3.-KL-(Kullback–Leibler)-Divergence">
<a class="anchor" href="#3.-KL-(Kullback%E2%80%93Leibler)-Divergence" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. KL (Kullback–Leibler) Divergence<a class="anchor-link" href="#3.-KL-(Kullback%E2%80%93Leibler)-Divergence"> </a>
</h3>
<p>정보이론에서 쿨백 라이블러 발산(KL-Divergence)은 크로스 엔트로피와 엔트로피의 차이입니다. 즉, <strong>우리가 직접 인코딩한 방식(크로스엔트로피)이 이론상의 최소 비트 인코딩 방식(엔트로피)에 비해 어느정도의 차이가 있는지</strong>를 나타냅니다.</p>
<p><br>

$$KLD = CrossEntropy - Entropy$$

<br>

$$KLD = - \sum_{i} P_i \cdot \log_2 Q_i - \sum_{i} P_i \cdot \log_2 P_i$$

<br>

$$KLD = - \sum_{i} P_i \cdot \log_2 \frac{Q_i}{P_i}$$

<br></p>
<p>즉, KL Divergence는 이론상 최소 비트을 할당하는 방식에 비해 우리의 방식이 얼마나 못하는지를 나타냅니다. 둘 사이의 차이가 작으면 거의 완벽하게 해낸 것이고, 둘 사이가 크면 최소 방식에 성능이 별로 좋지 못한 것입니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="4.-딥러닝에-적용하기">
<a class="anchor" href="#4.-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%97%90-%EC%A0%81%EC%9A%A9%ED%95%98%EA%B8%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. 딥러닝에 적용하기<a class="anchor-link" href="#4.-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%97%90-%EC%A0%81%EC%9A%A9%ED%95%98%EA%B8%B0"> </a>
</h3>
<p>그래서 이걸 왜 공부한걸까요? 로지스틱 회귀모형(회귀이지만 분류모델입니다)과 거의 대부분의 딥러닝 분류(Classification) 알고리즘은 현재 $KLD$를 최소화하는 방식으로 학습하고 있으며, 거의 대부분의 트리기반의 머신러닝 알고리즘은 Entropy로 정보의 양을 비교하여 트리를 설계합니다. 대부분 머신러닝 알고리즘 학습의 주축이기 때문에 소개한 것입니다. (이후 수업때 계속 써먹어야 해서 설명한거긴 합니다)<br><br></p>
<p>분류모델 학습시에 우리의 데이터에는 정답열이 있습니다. 이러한 정답열은 섀넌의 Entropy에 해당합니다. 전부 맞추면 100%의 성능을 보이겠지만 이론상으로만 가능하고 현실적으로 불가능합니다. 그에 비해 우리의 예측은 CrossEntropy입니다. 정답과는 다르지만 나름 잘 모델링해서 만든 예측입니다. 그리고 $KLD$는 정답데이터와 모델 예측간의 차이입니다. 만약 $KLD$가 크다면 모델의 예측과 실제 정답의 차이가 큰 것으로 모델을 더 강하게 학습시켜야하고, 만약 $KLD$가 작다면 모델의 예측과 실제 정답의 차이가 작은 것으로 모델을 더 약하게 학습시켜야 합니다. 실제 다양한 딥러닝 모델들은 이 $KLD$값을 목적함수로 두고 최소화 시키면서 학습을 진행합니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="4.1.-One---Hot-인코딩">
<a class="anchor" href="#4.1.-One---Hot-%EC%9D%B8%EC%BD%94%EB%94%A9" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1. One - Hot 인코딩<a class="anchor-link" href="#4.1.-One---Hot-%EC%9D%B8%EC%BD%94%EB%94%A9"> </a>
</h4>
<p>그렇다면 실질적으로 어떻게 $KLD$를 적용할까요? 분류데이터에는 구체적으로 나뉘어진 정답열이 있기 때문에 정답열에서 모델의 예측을 빼면 $KLD$가 됩니다.
이에 대한 간단한 예시를 보여드리겠습니다. (사실은 그냥 빼는 것이 아니라 log를 적용해서 빼야하지만, 여기에서는 정답 - 예측이라는 컨셉을 보여주기 위해 그냥 뺄셈으로 계산하겠습니다.) <br><br></p>
<p>만약 생체 정보를 바탕으로 질병을 나타내는 데이터가 있다고 가정합시다. 우리의 데이터셋은 아래와 같습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'혈압'</span><span class="p">,</span> <span class="s1">'몸무게'</span><span class="p">,</span> <span class="s1">'혈중 콜레스테롤 농도'</span><span class="p">,</span> <span class="s1">'질병(라벨)'</span><span class="p">],</span>

    <span class="n">data</span><span class="o">=</span><span class="p">[</span>
        <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mi">68</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="s1">'정상'</span><span class="p">],</span> 
        <span class="p">[</span><span class="mi">144</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">216</span><span class="p">,</span> <span class="s1">'당뇨'</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">122</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="s1">'정상'</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">133</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">220</span><span class="p">,</span> <span class="s1">'고혈압'</span><span class="p">],</span> 
        <span class="p">[</span><span class="mi">133</span><span class="p">,</span> <span class="mi">83</span><span class="p">,</span> <span class="mi">220</span><span class="p">,</span> <span class="s1">'당뇨'</span><span class="p">],</span> 
        <span class="p">[</span><span class="mi">118</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="s1">'정상'</span><span class="p">],</span> 
        <span class="p">[</span><span class="mi">143</span><span class="p">,</span> <span class="mi">86</span><span class="p">,</span> <span class="mi">220</span><span class="p">,</span> <span class="s1">'고혈압'</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">151</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="s1">'심장병'</span><span class="p">],</span>  
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">dataset</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>혈압</th>
      <th>몸무게</th>
      <th>혈중 콜레스테롤 농도</th>
      <th>질병(라벨)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>120</td>
      <td>68</td>
      <td>180</td>
      <td>정상</td>
    </tr>
    <tr>
      <th>1</th>
      <td>144</td>
      <td>82</td>
      <td>216</td>
      <td>당뇨</td>
    </tr>
    <tr>
      <th>2</th>
      <td>122</td>
      <td>55</td>
      <td>160</td>
      <td>정상</td>
    </tr>
    <tr>
      <th>3</th>
      <td>133</td>
      <td>82</td>
      <td>220</td>
      <td>고혈압</td>
    </tr>
    <tr>
      <th>4</th>
      <td>133</td>
      <td>83</td>
      <td>220</td>
      <td>당뇨</td>
    </tr>
    <tr>
      <th>5</th>
      <td>118</td>
      <td>58</td>
      <td>166</td>
      <td>정상</td>
    </tr>
    <tr>
      <th>6</th>
      <td>143</td>
      <td>86</td>
      <td>220</td>
      <td>고혈압</td>
    </tr>
    <tr>
      <th>7</th>
      <td>151</td>
      <td>40</td>
      <td>200</td>
      <td>심장병</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>컴퓨터는 자연어를 모릅니다. 때문에 질병(라벨)을 숫자로 맵핑해줍니다.
정상은 0번 질병, 당뇨는 1번 질병, 고혈압은 2번 질병, 심장병은 3번 질병이 됩니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'정상'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'당뇨'</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s1">'고혈압'</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s1">'심장병'</span><span class="p">:</span><span class="mi">3</span><span class="p">}</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">'질병(라벨)'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'질병(라벨)'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">label_map</span><span class="p">)</span>

<span class="n">dataset</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>혈압</th>
      <th>몸무게</th>
      <th>혈중 콜레스테롤 농도</th>
      <th>질병(라벨)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>120</td>
      <td>68</td>
      <td>180</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>144</td>
      <td>82</td>
      <td>216</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>122</td>
      <td>55</td>
      <td>160</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>133</td>
      <td>82</td>
      <td>220</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>133</td>
      <td>83</td>
      <td>220</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>118</td>
      <td>58</td>
      <td>166</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>143</td>
      <td>86</td>
      <td>220</td>
      <td>2</td>
    </tr>
    <tr>
      <th>7</th>
      <td>151</td>
      <td>40</td>
      <td>200</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'질병(라벨)'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'질병(라벨)'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이 때, 실제 라벨과 모델의 예측이 아래와 같다고 합시다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'label : '</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'predict : '</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>label :  [0 1 0 2 1 0 2 3]
predict :  [3 2 0 1 2 0 1 0]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<p>KLD는 정답과 예측의 차이입니다. 따라서 아래와 같이 계산합니다. (위에서 말한대로 간단하게 보여드리기 위해 log를 적용하지 않고 계산했습니다.)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'KLD : '</span><span class="p">,</span> <span class="n">label</span> <span class="o">-</span> <span class="n">predict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>KLD :  [-3 -1  0  1 -1  0  1  3]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<p>만약 정답을 맞췄다면 같은 라벨끼리 (e.g. 2번 질병 - 2번 질병 = 0) 뺄셈을 수행하기 때문에 결과는 0일것입니다. 그런데 <strong>문제는 오답</strong>에 있습니다.
어떤 질병은 차이가 -3이고 어떤질병은 차이가 3이고 어떤 질병은 차이가 1입니다. 이런문제는 다음과 같은 이유로 생깁니다.</p>
<ul>
<li>(정답 case) 정답 1번 / 예측 1번 : '정답(1) - 예측(1)' = 0</li>
<li>(오답 case) 정답 0번 / 예측 3번 : '정답(0) - 예측(3)' = -3</li>
<li>(오답 case) 정답 2번 / 예측 1번 : '정답(2) - 예측(1)' = 1 </li>
</ul>
<p><br></p>
<p>무언가 잘못된 것 같습니다. 차이가 3이 나는 질병이라면 더 크게 틀린것이고, 차이가 1이 나는 질병은 더 적게 틀린것일까요? 
절대 그렇지 않습니다. 예측한 오답 중에 <strong>더 많이 틀리고, 덜 틀리고 한 것</strong>은 없습니다. 
이러한 문제를 해결하기 위해서 우리는 One-Hot 인코딩 방법을 기본으로 사용합니다.
One-Hot 인코딩은 각 라벨을 다음과 같이 변경합니다.</p>
<ul>
<li>0 : [1, 0, 0, 0]</li>
<li>1 : [0, 1, 0, 0]</li>
<li>2 : [0, 0, 1, 0]</li>
<li>3 : [0, 0, 0, 1]</li>
</ul>
<p><br></p>
<p>One-Hot 인코딩을 적용하면 아래와 같은 결과를 확인할 수 있습니다.</p>
<ul>
<li>(정답 case) 정답 1번 / 예측 1번 : '[0, 1, 0, 0] - [0, 1, 0, 0]' = [0, 0, 0, 0]</li>
<li>(오답 case) 정답 0번 / 예측 3번 : '[1, 0, 0, 0] - [0, 0, 0, 1]' = [1, 0, 0, -1]</li>
<li>(오답 case) 정답 2번 / 예측 1번 : '[0, 0, 1, 0] - [0, 1, 0, 0]' = [0, -1, 1, 0]</li>
</ul>
<p><br></p>
<p>정답인 경우 결과가 [0, 0, 0, 0]이 되지만, 오답의 경우 리스트에 1과 -1이 반드시 생깁니다. 그래서 이 리스트 원소들을 모두 제곱해서 음수를 없애주면 아래와 같습니다.</p>
<ul>
<li>(오답 case) 정답 0번 / 예측 3번 : [1$^2$, 0$^2$, 0$^2$, -1$^2$] → [1, 0, 0, 1]</li>
<li>(오답 case) 정답 2번 / 예측 1번 : [0$^2$, -1$^2$, 1$^2$, 0$^2$] → [0, 1, 1, 0]</li>
</ul>
<p><br></p>
<p>그리고 나서 리스트의 원소 합을 모두 더해주면 반드시 2가 됩니다.</p>
<ul>
<li>(오답 case) 정답 0번 / 예측 3번 : [1, 0, 0, 1] → 2</li>
<li>(오답 case) 정답 2번 / 예측 1번 : [0, 1, 1, 0] → 2</li>
</ul>
<p><br></p>
<p>이런 방식으로 '정답 - 예측'을 구하면 정답인 경우 합이 0, 오답인 경우 합이 반드시 2가 나오게 되고, <strong>어떻게 틀리던지 출력값이 동일</strong>하게 됩니다.
(실제 출력은 log값을 사용하기 때문에 2가 나오진 않습니다만, 어떻게 틀리던지간에 <strong>오답이라면 동일한 출력값</strong>이 나오게 됩니다.)
Tensorflow, Sklearn 등의 유명 머신러닝, 딥러닝 패키지의 분류 모델들은 기본적으로 이러한 One hot 인코딩 기능이 자동으로 구현되어있습니다. 
따라서 사용자가 직접 인코딩 하지 않아도 무방하지만, 태스크에 따라서는 직접 One-Hot 인코딩을 해야하는 경우도 존재하니 잘 숙지하시길 바랍니다.</p>

</div>
</div>
</div>
</div>





    <br><br>
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = “https://gusdnd852.github.io/information_theory“;
this.page.identifier = 04. 정보이론 기초;
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://gusdnd852.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<a class="u-url" href="/bigdata-lecture/information_theory" hidden></a>
</article>

<script>
	function run_exec(){
		if("colab" === "binder"){
			if(confirm("이 노트북은 CPU환경인 Binder에서 소스코드를 실행합니다.")){
				var binder_path = "https://mybinder.org/v2/gh/gusdnd852/bigdata-lecture/master?filepath=_notebooks%2F02_04_%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0_%EA%B8%B0%EC%B4%88.ipynb";
				alert("Binder는 Docker기반의 컨테이너를 직접 빌드하기 때문에 시간이 다소 소요됩니다. Build logs의 show버튼을 눌러서 진행상황을 확인하세요");
				window.open(binder_path, "_blank");
							
			}
			
		}else if("colab" === "colab"){
			if(confirm("이 노트북은 GPU환경인 Google Colab에서 소스코드를 실행합니다.")){
				var colab_path = "https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_04_정보이론_기초.ipynb";
				alert("GPU 설정을 위해 Colab 상단 메뉴중, 런타임 → 런타임 유형변경에서 GPU를 선택해주세요. (미선택시 CPU에서 실행됨)");
				window.open(colab_path, "_blank");
			}			
		}else{
			alert("해당 실행환경은 구동이 불가능합니다. Binder와 Google Colab 중 한가지를 선택해주세요.");
		}
	}

    function code_listener() {
	var input_area = document.getElementsByClassName("input_area");

	for(var i = 0 ; i < input_area.length; i++){
		input_area[i].onclick = function(){
			run_exec();
    		}
	}

    } 

    code_listener();
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bigdata-lecture/"></data>

  <div class="wrapper">
      <div class="footer-col">
	<footer-elem class=px-2>
		<a href="https://github.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#github"></use>
			</svg>
			<span class="username">Github</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://youtube.com/channel/UCb7DJAuj1LulbY8WEr2EDUw">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#youtube"></use>
			</svg>
			<span class="username">Youtube</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://www.facebook.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#facebook"></use>
			</svg>
			<span class="username">Facebook</span>
		</a>
	</footer-elem>
	<br>
	<div class="copyright">
		© Copyright 2020 Hyunwoong Go. All rights reserved.
	</div>
      </div>
  </div>
</footer>
</body>
</html>
