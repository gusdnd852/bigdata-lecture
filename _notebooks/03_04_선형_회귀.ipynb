{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 선형 회귀 알고리즘\n",
    "> 가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 3]\n",
    "- permalink: /linear_regression\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워보고, 실습을 진행해보겠습니다.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 회귀(linear regression) 알고리즘 이란?\n",
    "\n",
    "![](https://mlfromscratch.com/content/images/2020/01/linearRegression2-3.png)\n",
    "\n",
    "이번에 알려드릴 선형 회귀(linear regression) 알고리즘은 머신 러닝에 기초가 되는 **회귀 알고리즘**입니다. 선형 회귀 알고리즘은 데이터 샘플에 맞는 **최적의 직선**을 구하고 이를 통해 예측값을 산출하는 회귀 알고리즘입니다. 회귀알고리즘이기 때문에 정답열의 데이터가 연속형일때 사용합니다.\n",
    "<br><br>\n",
    "\n",
    "그러나 모든 경우에 사용할 수는 없고, 선형 회귀는 **\"우리가 예측하고자 하는 데이터는 선형적인 특성을 갖고 있다. 즉, $y = wx + b$에 적합하다.\"** 라는 가정이 있을 때만 사용할 수 있습니다. (그렇지 않은 경우 좋은 성능을 얻기 어렵습니다.)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 오차 함수 (Loss Function) 정의하기\n",
    "\n",
    "오차를 구하는 방법을 함께 생각해보면서 오차함수라는 것을 우리가 직접 만들어봅시다. 먼저 직선 $y = wx + b$의 $w$와 $b$는 랜덤하게 초기화 되어있습니다. 그렇다면 아래처럼 형편없는 직선이 만들어질 것입니다. 아래 직선은 데이터를 잘 설명한다고 보기 어렵습니다. (너무 낮게 위치하고 있음)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/01.jpg?raw=true)\n",
    "\n",
    "이 상태에서 오차를 측정해봅시다. 오차라는 건 무엇일까요? 매우 간단하게 생각해서 오차는 아래처럼 우리의 예측 직선 $\\hat{y} = wx + b$ 과 실제 데이터 $y$ 사이의 차이라고 생각할 수 있습니다. (앞으로 정답은 $y$, 예측은 $\\hat{y}$라고 하겠습니다) \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/02.jpg?raw=true)\n",
    "\n",
    "우리는 데이터셋에 $x$좌표를 가지고 있습니다. 이 $x$좌표를 우리가 현재 추정하고 있는 직선인 $\\hat{y} = wx + b$에 대입하면 녹색 직선의 $\\hat{y}$좌표가 나옵니다. 그리고 데이터셋에 있는 실제 $y$좌표(파란 점)와 비교하여 차이를 계산할 수 있습니다. <br><br>\n",
    "\n",
    "그런데 생각해보면, 오차의 값은 1개가 아닙니다. 만약 데이터 샘플이 100개가 있으면 100개의 빨간 선($\\hat{y} - y$)이 있을 것입니다. 때문에 전체적인 오차를 평가하기 위해 이 오차의 평균을 구합니다. 데이터 샘플의 수가 $N$개 있다면 평균 오차는 $\\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)$가 됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/03.jpg?raw=true)\n",
    "\n",
    "음.. 꽤나 괜찮아보입니다. 그런데 아직 몇가지 문제가 있습니다. 몇몇 오차 점들은 직선보다 아래에 있어서 음수 오차를 만들어냅니다. 직선보다 아래에 있는 점들도 오차이기 때문에 오차로서 더해져야하는데, 이러면 오히려 평균 오차를 줄여주게 되기 때문에 학습에 방해가 됩니다.\n",
    "<br><br>\n",
    "\n",
    "이 문제를 해결하기 위해 오차 $\\hat{y} - y$를 제곱해서 $(\\hat{y} - y)^2$로 만듭니다. 그러면 모든 음수가 사라지기 때문이죠. 그러면 이제부터는 평균 오차는 $\\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)^2$가 됩니다.\n",
    "<br><br>\n",
    "\n",
    "$$Variance = \\frac{1}{N} \\sum_{i = 1}^N (x_i - mean)^2$$\n",
    "$$LossFunction = \\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)^2$$\n",
    "<br>\n",
    "\n",
    "음... 그런데 이거 어디서 많이 본 것 같은 식 아닌가요? 맞습니다. 통계시간에 배운 분산과 동일합니다. 분산은 샘플과 평균의 차이의 제곱입니다. 오차함수는 샘플과 직선과의 차이의 제곱입니다. 즉, 오차함수라는건, 직선을 평균으로 봤을 때, 직선과 데이터들 사이의 분산을 의미합니다. \n",
    "<br><br>\n",
    "\n",
    "- 오차함수의 진짜 학습 원리 (읽어볼 사람만 읽어보세요)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/04.jpg?raw=true)\n",
    "\n",
    "표준편차는 오차(분산)의 제곱근이기 때문에, 오차가 작아지면 표준편차도 작아집니다. 표준편차가 작아지면 정규분포 형태를 띄는 모델의 확률분포가 뾰족해지면서 함수값이 높아집니다. 확률분포의 함수값은 통계시간에 배운 Likelihood를 의미합니다. 때문에 함수값이 올라간다는건 곧 모델이 예측하는 확률분포 함수의 Likelihood가 높아진다는 것이 되고 그러면 Likelihood와 비례하는 사후확률 (정확도)는 자연스레 올라가게 됩니다.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 학습 알고리즘 : 경사 하강 알고리즘 (Gradient Descent)\n",
    "\n",
    "이제 오차함수를 정의했습니다. 오차함수는 직선과 데이터 사이의 분산이였습니다. 그러면 이 분산을 어떻게 줄일 수 있을까요? \n",
    "\n",
    "<br>\n",
    "$$LossFunction = \\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)^2$$\n",
    "<br>\n",
    "\n",
    "오차는 위와 같이 계산됩니다. 우리가 찾고자 하는 것은 $w$와 $b$이므로, $LossFunction$을 $LossFunction(w, b)$처럼 함수로 표현하겠습니다. 또한, $\\hat{y} = wx + b$이기 때문에 이들을 그대로 대입해서 아래처럼 다시 쓸 수 있습니다.\n",
    "\n",
    "<br>\n",
    "$$LossFunction(w, b) = \\frac{1}{N}\\sum_{i = 1}^N ((wx+b)-y_i)^2$$\n",
    "<br>\n",
    "\n",
    "미지수가 $w$와 $b$이기 때문에 그외의 것들은 모두 상수입니다. 그러면 오차함수라는 것은 $w$와 $b$의 2차함수로 설명됩니다. 식만 봐서는 와닿지 않으니 아래의 그림을 볼까요?\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/05.jpg?raw=true)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
