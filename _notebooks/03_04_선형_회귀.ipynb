{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 선형 회귀 알고리즘\n",
    "> 가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 3]\n",
    "- permalink: /linear_regression\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 기초적인 머신러닝 알고리즘인 선형회귀에 대해 배워보고, 실습을 진행해보겠습니다.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 회귀(linear regression) 알고리즘 이란?\n",
    "\n",
    "![](https://mlfromscratch.com/content/images/2020/01/linearRegression2-3.png)\n",
    "\n",
    "이번에 알려드릴 선형 회귀(linear regression) 알고리즘은 머신 러닝에 기초가 되는 **회귀 알고리즘**입니다. 선형 회귀 알고리즘은 데이터 샘플에 맞는 **최적의 직선**을 구하고 이를 통해 예측값을 산출하는 회귀 알고리즘입니다. 회귀알고리즘이기 때문에 정답열의 데이터가 연속형일때 사용합니다.\n",
    "<br><br>\n",
    "\n",
    "그러나 모든 경우에 사용할 수는 없고, 선형 회귀는 **\"우리가 예측하고자 하는 데이터는 선형적인 특성을 갖고 있다. 즉, $y = wx + b$에 적합하다.\"** 라는 가정이 있을 때만 사용할 수 있습니다. (그렇지 않은 경우 좋은 성능을 얻기 어렵습니다.)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 오차 함수 (Loss Function) 정의하기\n",
    "\n",
    "오차를 구하는 방법을 함께 생각해보면서 오차함수라는 것을 우리가 직접 만들어봅시다. 먼저 직선 $y = wx + b$의 $w$와 $b$는 랜덤하게 초기화 되어있습니다. 그렇다면 아래처럼 형편없는 직선이 만들어질 것입니다. 아래 직선은 데이터를 잘 설명한다고 보기 어렵습니다. (너무 낮게 위치하고 있음)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/01.jpg?raw=true)\n",
    "\n",
    "이 상태에서 오차를 측정해봅시다. 오차라는 건 무엇일까요? 매우 간단하게 생각해서 오차는 아래처럼 우리의 예측 직선 $\\hat{y} = wx + b$ 과 실제 데이터 $y$ 사이의 차이라고 생각할 수 있습니다. (앞으로 정답은 $y$, 예측은 $\\hat{y}$라고 하겠습니다) \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/02.jpg?raw=true)\n",
    "\n",
    "우리는 데이터셋에 $x$좌표를 가지고 있습니다. 이 $x$좌표를 우리가 현재 추정하고 있는 직선인 $\\hat{y} = wx + b$에 대입하면 녹색 직선의 $\\hat{y}$좌표가 나옵니다. 그리고 데이터셋에 있는 실제 $y$좌표(파란 점)와 비교하여 차이를 계산할 수 있습니다. <br><br>\n",
    "\n",
    "그런데 생각해보면, 오차의 값은 1개가 아닙니다. 만약 데이터 샘플이 100개가 있으면 100개의 빨간 선($\\hat{y} - y$)이 있을 것입니다. 때문에 전체적인 오차를 평가하기 위해 이 오차의 평균을 구합니다. 데이터 샘플의 수가 $N$개 있다면 평균 오차는 $\\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)$가 됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/03.jpg?raw=true)\n",
    "\n",
    "음.. 꽤나 괜찮아보입니다. 그런데 아직 몇가지 문제가 있습니다. 몇몇 오차 점들은 직선보다 아래에 있어서 음수 오차를 만들어냅니다. 직선보다 아래에 있는 점들도 오차이기 때문에 오차로서 더해져야하는데, 이러면 오히려 평균 오차를 줄여주게 되기 때문에 학습에 방해가 됩니다.\n",
    "<br><br>\n",
    "\n",
    "이 문제를 해결하기 위해 오차 $\\hat{y} - y$를 제곱해서 $(\\hat{y} - y)^2$로 만듭니다. 그러면 모든 음수가 사라지기 때문이죠. 그러면 이제부터는 평균 오차는 $\\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)^2$가 됩니다.\n",
    "<br><br>\n",
    "\n",
    "$$Variance = \\frac{1}{N} \\sum_{i = 1}^N (x_i - mean)^2$$\n",
    "$$LossFunction = \\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)^2$$\n",
    "<br>\n",
    "\n",
    "음... 그런데 이거 어디서 많이 본 것 같은 식 아닌가요? 맞습니다. 통계시간에 배운 분산과 동일합니다. 분산은 샘플과 평균의 차이의 제곱입니다. 오차함수는 샘플과 직선과의 차이의 제곱입니다. 즉, 오차함수라는건, 직선을 평균으로 봤을 때, 직선과 데이터들 사이의 분산을 의미합니다. \n",
    "<br><br>\n",
    "\n",
    "- 오차함수의 진짜 학습 원리 (읽어볼 사람만 읽어보세요)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/04.jpg?raw=true)\n",
    "\n",
    "표준편차는 오차(분산)의 제곱근이기 때문에, 오차가 작아지면 표준편차도 작아집니다. 표준편차가 작아지면 정규분포 형태를 띄는 모델의 확률분포가 뾰족해지면서 함수값이 높아집니다. 확률분포의 함수값은 통계시간에 배운 Likelihood를 의미합니다. 때문에 함수값이 올라간다는건 곧 모델이 예측하는 확률분포 함수의 Likelihood가 높아진다는 것이 되고 그러면 Likelihood와 비례하는 사후확률 (정확도)는 자연스레 올라가게 됩니다.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 학습 알고리즘 : 경사 하강 알고리즘 (Gradient Descent)\n",
    "\n",
    "이제 오차함수를 정의했습니다. 오차함수는 직선과 데이터 사이의 분산이였습니다. 그러면 이 분산을 어떻게 줄일 수 있을까요? \n",
    "\n",
    "<br>\n",
    "$$LossFunction = \\frac{1}{N}\\sum_{i = 1}^N (\\hat{y_i}-y_i)^2$$\n",
    "<br>\n",
    "\n",
    "오차는 위와 같이 계산됩니다. 우리가 찾고자 하는 것은 $w$와 $b$이므로, $LossFunction$을 $LossFunction(w, b)$처럼 함수로 표현하겠습니다. 또한, $\\hat{y} = wx + b$이기 때문에 이들을 그대로 대입해서 아래처럼 다시 쓸 수 있습니다.\n",
    "\n",
    "<br>\n",
    "$$LossFunction(w, b) = \\frac{1}{N}\\sum_{i = 1}^N ((wx+b)-y_i)^2$$\n",
    "<br>\n",
    "\n",
    "미지수가 $w$와 $b$이기 때문에 그외의 것들은 모두 상수입니다. 그러면 오차함수라는 것은 $w$와 $b$의 2차함수로 설명됩니다. 식만 봐서는 와닿지 않으니 아래의 그림을 볼까요? 우리가 직접 만들어낸 오차함수는 2차함수이기 때문에 공간상에서 아래와 같은 모습을 하고 있습니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/05.jpg?raw=true)\n",
    "<br><br>\n",
    "\n",
    "현재 랜덤으로 w와 b를 초기화 했기 때문에 현재 w와 b의 위치에서 Loss함수의 함수값이 굉장히 높습니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/06.jpg?raw=true)\n",
    "<br><br>\n",
    "\n",
    "그러나 만약 w와 b가 움직여서 저 지점에 있다면 Loss가 최소값이 되겠죠?\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/07.jpg?raw=true)\n",
    "<br><br>\n",
    "\n",
    "실제로 경사하강법 알고리즘은 경사를 타고 쭉 내려가서 w와 b를 조정하고 Loss가 최소인곳을 찾아냅니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/08.gif?raw=true)\n",
    "\n",
    "그런데 3차원에서 생각하는건 너무 복잡하고 머리아프니 2차원이라고 생각하고 수식으로 풀어봅시다.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/10.png?raw=true)\n",
    "\n",
    "위 그림에서 최종적으로 도착해야 하는 가장 Loss가 낮은 지점은 w=-2라고 해보자. 현재 w는 20에 있습니다. 이 위치에서는 기울기가 양수입니다. 따라서 $w := w - \\frac{dLoss}{dW}$와 같이, w에서 w방향의 기울기를 뺍니다. 그러면 w는 줄어들고 조금 더 최소값에 가까워집니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/11.png?raw=true)\n",
    "\n",
    "위 그림에서 현재 w는 8에 있습니다. 이 위치에서는 기울기가 양수입니다. 따라서 $w := w - \\frac{dLoss}{dW}$와 같이, w에서 w방향의 기울기를 뺍니다. 기울기는 중앙으로 다가갈수록 점점 작아집니다. 결과적으로 w는 줄어들고 조금 더 최소값에 가까워집니다. \n",
    "\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/12.png?raw=true)\n",
    "\n",
    "위 그림에서 현재 w는 2까지 왔습니다. 여기에서 이 위치에서는 기울기가 양수입니다. 따라서 $w := w - \\frac{dLoss}{dW}$와 같이, w에서 w방향의 기울기를 뺍니다. 기울기는 중앙으로 다가갈수록 점점 작아집니다. 결과적으로 w는 줄어들고 조금 더 최소값에 가까워집니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/13.png?raw=true)\n",
    "\n",
    "위 그림에서 현재 w가 -2에 도달했습니다. 여기에서 이 위치에서는 기울기가 0입니다. 따라서 $w := w - \\frac{dLoss}{dW}$와 같이, w에서 w방향의 기울기를 빼도 w는 변화하지 않고 최종 적으로 원하는 곳으로 이동합니다.\n",
    "\n",
    "만약 w의 랜덤값이 반댓방향으로 초기화 되면 아래와 같이 동작할 텐데, 기울기가 음수이므로 $w := w - \\frac{dLoss}{dW}$가 되면, w - (-기울기) 이므로 w는 증가하게 되고 마찬가지로 최소지점에 도달하게 됩니다. 이 것이 경사하강 알고리즘입니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/14.png?raw=true)\n",
    "\n",
    "그러나, 실제에서는 조금 더 디테일이 있는데, 실제로는 기울기 값이 너무 커서 w값이 크게 벗어나 버립니다. 기울기 값이 너무 크기 떄문에 기울기에 0.001과 같은 작은 숫자를 곱하는데 그 것을 '러닝레이트' 혹은 '학습율'이라고 부릅니다. 따라서 아래와 같이 다시 쓸 수 있습니다. 또한 우리는 w말고 b도 학습시켜야 2방향 모두에서 원하는 위치로 갈 수 있기 때문에 w와 b를 동시에 학습시켜야합니다.\n",
    "\n",
    "$$w := w - (lr \\times \\frac{dLoss}{dW}) $$\n",
    "\n",
    "$$b := b - (lr \\times \\frac{dLoss}{db}) $$\n",
    "<br>\n",
    "\n",
    "러닝레이트가 얼마일지 정하는게 중요한데, 러닝레이트가 크면 조금 더 빨리 학습하지만 세밀한 학습은 불가능하고, 러닝레이트가 작으면 학습은 느리지만 조금 더 세밀해집니다.\n",
    "\n",
    "이렇게 계속해서 반복하다보면 w는 결국 우리가 원하는 지점에 도달하게 되고, 정확도는 올라가게 됩니다. 이제 실습코드를 직접 작성하면서 따라와봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4. 보스턴 주택 가격 예측\n",
    "\n",
    "보스턴 주택 가격 데이터셋은 1978년에 미국 보스턴 지역의 주택가격에 영향을 미치는 요소를 정리한 데이터셋입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747947739060288\n",
      "0.699644958940393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "#\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_boston()\n",
    "x_data = dataset.data\n",
    "y_data = dataset.target\n",
    "#print(x_data.shape) #(506, 13)\n",
    "#print(y_data.shape) #(506,)\n",
    "\n",
    "####################\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_data, y_data, test_size=0.3)\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "y_predict = estimator.predict(x_train) \n",
    "score = metrics.r2_score(y_train, y_predict)\n",
    "print(score) #1.0\n",
    "\n",
    "y_predict = estimator.predict(x_test) \n",
    "score = metrics.r2_score(y_test, y_predict)\n",
    "print(score) #1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
