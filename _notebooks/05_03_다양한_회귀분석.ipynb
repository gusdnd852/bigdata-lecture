{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 다양한 회귀모델\n",
    "> 이번 시간에는 다양한 회귀모델에 대해 공부해봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 5]\n",
    "- permalink: /regressions\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형회귀 리마인드\n",
    "\n",
    "우리가 이전에 배운 선형 회귀 모델은 회귀 모델의 가장 기본 형태입니다. 이 모델을 수식으로 표현하면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg01.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "고전적 선형 회귀 모델은 독립 변수의 개수에 따라 아래와 같이 명칭을 구분하기도 합니다. \n",
    "- 단순 선형 회귀 (simple linear regression): 독립 변수가 하나\n",
    "- 다중 선형 회귀 (multiple linear regression): 독립 변수가 둘 이상\n",
    "<br><br>\n",
    "\n",
    "다중선형회귀는 처음들어보실텐데요. 이전시간에는 단순 선형회귀만 배웠기 때문에 이번에는 다중 선형회귀에 대해 배워봅시다. 만약 아래와 같은 데이터가 있다고 해봅시다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg09.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "이전과 다르게 $x$(독립변수)가 3개가 되었습니다. 따라서 회귀를 진행하려면 이에 곱해질 가중치($w$)도 3개가 있어야합니다. 따라서 아래와 같이 수식을 변경합니다. 이러한 방식의 회귀를 다중 선형회귀라고 합니다. 즉, $x$(독립변수)가 여러개여서, 그에 곱해지는 가중치($w$)도 여러개인 그런 선형회귀 방법입니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg10.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "선형 회귀 모델은 형태가 단순한만큼 데이터에 대해 많은 가정을 갖고 있었습니다.\n",
    "- 독립변수와 종속변수는 반드시 선형 관계이다.\n",
    "- 데이터에 아웃라이어가 없어야 한다.\n",
    "- 독립변수와 오차항은 서로 독립이다.\n",
    "- 자기 상관성이 없다.\n",
    "- 독립변수 간에서는 서로 선형적으로 독립이다. (다중공선성이 없어야한다)\n",
    "- 에러(MSE)는 평균이 0이고 분산이 일정한 정규 분포를 갖는다.\n",
    "<br><br>\n",
    "\n",
    "따라서 만약 실제 데이터가 이런 가정을 충족하지 않는다면, 고전적 선형 회귀 모델은 실제 데이터를 정확히 반영하지 못하게 되므로 다른 방법을 사용해야 합니다. 일반적으로 알려진 가이드라인은 다음과 같습니다. 이 내용에 대해 하나하나 배워보도록 하겠습니다.\n",
    "\n",
    "- 독립 변수와 종속 변수가 선형 관계가 아닌 경우: Polynomial regression, Generalized Additive Model (GAM)\n",
    "- 데이터에 아웃라이어가 있는 경우: Robust regression, Quantile regression\n",
    "- 오차항의 확률분포가 정규분포가 아닌 경우: Generalized Linear Model (GLM)\n",
    "- 자기 상관성이 있는 경우 (시계열): Auto-regression\n",
    "- 독립변수 간에 상관성이 있는 경우 (다중공선성): Ridge regression, Lasso regression, Elastic Net regression, Principal Component Regression (PCR), Partial Least Square (PLS) regression\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 회귀 모델이란?\n",
    "\n",
    "![](https://lh3.googleusercontent.com/proxy/SKnEwTl0fSx-go600KHuevf4OJ_lwaYDsAwwnN9lNX6vy4X-0KFLHLuWhvXmdSOhweMySF39GizTx4moV_afLurIXFzeG1Lop-_3UPTGw4TgRJktQlBv9SRN1BvperZ_nTz2WLP8pZcoC3rnxhFS6daiQJ6z9XO48ToslJvP8QnP8MvORwamxZYYL61VQtXS8IVSYeBYozuRFM_DwURmSPAQWrs_etFyQeAI4serH5Xo4zrwNe-Jftj84lQDhPFugnh4LLOGTwZGxd-do8Y6xq19agS_mFL55nf4)\n",
    "\n",
    "회귀 모델을 한 마디로 정의하면 **'어떤 자료에 대해서 그 값에 영향을 주는 조건을 고려하여 구한 평균'** 입니다. 즉, 데이터를 가장 잘 설명하여 데이터의 평균과 같은 함수를 찾는 것이 회귀분석이며, 통계학적인 관점에서 보면 모든 데이터는 아래와 같은 수식으로 표현할 수 있다고 가정합니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg00.png?raw=true)\n",
    "\n",
    "위 수식에서 h() 가 위에서 말한 조건에 따른 평균을 구하는 함수이며 우리는 이것을 보통 '회귀 모델'이라고 부릅니다. 이 함수는 어떤 조건(x1, x2, x3, ...)이 주어지면 각 조건의 가중치(w1, w2, w3, ...)을 고려하여 해당 조건에서의 평균값을 계산해 주는 것이죠. 뒤에 붙는 e 는 '에러'를 의미합니다. 측정상의 에러나 모든 정보를 파악할 수 없는 점 등 다양한 현실적인 한계로 인해 발생하는 불확실성이 여기에 포함됩니다. 이것은 일종의 '잡음(noise)'인데, 이런 잡음은 이론적으로 보면 평균이 0이고 분산이 일정한 정규 분포를 띄는 성질이 있습니다.\n",
    "<br><br>\n",
    "\n",
    "우리가 회귀 분석을 한다는 것은 이 h() 함수가 무엇인지를 찾는 과정을 의미합니다. 그럼 우리가 추정한 회귀 모델이 정말 h() 라는 걸 어떻게 확신할 수 있을까요? 엄밀히 말하면 정확히 맞다는 것을 알 방법은 없습니다. 다만 그럴 것이라고 어느 정도는 확신할 수 있는 방법이 있는데, 바로 우리가 만든 회귀 모델의 예측치와 실측치 사이의 차이인 **'잔차(residual = MSE)'가 정말 우리가 가정한 에러(e)와 비슷한지 확인하는 것**입니다. <br><br>\n",
    "\n",
    "![](https://t1.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/Jr9/image/SbI5NgVY5ZLzvG6QCT8hUcC00lA.jpg)\n",
    "\n",
    "어쨌든 우리는 최대한 실제 h() 에 가깝게 회귀 모델을 만드는 것이 목표입니다. 이전시간에 이미 언급했지만, 만약 **추정을 잘못하면 몇몇 중요한 조건들을 반영하지 못해 h()의 일부분만 회귀 모델로 만들 수 있는데 이것을 'underfitting' 이라고 부릅니다.** 반대로 실제 종속변수에 영향을 주는 조건이 아닌 단순한 **'잡음'을 평균에 영향을 주는 조건으로 착각하고 모델에 반영할 수도 있는데 이런 것을 'overfitting' 이라고 부릅니다.** 보통 overfitting 문제를 많이 다루고 있지만 사실 현실 세계에서 우리가 만드는 대부분의 회귀 모델은 underfitting 문제도 같이 갖고 있습니다. 다시 말해, 우리가 만드는 대부분의 회귀 모델들은 h()의 일부분과 e의 일부분을 같이 반영하고 있는 상태입니다. 단지 둘 중 어느 쪽이 더 많은 비중을 차지하고 있느냐의 문제일 뿐이지요.<br><br>\n",
    "\n",
    "한편 우리가 모델을 만드는 이유는 **현실을 좀 더 단순한 형태로 표현하기 위해서**입니다. 그리고 이렇게 단순화하려면 불필요하다고 생각하는 정보들을 버려야 합니다. 이때, **우리가 회귀 모델을 만들기 위해 버린 정보들이 무엇인지를 설명하는 것이 회귀 모델의 가정(assumption)입니다.** 즉, 회귀 모델을 만들 때 '실제 데이터는 이러 이러한 특성을 갖고 있다고 가정'하는 것입니다. 따라서 이런 가정이 많아질수록 모델은 좀 더 단순해집니다. 반대로 가정을 최소화할수록 모델은 복잡해지겠죠. 여기서 설명할 다양한 회귀 모델들은 이렇게 데이터가 어떤 특성을 갖고 있다고 가정했느냐에 따라 나뉘어 집니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3. 고려할점 1 (선형성 vs. 비선형성)\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/9987BB345B88C60B33)\n",
    "\n",
    "가장 먼저 고려해야 할 가정은 **선형성**과 **비선형성**입니다. '선형성(linearity)'이란 것은 이전 강의에서도 말한 것 처럼 우리가 어떤것을 변화시키면 딱 그 만큼만 변화되는 것을 의미하며, 선형대수에서는 벡터의 두가지 연산(벡터합, 스칼라곱)을 만족해야 선형적이라고 할 수 있습니다. 즉, 어떤 집합의 원소쌍(아래 수식의 u와 v)에 대해서 함수 f()가 아래 두 가지 성질을 만족시키는 것을 말합니다 (직관적으로 잘 와닿지 않는 분들을 위해 쉽게 말하면, 일차 다항식을 선형 함수라고 생각하면 됩니다.\n",
    "\n",
    "- $f(c*u) = c*f(u)$  ---------- (스칼라곱)\n",
    "- $f(u+v) = f(u) + f(v)$  ---- (벡터 합)\n",
    "<br><br>\n",
    "\n",
    "\n",
    "어쨌든 우리가 배우는 대부분의 회귀 모델은 선형 회귀 모델입니다. 반면 최근에 크게 주목받고 있는 딥러닝은 대표적인 비선형 회귀 모델링 방법입니다. 선형 회귀 모델은 회귀 계수간의 관계가 비교적 직관적이기 때문에 각 조건의 영향력을 해석하기가 비선형 모델에 비해 쉽습니다. 대신 모든 조건들을 오직 선형 결합(쉽게 말해 더하기)으로만 표현해야 하기 때문에 표현력에 한계가 있습니다. **다시 말해 실제 모델링 대상이 되는 현실 데이터가 선형적이지 못한 데이터라면 정확한 회귀 모델을 만들 수 없습니다.** \n",
    "<br><br>\n",
    "\n",
    "선형 회귀 모델의 이런 한계점 때문에 모델의 **해석보다는 예측 자체가 중요한 복잡한 문제에 대해서는 딥러닝을 이용**합니다. 딥러닝은 비선형 회귀 모델이기 때문에 현실 세계의 복잡한 관계도 거의 대부분 표현이 가능합니다. 즉, underfitting 문제에서 상대적으로 자유롭습니다. 대신 불필요한 잡음을 모델에 반영하는 overfitting 문제가 발생할 가능성이 더 크기 때문에 이 문제를 피하기 위한 다양한 기법들이 연구되고 있죠. 정리하자면, 회귀 모델은 모델링 대상을 **회귀 계수의 선형 결합만으로 표현할 것인지 여부에 따라 '선형' 회귀 모델과 '비선형' 회귀 모델로 구분**됩니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4. 고려할점 2 (타겟 = 정답열 = 종속변수의 개수에 따른 구분 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두번째로 고려해야할 가정은 종속 변수 개수입니다. 종속변수와 독립변수에 대해서 모르시는 분들도 존재할 수 있을 것 같아서 설명드립니다. 말이 어려워보이는데 간단합니다. 만약 우리가 회귀모델 $y = 2x + 3$을 만들었다면, 입력 데이터 $x$는 독립변수이고, 출력 $y$는 종속변수 입니다. 앞으로 이렇게 이해하시면 될 것 같습니다. <br><br>\n",
    "\n",
    "보통 $y = h(x) + e$ 라고 하면 종속변수인 y는 하나인것만 생각합니다만 실제로는 종속변수가 여러 개인 경우에 대해서도 회귀 모델을 만들 때가 있습니다. 그래서 종속변수가 하나인 회귀 모델을 **'단변량(univariate)' 회귀 모델** 이라고 부르며, 종속변수가 2개 이상인 경우를 **'다변량(multivariate)' 회귀 모델**이라고 부릅니다.   다변량 회귀 모델은 주로 계량 경제학에서 많이 다루는데 종속변수 간에 서로 상관성이 있는지, 종속변수가 서로 다른 종속변수의 독립변수 역할도 수행하는지, 어떤 두 종속 변수가 장기 균형 관계를 갖는지 등의 조건에 따라 다양한 회귀 모델로 나뉩니다.\n",
    "<br><br>\n",
    "\n",
    "여기까지가 회귀 모델의 가장 상위 단계에서 고려해야 하는 모델링 방식입니다. 즉, 모델링 대상이 하나의 종속변수만 다루는지 아니면 여러 개의 종속변수를 같이 고려해야 하는지, 종속변수가 가중치(w)와의 선형 결합만으로 표현이 가능한지 아닌지에 따라 '단변량 선형 회귀 모델', '단변량 비선형 회귀 모델', '다변량 선형 회귀 모델', '다변량 비선형 회귀 모델'로 구분하게 되며 이 각각이 다시 하위에 광범위한 모델링 기법들을 포함하고 있습니다 (그런데 보통 비선형 회귀 모델에서는 단변량과 다변량을 따로 구분하지는 않는 것 같습니다). 이 글에서 이 모든 회귀 모델들을 다루기는 불가능하므로 범위를 좁혀서, 회귀 분석을 할 때 주로 사용하고 있는 '단변량 선형 회귀 모델'에 대해서 좀 더 자세히 다루겠습니다. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5. 독립 변수와 종속 변수가 선형 관계가 아닌 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. 다항 회귀 (Polynomial regression)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg02.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "이름 그대로 독립 변수가 다항식으로 구성되는 회귀 모델입니다. 아래 그림처럼 만약 종속변수인 y와 독립변수인 x가 선형 관계가 아닌 곡선 형태를 갖는다면 독립변수에 지수승을 붙여서 여러 개의 변수로 만들어 회귀 모델을 구성하는 기법을 말합니다. 실상 형태적으로 보면 고전적인 다중 선형 회귀 모델과 똑같습니다. (당뇨병 예측때 제가 독립변수를 제곱하여 다항회귀와 비슷한 작업을 수행하는 것을 본적 있으실 겁니다)\n",
    "<br><br>\n",
    "\n",
    "#### 5.2. Generalized Additive Model (GAM)\n",
    "\n",
    "'일반화 가법 모형(GAM)'은 회귀 모델을 만들 때 독립변수에 단순히 w만 곱하는 것이 아니라독립변수(x)를 다른 함수에 입력한 출력을 독립변수로써 사용하는 기법입니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg03.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "독립변수에 적용되는 함수들을 smooth function 이라고 부르는데, 이 smooth function으로 비선형 함수를 사용함으로써 종속변수와 독립변수 간의 비선형 관계를 표현하는 방식입니다. smooth function 으로 어떤 함수를 사용하느냐에 따라 다양한 관계를 표현할 수 있기 때문에 'generalized' 라는 이름이 붙었죠. <br><br>\n",
    "\n",
    "\n",
    "실상 바로 위에서 소개한 다항 회귀 역시 일종의 GAM 입니다. GAM은 이렇게 표현할 수 있는 모델의 범위가 넓긴 하지만 여전히 회귀 계수 관점에서는 선형 결합 형태로만 표현되는 한계가 있습니다(즉, 비선형 회귀 모델이 아닙니다). 가령, GAM은 두개 이상의 독립 변수 간의 상호 작용에 대한 추정(다중공선성 체크)을 자동으로 해주지는 못합니다. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 오차항의 확률분포가 정규분포가 아닌 경우\n",
    "\n",
    "#### 6.1. Generalized Linear Model (GLM)\n",
    "\n",
    "일반화 선형 회귀 모델은 종속 변수에 적절한 함수를 적용하는 회귀 모델링 기법입니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg04.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "이렇게 종속변수에 적용하는 함수를 link function 이라고 부르는데 오차항의 확률 분포가 무엇이냐에 따라 일반적으로 사용하는 link function 이 정해져 있습니다. 우리는 이미 Logistic회귀에서 Sigmoid함수를 link function으로 사용해봤죠. 보통 GLM은 종속 변수의 특성에 따라 세부적인 명칭을 구분하기도 합니다.\n",
    "<br><br>\n",
    "\n",
    "- 종속 변수가 0 아니면 1인 경우: Logistic regression\n",
    "- 종속 변수가 순위나 선호도와 같이 순서만 있는 데이터인 경우: Ordinal regression\n",
    "- 종속 변수가 개수(count)를 나타내는 경우: Poisson regression\n",
    "<br><br>\n",
    "\n",
    "GLM은 가장 많이 사용하는 회귀 분석 기법 중 하나입니다. http://www.theanalysisfactor.com/r-glm-model-fit/ 에는 다양한 GLM 활용 방법이 소개되어 있습니다 (시리즈 글이므로 전체 글을 다 보시면 좋습니다). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 7. 오차항에 자기 상관성이 있는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Autoregressive Model\n",
    "\n",
    "보통 시계열 데이터와 같이 순서가 정해져 있는 데이터의 경우 주기성이나 계절성 같이 일정한 패턴을 갖고 있는 경우가 많은데 이것을 '자기 상관성' 이라고 합니다. 그래서 이런 경우에는 아래와 같이 회귀 모델을 만들게 됩니다. 바로 이전 타임스텝의 종속변수가 다음 타임스텝을 설명하게 되는 꼴입니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg05.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "수식을 보면 알 수 있듯이, 특정 시점 t의 데이터를 과거 시점의 종속변수들이 설명하는 방식입니다. 이런 모델을 '자기 회귀 (Autoregressive)' 모델이라고 부릅니다. 대개의 경우 현실 세계에서는 위와 같이 단순한 자기 회귀 모델로만 적용할 수 있는 경우는 거의 없습니다. 보통 위 수식에 있는 e_t 에 또 다른 패턴이 있는 경우가 많으며 어떤 패턴이 있느냐에 따라 크게 두 가지 방식으로 확장됩니다.\n",
    "\n",
    "- 시간에 따라 평균이 변하는 경우: Auto-Regressive Moving Average (ARMA) model\n",
    "- 시간에 따라 분산이 달라지는 경우: Auto-Regressive Conditionally Heteroscedastic (ARCH) model\n",
    "<br><br>\n",
    "\n",
    "우리는 차후 딥러닝 시간에 이러한 Autoregressive Model인 RNN에 대해서 배워볼 예정입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 8. 데이터에 아웃라이어가 있는 경우\n",
    "\n",
    "#### 8.1. Robust regression\n",
    "\n",
    "일반적으로 선형 회귀 모델에서는 회귀 계수를 추정할 때 잔차의 제곱합을 이용하는 '최소 제곱법(ㅡMSE)'을 사용합니다. 그런데 이렇게 잔차의 제곱을 이용할 경우 아웃라이어와 같이 잔차가 다른 데이터에 비해 매우 큰 경우에는 제곱을 하면 그 값이 비례적으로 커지기 때문에 이 값 하나로 인해 전체 추정치가 왜곡되기 쉽습니다. <br><br>\n",
    "\n",
    "Robust regression 은 이런 문제를 완화하기 위한 회귀 모델 기법입니다. 모델의 형태 자체는 일반적인 선형 회귀 모델과 동일하지만 회귀 계수의 추정 방식에서 차이가 있는 것이죠. 가장 널리 알려진 Robust regression 기법은 잔차의 제곱 대신 절대값의 합이 최소가 되도록 계수를 추정하는 방식입니다. 이렇게 절대값을 이용하면 아웃라이어의 영향력이 줄어들기 때문에 왜곡 현상이 완화됩니다. 고전적 선형 회귀와 로버스트 회귀의 계수 추정 방법을 수식으로 비교하면 아래와 같습니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg06.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "\n",
    "#### 8.2. Quantile regression\n",
    "\n",
    "첫부분에서 언급했듯이 대부분의 회귀 모델은 어떤 조건에서 종속 변수의 '평균'을 추정하는 방식입니다. 그런데 특이하게도 **quantile regression은 평균이 아니라 특정 분위값을 추정**하는 기법입니다. 분위값이란 전체 데이터를 정렬한 후 전체 순위를 백분율로 표시했을 때 특정 %에 위치한 값을 의미합니다. <br><br>\n",
    "\n",
    "예를 들어 1%의 분위값은 상위 1% 그룹의 경계에 있는 데이터의 종속변수값을 말합니다. 따라서 만약 quantile regression을 이용해서 50% 분위값인 중앙값을 추정하는 모델을 만들면 아웃라이어의 영향을 거의 받지 않게 됩니다. 왜냐하면 아웃라이어의 값이 아무리 비정상적으로 크더라도 전체 데이터 상에서 다른 관측값들의 순위는 영향을 받지 않기 때문입니다. \n",
    "<br><br>\n",
    "\n",
    "Quantile regression이 갖는 또다른 장점은 분산이 일정하지 않은 이분산(heteroscedasticity) 데이터도 회귀 모델링이 가능하다는 점입니다. 더 나아가 다양한 분위값에 대해 각각 회귀 모델을 만들 경우 데이터의 전반적인 분포와 그에 따른 가중치(w)의 관계를 추정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 9. 다중공선성이 있는 경우\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1. Ridge / Lasso / Elastic net regression\n",
    "\n",
    "다중공선성이 있는 데이터에 대해서 그냥 고전적인 선형 회귀 모델을 만들게 되면 **특정 회귀 계수의 영향력이 과다 추정**될 수 있습니다. 이런 문제를 피하기 위해 가장 널리 알려진 방법이 'regularization'이라고 부르는 기법입니다. 우리는 이전 선형회귀 시간에 $y = wx + b$꼴의 선형회귀 식을 풀때, 오차함수로 $LossFunction(w, b) = \\sum_i (\\hat{y_i} - y_i)^2$와 같은 최소제곱합(MSE)를 최소화 한다고 배웠습니다. <br><br>\n",
    "\n",
    "\n",
    "이때, 오차함수인 LossFunction에 $\\sum_i w_i$를 다시 더해주는 것을 'regularization'라고 부릅니다. 그래서 전체 오차함수는 $LossFunction(w, b) = \\sum_i (\\hat{y_i} - y_i)^2 + \\lambda \\sum_i w_i$와 같은 형태가 됩니다. 여기에서 $\\lambda$는 얼마나 regularization할지 결정하는 상수입니다.\n",
    "<br><br>\n",
    "\n",
    "이러한 작업이 왜 도움이 되냐면 특정 가중치(w)값이 과다하게 커지지 못하게 가중치(w)자체의 값을 Loss함수에 넣으면 $w$값들도 Loss를 최소화하면서 같이 작아져서 특정 가중치(w)값이 과다하게 커지지 못합니다. 즉, 일종의 페널티 역할을 하는 것이죠. <br><br>\n",
    "\n",
    "여기서 소개하는 Ridge / lasso / elastic net 이 모두 이런 regularization 을 이용한 회귀 모델링 기법입니다. 이것 역시 로버스트 회귀처럼 모델의 형태 자체는 고전적인 선형 회귀 모델과 동일하나 회귀 계수를 추정하는 방식에서 차이가 있습니다. 말로 설명하기에 앞서 우선 수식으로 표현하면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg07.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "위 수식을 보면 고전적인 선형 회귀 모델은 회귀 계수를 추정할 때 잔차의 제곱의 합을 계산합니다. 이 함수를 Loss함수라고 부르는데 이 Loss함수가 최소가 되는 회귀 계수를 찾는 것이죠. 그런데 여기서 소개하는 회귀 모델들은 이 비용함수에 (그림에서 빨간색으로 표시한) 추가적인 수식들이 붙습니다. 이런 추가적인 수식을 페널티 함수라고 부릅니다. 말그대로 회귀 계수 값 자체가 너무 커지지 않도록 페널티를 줌으로써 회귀계수값들이 과다 추정되는 것을 막는 것입니다. 이 때 페널티 함수의 형태에 따라 ridge 와 lasso 가 구분됩니다. ridge regression 은 회귀 계수의 제곱합을 계산하는 방식이고, lasso 는 회귀 계수의 절대값을 계산하는 방식입니다. 그리고 elastic net은 이 둘을 결합한 방식이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 기타\n",
    "\n",
    "#### 10.1. Survival regression\n",
    "\n",
    "특정 사건이 발생한 시간을 추정할 때 사용하는 회귀 모델입니다. '생존 (Survival)' 이라는 이름이 붙은 이유는 이 모델링 기법이 의학 분야에서 임상 실험 환자가 사망할 때까지의 시간을 추정하기 위한 기법을 고안하는 과정에서 만들어졌기 때문입니다.  예를 들어 어떤 병에 걸린 환자가 그 병으로 인해 사망할 때까지 걸리는 시간을 추정할 때는 다음과 같은 문제들을 처리해야 합니다.\n",
    "\n",
    "-  회귀 모델을 만드는 시점에 살아 있는 환자들의 경우 사망할 때까지 걸리는 시간을 어떻게 정의해야 하나?\n",
    "- 원래 모델링하려는 병이 아닌 다른 원인(자연사 혹은 교통 사고 등)으로 인해 사망한 환자의 경우는 어떻게 처리해야 하나?\n",
    "<br><br>\n",
    "\n",
    "만약 해당 병으로 인해 사망한 환자들만을 갖고 회귀 분석을 한다면 사망 환자에 대해서만 편향된 모델이 만들어지기 때문에 전반적으로 사망하는데까지 걸리는 시간이 과소 추정될 수 있습니다. **이런 식으로 내가 모델링하고자 하는 사건이 아직 발생하지 않은 데이터들을 '중도 절단 (censoring)' 데이터**라고 부르는데 이런 중도 절단 데이터 문제로 인해 일반적인 회귀 모델을 이용할 수 없습니다. \n",
    "<br><br>\n",
    "\n",
    "그래서 생겨난 기법이 생존 분석 기법입니다. 생존 분석 분야는 그 자체로도 굉장히 광범위하기 때문에 전체 내용을 여기서 다 다룰수는 없고 아주 간략하게만 소개하자면, 생존 회귀는 사망 시간을 직접적으로 모델링하는 대신, 어떤 그룹의 시간에 따른 생존율의 변화에 대한 함수를 추정하는 방식을 취합니다.  이후 실제 예측을 할 때는 어떤 관측치가 속한 그룹의 생존율이 50% 미만으로 떨어지는 시간을 추정치로 사용합니다. 참고로 생존 회귀 모델 중 가장 널리 사용되는 기법으로는 Cox Proportional Hazard Model 이 있습니다. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 마무리\n",
    "\n",
    "지금까지 언급한 회귀 모델들을 특징에 따라 분류한 내용을 도식화하면 아래 그림과 같습니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day5/reg08.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "사실 회귀분석에 대한 내용을 이보다 깊게 설명하는것은 시간상 다소 무리가 있습니다. 제게 주어진 8일 동안 최대한 도움이 되려면 어떻게 알려줘야할까 고민했는데, 이렇게 키워드 형식으로 제공하는 것이 가장 낫다고 판단이 되었습니다. 제가 이번시간에 키워드 형식으로 빠른시간동안 다양하게 제공해드렸으니 깊은 이해를 원하시면 검색하셔서 공부하시고 프로젝트에 적용하시면 좋을 것 같습니다. 다음시간부터는 비지도 학습에 대해 공부해보도록하겠습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
