{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. 다양한 최적화 기법들\n",
    "> 다층 퍼셉트론을 최적화 하기 위한 다양한 기법들을 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 6]\n",
    "- permalink: /mlp_optimization\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 옵티마이저 (Optimizer)\n",
    "\n",
    "딥러닝 모델을 학습하기 위해서 지금까지 Gradient Descent 알고리즘을 배웠습니다. 그러나 이러한 Gradient Descent 알고리즘은 기본적으로 많은 문제가 있습니다. 그에 대해 알아볼까요? <br><br>\n",
    "<br><br>\n",
    "\n",
    "#### 1.1. 경사하강법(Gradient Descent) 복습\n",
    "\n",
    "뉴럴 네트워크의 loss function의 현 weight의 기울기(gradient)를 구하고 loss를 줄이는 방향으로 업데이트(조정)해 나가는 방법을 통해서 뉴럴 네트워크를 학습하였습니다. loss(cost) function이라는게 나왔군요. 뉴럴 네트워크에서 loss function은 무엇일까요? 간단히 설명하면 지금 현재의 가중치에서 \"틀린정도\"를 알려주는 함수이죠.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99E6363359D86A8805)\n",
    "\n",
    "즉, 현재 네트워크의 weight에서 내가 가진 데이터를 다 넣어주면 전체 에러가 계산 되겠죠? 거기서 미분을 하면 에러를 줄이는 방향을 알 수 있습니다. 바로 위의 그림과 같이 말이죠. 그 방향으로 정해진 스텝량(learning rate)을 곱해서 weight을 이동시킵니다. 이걸 계속 반복해서 학습을 하는 것이죠.\n",
    "<br><br>\n",
    "\n",
    "그러나 기존의 Gradient Descent 방식에는 크나큰 단점이 있었습니다. 위에서 적은 내용을 보시게 되면 한가지 큰 문제점을 발견할 수 있습니다. **최적값을 찾아 나가기 위해서 한칸 전진할 때마다 모든 데이터 셋을 넣어주어야 한다는 것**이죠. 그래서 학습이 굉장히 오래 걸리는 문제가 발생하게 되는 것이죠. 언제 다 학습시킬 것인가라는 문제가 발생한 것이죠. 그러면 Gradient Descent 말고 더 빠른 Optimizer는 없는지 연구자들이 고민을 하게 되죠 그래서 나온 것이 Stochastic Gradient Descent 입니다.\n",
    "<br><br>\n",
    "\n",
    "#### 1.2. 확률적 경사 하강법 (Stochastic Gradient Descent)\n",
    "\n",
    "Stochastic Gradient Descent(이하 SGD)의 아이디어는 간단합니다. 바로 \"조금만 훑어보고 빠르게 가봅시다\"라는 것이죠. GD와 SGD의 차이를 간단히 그림으로 비교해보면 아래의 그림과 같습니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/999EA83359D86B6B0B)\n",
    "\n",
    "기본적으로 SGD는 데이터 1개만 보고 움직이는 것인데 그러면 학습이 너무 Noisy하게 되므로 조금 더 보완해서 **데이터 N개를 보고 움직이는 Mini Batch Traning 방식이 현재에도 많이 쓰이고 있습니다.** 이렇게 해야하는 가장 큰 이유는 사실 데이터셋이 너무 커서 메모리에 모두 올리지 못하기 때문이죠. 또한 일반 GD보다 SGD가 훨씬 나은 속도를 보여줍니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/9961913359D86B9833)\n",
    "\n",
    "GD의 경우 항상 전체 데이터 셋을 가지고 한발자국 전진할 때마다(learning rate) 최적의 값을 찾아 나아가고 있는 모습을 볼수 있습니다. 그러나 SGD는 Mini-batch 사이즈 만큼 조금씩 돌려서 최적의 값으로 가고 있습니다. 흠... 꽤 괜찮아 보입니다. 그러나 SGD에도 문제점이 존재합니다. 미니 배치를 통해 학습을 시키는 경우 최적의 값을 찾아 가기 위한 방향 설정이 뒤죽 박죽입니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/9969013359D86BD731)\n",
    "\n",
    "또 다른 문제점은 스텝의 사이즈입니다. 이것을 다른 말로 정의하면 learning rate입니다. 한걸음 나아가기 위한 보폭이 낮으면 학습하는데 오래 걸리고, 너무 크면 최적의 값을 찾지 못하는 문제가 있겠습니다. 도대체 어떤 learning rate를 적용해야하는가에 대한 문제도 존재하죠.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/999A143359D86C022F)\n",
    "\n",
    "\n",
    "그래서 최근의 연구에서는 이러한 SGD의 문제점을 인지하고 각각의 문제점들을 개선하는 더 좋은 Optimizer들이 많이 있습니다. 새로운 Optimizer 들에 대해 알아보도록 하겠습니다.\n",
    "<br><br>\n",
    "\n",
    "#### 1.3. SGD의 문제 점\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/60.png?raw=true)\n",
    " \n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/61.png?raw=true)\n",
    "\n",
    "만약 오차함수가 위처럼 만들어졌다고 생각해봅시다. 딥러닝에서 오차함수는 어떤 모양이든 가능하기 때문에 이러한 가정이 문제될 것은 없습니다. <br><br> 위 함수의 경우 위 그림의 왼쪽과 같이 '밥그릇'을 x축 방향으로 늘인 듯한 모습이고, 실제로 이 등고선은 오른쪽과 같이 x축 방향으로 늘인 타원으로 되어 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/62.png?raw=true)\n",
    "\n",
    "이 기울기의 y축 방향은 크고 x축 방향은 작다는 것이 특징입니다. 말하자면 y축 방향은 가파른데 x축 방향은 완만한 것입니다. 또, 여기에서 주의할 점으로는 위 식이 최솟값이 되는 장소는 (x, y) = (0, 0)이지만, 위의 그림이 보여주는 기울기 대부분은 (0, 0) 방향을 가리키지 않는다는 것입니다. 그러니까, 만약 x가 -5나 +5와 같은 곳으로 잡히면 (0, 0)으로 갈 수가 없습니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/63.png?raw=true)\n",
    "\n",
    "이제 위 함수에 SGD를 적용해볼까요? 탐색을 시작하는 장소(초깃값)는 (x, y) = (-7.0, 2.0)으로 하자. 결과는 위와 같습니다. SGD는 위 그림과 같이 심하게 굽이진 움직임을 보여줍니다. 상당히 비효율적이다! 즉, SGD의 단점은 비등방성(anisotropy) 함수(이렇게 축마다 기울기가 크게 상이한 함수)에서는 탐색 경로가 비효율적이라는 것입니다. <br><br>\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/993D383359D86C280D)\n",
    "\n",
    "때문에 위 그림과 같이 SGD의 스텝방향(파란색)의 문제점을 집중적으로 개선한 알고리즘들과 스텝 사이즈(빨간색)를 얼마나 하는게 좋을 것인가에 대한 알고리즘들이 집중적으로 연구되어왔으며 마지막엔 이러한 2가지 방법을 같이 사용한 알고리즘들이 나왔습니다. 그럼 한번 각 알고리즘의 성능에 관하여 그림으로 살펴보면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/57.gif?raw=true)\n",
    " \n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/58.gif?raw=true)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/59.gif?raw=true)\n",
    " \n",
    "위 그림들을 보시면 매우 다양한 학습 알고리즘이 존재하고, 우리가 지금까지 썼던 SGD가 제일 안좋은것처럼 보입니다. SGD는 단순하고 구현도 쉽지만, 문제에 따라서는 비효율적일 때가 있습니다. \n",
    "<br><br>\n",
    "\n",
    "그럼 이제부터 SGD의 이러한 단점을 개선해주는 몇가지 알고리즘을 간단하게 소개해드리도록 하겠습니다. <br><br>\n",
    "\n",
    "#### 1.4. 모멘텀\n",
    "\n",
    "모멘텀(Momentum)은 '운동량'을 뜻하는 단어로, 물리와 관계가 있습니다. 모멘텀 기법은 수식으로는 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/64.png?raw=true)\n",
    "\n",
    "기존의 Gradient Descent 알고리즘에 $\\alpha \\triangle w$가 더해져있습니다. 이 때, $ \\triangle w$는 이전 스텝의 이동량으로 이해하시면 됩니다. 이 것은 움직임에 있어서 '관성'의 의미와 매우 흡사합니다. 관성을 추가함으로서 새로운 이동량은 이전의 이동량에 영향을 받게 됩니다. 그래서 이동량이 급격하게 변화하는 것을 막습니다. 또한 Local Minimum에 빠지는 문제가 어느정도는 보완됩니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/65.png?raw=true)\n",
    "\n",
    "딥러닝의 경우 찾고자 하는 함수 자체가 비선형함수이기 때문에 Loss 함수가 밥그릇 모양(전문용어로는 Convex하다고 합니다)이라는 보장이 없습니다. 즉, $(y - \\hat{y})^2$이 2차식일거라는 보장이 없습니다. $\\hat{y}$이 어떻게 생긴지 모르니까요. 때문에 아래와 같은 모습의 Loss함수도 가능한데, 기존 SGD의 경우 Local Minimum 지점에서 기울기가 0이 되기 때문에 움직일가 없습니다. 그래서 더 낮은 위치인 Global Minimum에 도달할 수 없습니다. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/proxy/qfD-puc3c0jf9TrQTeYuKp1t0eAgLYZrLVsoq-kbVbaBnW18BLT4-0NpZcUnMWhvPdmfQJRpyvY7eqK9oK7urSR95luj97JCXs3hfEg)\n",
    "\n",
    "그러나 관성이 추가되면 이전에 급한 기울기로 이동해왔다면 저러한  Local Minimum을 뛰어넘고 Global Minimum까지 도달할 수도 있게 됩니다. <br><br>\n",
    "\n",
    "그러나 사실 딥러닝 모델이 왜 Local Minimum에 잘 빠지지 않고 거의 대부분의 경우 Global Minimum으로 이동하는지에 대한 이유는 아직도 밝혀지지가 않았습니다. 모멘텀이 Local Minimum 탈출에 도움을 줄 수 있긴 하지만 항상 모멘텀만으로 Local Minimum을 탈출한다고 말하기는 어려울 것입니다. <br><br>\n",
    "\n",
    "흔히 인터넷에서 딥러닝이 왜 잘되는지 모른다는 이유가 여기에 있습니다. 딥러닝 모델은 너무 쉽게 Loss함수에서 Global Minimum을 찾아 움직입니다. 그런데 왜 그런지는 아직도 밝혀진바가 없습니다. 무튼, 이러한 기법이 바로 모멘텀(관성)기법입니다. 모멘텀을 적용하면 움직임이 아래처럼 바뀝니다.\n",
    "\n",
    "- SGD\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/63.png?raw=true)\n",
    "\n",
    "- Momentum\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/65.png?raw=true)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
