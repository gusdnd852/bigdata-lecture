{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. 다양한 최적화 기법들\n",
    "> 다층 퍼셉트론을 최적화 하기 위한 다양한 기법들을 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 6]\n",
    "- permalink: /mlp_optimization\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 옵티마이저 (Optimizer)\n",
    "\n",
    "딥러닝 모델을 학습하기 위해서 지금까지 Gradient Descent 알고리즘을 배웠습니다. 그러나 이러한 Gradient Descent 알고리즘은 기본적으로 많은 문제가 있습니다. 그에 대해 알아볼까요? <br><br>\n",
    "<br><br>\n",
    "\n",
    "#### 1.1. 경사하강법(Gradient Descent) 복습\n",
    "\n",
    "뉴럴 네트워크의 loss function의 현 weight의 기울기(gradient)를 구하고 loss를 줄이는 방향으로 업데이트(조정)해 나가는 방법을 통해서 뉴럴 네트워크를 학습하였습니다. loss(cost) function이라는게 나왔군요. 뉴럴 네트워크에서 loss function은 무엇일까요? 간단히 설명하면 지금 현재의 가중치에서 \"틀린정도\"를 알려주는 함수이죠.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99E6363359D86A8805)\n",
    "\n",
    "즉, 현재 네트워크의 weight에서 내가 가진 데이터를 다 넣어주면 전체 에러가 계산 되겠죠? 거기서 미분을 하면 에러를 줄이는 방향을 알 수 있습니다. 바로 위의 그림과 같이 말이죠. 그 방향으로 정해진 스텝량(learning rate)을 곱해서 weight을 이동시킵니다. 이걸 계속 반복해서 학습을 하는 것이죠.\n",
    "<br><br>\n",
    "\n",
    "그러나 기존의 Gradient Descent 방식에는 크나큰 단점이 있었습니다. 위에서 적은 내용을 보시게 되면 한가지 큰 문제점을 발견할 수 있습니다. **최적값을 찾아 나가기 위해서 한칸 전진할 때마다 모든 데이터 셋을 넣어주어야 한다는 것**이죠. 그래서 학습이 굉장히 오래 걸리는 문제가 발생하게 되는 것이죠. 언제 다 학습시킬 것인가라는 문제가 발생한 것이죠. 그러면 Gradient Descent 말고 더 빠른 Optimizer는 없는지 연구자들이 고민을 하게 되죠 그래서 나온 것이 Stochastic Gradient Descent 입니다.\n",
    "<br><br>\n",
    "\n",
    "#### 1.2. 확률적 경사 하강법 (Stochastic Gradient Descent)\n",
    "\n",
    "Stochastic Gradient Descent(이하 SGD)의 아이디어는 간단합니다. 바로 \"조금만 훑어보고 빠르게 가봅시다\"라는 것이죠. GD와 SGD의 차이를 간단히 그림으로 비교해보면 아래의 그림과 같습니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/999EA83359D86B6B0B)\n",
    "\n",
    "기본적으로 SGD는 데이터 1개만 보고 움직이는 것인데 그러면 학습이 너무 Noisy하게 되므로 조금 더 보완해서 **데이터 N개를 보고 움직이는 Mini Batch Traning 방식이 현재에도 많이 쓰이고 있습니다.** 이렇게 해야하는 가장 큰 이유는 사실 데이터셋이 너무 커서 메모리에 모두 올리지 못하기 때문이죠. 또한 일반 GD보다 SGD가 훨씬 나은 속도를 보여줍니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/9961913359D86B9833)\n",
    "\n",
    "GD의 경우 항상 전체 데이터 셋을 가지고 한발자국 전진할 때마다(learning rate) 최적의 값을 찾아 나아가고 있는 모습을 볼수 있습니다. 그러나 SGD는 Mini-batch 사이즈 만큼 조금씩 돌려서 최적의 값으로 가고 있습니다. 흠... 꽤 괜찮아 보입니다. 그러나 SGD에도 문제점이 존재합니다. 미니 배치를 통해 학습을 시키는 경우 최적의 값을 찾아 가기 위한 방향 설정이 뒤죽 박죽입니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/9969013359D86BD731)\n",
    "\n",
    "또 다른 문제점은 스텝의 사이즈입니다. 이것을 다른 말로 정의하면 learning rate입니다. 한걸음 나아가기 위한 보폭이 낮으면 학습하는데 오래 걸리고, 너무 크면 최적의 값을 찾지 못하는 문제가 있겠습니다. 도대체 어떤 learning rate를 적용해야하는가에 대한 문제도 존재하죠.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/999A143359D86C022F)\n",
    "\n",
    "\n",
    "그래서 최근의 연구에서는 이러한 SGD의 문제점을 인지하고 각각의 문제점들을 개선하는 더 좋은 Optimizer들이 많이 있습니다. 새로운 Optimizer 들에 대해 알아보도록 하겠습니다.\n",
    "<br><br>\n",
    "\n",
    "#### 1.3. Optimizer 계보\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/993D383359D86C280D)\n",
    "\n",
    "위 그림과 같이 SGD의 스텝방향(파란색)의 문제점을 집중적으로 개선한 알고리즘들부터 스텝 사이즈(빨간색)를 얼마나 하는게 좋을 것인가에 대한 알고리즘 2개로 나누어져 있으면 마지막엔 이러한 2가지 방법을 같이 사용한 알고리즘들이 나왔습니다. 그럼 한번 각 알고리즘의 성능에 관하여 그림으로 살펴보면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/57.png?raw=true)\n",
    " \n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/58.png?raw=true)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
