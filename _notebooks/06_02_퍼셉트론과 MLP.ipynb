{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 퍼셉트론과 다층 퍼셉트론\n",
    "> 인공신경망 알고리즘인 퍼셉트론에 대해 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 5]\n",
    "- permalink: /perceptron\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1. 퍼셉트론의 탄생 배경\n",
    "\n",
    "![](https://assets-global.website-files.com/5bc662b786ecfc12c8d29e0b/5d018a9ba972742ad9698ed9_deep%20learning.jpg)\n",
    "\n",
    "인공지능(AI)은 우리 사람의 뇌를 흉내내는 인공신경망(딥러닝)과 다양한 머신러닝 알고리즘을 통해 구현됩니다. 이전 수업에서도 잠시 언급했던 딥러닝 알고리즘이 현재 가장 널리 사용되는 인공지능을 위한 알고리즘이죠. 이런 딥러닝도 수십년전에 개념이 정립되었던 초기 인공신경망으로부터 발전된 것이라 볼 수 있습니다. 여기서 소개할 퍼셉트론은 이해하기가 다소 까다로울수 있지만 앞으로 계속 등장할 딥러닝의 다양한 내용을 위한 기초 개념이 되므로 이해하고 넘어가는 것이 좋습니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/11.png?raw=true)\n",
    "\n",
    "우리 사람의 뇌는 신경계를 구성하는 주된 세포인 뉴런(neuron)을 약 1000억개 정도 가지고 있으며, 뉴런들은 시냅스라는 구조를 통해 전기, 화학적 신호를 주고 받음으로써 다양한 정보를 받아들이고, 그 정보를 저장하는 기능을 수행합니다. 위 그림은 하나의 뉴런에서 신호를 입력 받고 그에 대한 결과 신호를 출력하는 개념을 도식화 한 것입니다.\n",
    "<br><br>\n",
    "\n",
    "1943년 신경과학자인 Warren S. McCulloch과 논리학자인 Walter Pitts는 하나의 사람 뇌 신경세포를 단순 논리 게이트로 설명했습니다. 위 그림에서 **여러 개의 전기적 신호가 가지돌기(Dendrite)에 도착하면 신경세포 내에서 이들을 하나의 신호로 통합하고, 통합된 신호 값이 어떤 임계값(역치)를 초과하면 하나의 단일 신호가 생성되며, 이 신호가 축삭돌기(Axon)를 통해 다른 신경세포로 전달**하는 것으로 이해했습니다. 이렇게 단순화 된 원리로 동작하는 뇌 세포를 McCulloch-Pitts뉴런(MCP 뉴런)이라 부릅니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://t3.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/301t/image/XcA97WckMIJnlE3Vrw8puEIAfDo.jpg)\n",
    "<br><br>\n",
    "\n",
    "1957년 코넬 항공 연구소에 근무하던 Frank Rosenblatt은 MCP 뉴런 모델을 기초로 **퍼셉트론(Perceptron) 학습 규칙**이라는 개념을 고안하게 되는데, Rosenblatt은 하나의 MCP 뉴런이 출력신호를 발생할지 안할지 결정하기 위해, MCP 뉴런으로 들어오는 각 입력값에 곱해지는 가중치 값을 자동적으로 학습하는 알고리즘을 제안했습니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/12.png?raw=true)\n",
    "\n",
    "이 알고리즘은 머신러닝의 지도학습이나 분류(classification)의 맥락에서 볼 때, 하나의 샘플이 어떤 클래스에 속해 있는지 예측하는데 사용될 수 있습니다. 여기서 $x_0$부터 $x_n$은 퍼셉트론 알고리즘으로 입력되는 값이며, $w_0$부터 $w_n$은 각각 입력에 곱해지는 가중치입니다. 입력값은 보통 분류를 위한 데이터의 특성(feature)을 나타내는 값으로 이루어져 있으며, 이 **특성값에 가중치 곱한 값을 모두 더하여 하나의 값으로 만듭**니다. 이 값을 만드는 함수를 순입력 함수(net input 함수)라고 부릅니다. 순입력 함수의 결과값을 특정 임계값과 비교를 하고, 순입력 함수 결과값이 이 임계값보다 크면 1, 그렇지 않으면 -1로 출력하는 함수를 정의합니다. 이 함수를 **활성화 함수(Activation function)라고 부릅니다.**\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/13.png?raw=true)\n",
    "\n",
    "퍼셉트론은 다수의 트레이닝 데이터를 이용하여 일종의 지도 학습을 수행하는 알고리즘입니다. 트레이닝 데이터에는 데이터의 특성값에 대응되는 실제 결과값을 가지고 있어야 합니다. 입력되는 특성값 $x_0$부터 $x_n$까지에 대한 실제 결과값을 y라고 한다면 이 y를 활성 함수에 의해 -1 또는 1로 변환합니다. 이렇게 변환한 값과 퍼셉트론 알고리즘에 의해 예측된 값이 다르면 이 두 개의 값이 같아질 때까지 특정식에 의해 가중치 $w$들을 업데이트 합니다. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. 퍼셉트론의 학습 알고리즘\n",
    "\n",
    "\n",
    "그러면 좀 더 자세하게 살펴보도록 하겠습니다. MCP 뉴런과 Rosenblatt의 퍼셉트론 모델은 사람 뇌의 단일 뉴런이 작동하는 방법을 흉내내기 위해 환원 접근법(redcutionist approach)을 이용합니다. 이는 **초기 가중치를 임의의 값으로 정의하고 예측값의 활성 함수 리턴값과 실제 결과값의 활성 함수 리턴값이 동일하게 나올 때까지 가중치의 값을 계속 수정**하는 방법입니다. <br><br>\n",
    "\n",
    "Rosenblatt의 초기 퍼셉트론 알고리즘을 요약하면 다음과 같습니다.\n",
    "\n",
    "- 가중치 $w$ 값들을 모두 0 또는 작은 값으로 무작위 할당\n",
    "- 임계값을 정의함(보통 0으로 정의합니다.)\n",
    "- 학습 데이터 $x$와 가중치 $w$를 이용해 $\\sum_i w_i x_i$를 계산함\n",
    "- 위에서 계산한 결과가 임계값을 넘으면 1, 넘지못하면 -1 반환\n",
    "- 위에서 나온 리턴값(1, -1)과 정답 비교후 틀리면 $w$업데이트\n",
    "- 업데이트는 $w_i = w_i + \\eta (y - \\hat{y})x_i$로 수행\n",
    "<br><br>\n",
    "\n",
    "퍼셉트론 업데이트 식이 등장한 이후로 우리는 컴퓨터에게 무언가를 학습할 수 있다는 기대를 할 수 있었습니다. 이 식은 나중에 우리가 배운 Gradient Descent 알고리즘으로 발전되게 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3. AND 게이트 직접 학습해보기\n",
    "\n",
    "컴퓨터 없이 손으로 계산해서 AND게이트를 학습시켜봅시다. AND 게이트는 아래와 같이 정의됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/14.png?raw=true)\n",
    "\n",
    "우리는 퍼셉트론 알고리즘에서 보통 0 또는 매우 작은 값을 가중치 초기값으로 준다고 했는데, 우선은 모두 0으로 놓습니다. 그리고 러닝레이트 $\\eta$는 0.1로, 임게값은 0으로 정의하고 순입력($\\sum_i w_i x_i$)가 0보다 크면 1, 그렇지 않으면 -이라고 정의합니다. \n",
    "<br><br>\n",
    "\n",
    "- 1. 트레이닝 데이터1\n",
    "\n",
    "트레이닝 데이터1에 대해 순입력 함수 리턴값을 계산합니다. \n",
    "\n",
    "$w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2$<br>\n",
    "$=0.0  \\cdot 1 + 0.0 \\cdot 0 + 0.0 \\cdot 0 = 0$\n",
    "\n",
    "이 때 항상 1로 존재하는 $x_0$은 bias 값으로 쓰입니다. ($y = wx + b$에서 $b$와 같은 역할) 순입력 함수의 리턴값(0)과 임계값(0)을 비교하면 리턴값이 임계값보다 크지는 않기 때문에 정답을 맞췄고, 업데이트는 일어나지 않습니다.\n",
    "<br><br>\n",
    "\n",
    "- 2. 트레이닝 데이터2\n",
    "\n",
    "마찬가지로 업데이트는 일어나지 않습니다.\n",
    "<br><br>\n",
    "\n",
    "- 3. 트레이닝 데이터3\n",
    "\n",
    "마찬가지로 업데이트는 일어나지 않습니다.\n",
    "<br><br>\n",
    "\n",
    "- 4. 트레이닝 데이터4\n",
    "\n",
    "퍼셉트론 예측값의 활성 함수 리턴값은 -1로 나왔지만 실제 결과값의 활성 함수 리턴값은 1이므로 예측값과 결과값이 다릅니다. 따라서 아래의 식에 의해 가중치를 다시 계산합니다.\n",
    "\n",
    "```python\n",
    "w0 = w0 + 0.1(1-(-1))1 = 0 + 0.2 = 0.2\n",
    "w1 = w1 + 0.1(1-(-1))1 = 0 + 0.2 = 0.2\n",
    "w2 = w2 + 0.1(1-(-1))1 = 0 + 0.2 = 0.2\n",
    "```\n",
    "\n",
    "업데이트된 $w$들로 새롭게 학습을 시작합니다.\n",
    "<br><br>\n",
    "\n",
    "이렇게 계속 계산하다보면 $w_0 = -0.4$, $w_1 = 0.4$, $w_2 = 0.2$ 일 때, 모든 트레이닝 데이터에 대한 예측값과 결과값의 활성 함수 리턴값이 같아집니다. 이와 같은 퍼셉트론 알고리즘은 다음과 같은 파이썬 코드로 구현됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, thresholds=0.0, eta=0.01, n_iter=10):\n",
    "        self.thresholds = thresholds\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.w_ = np.zeros(1+X, shape[1])\n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            errors = []\n",
    "            \n",
    "            for x_i, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(x_i))\n",
    "                self.w_[1:] += update * x_i # weight\n",
    "                self.w_[0] += update * 1 # bias\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "            print(self.w_)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return X @ self.w_[1:] + self.w_[0] # bias \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return 1 if self.net_input(X) > self.thresholds else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
