{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 회귀(linear regression) 알고리즘 이란?\n",
    "\n",
    "이번에 알려드릴 선형 회귀(linear regression) 알고리즘은 머신 러닝에 기초가 되는 회귀 알고리즘입니다. 선형 회귀 알고리즘은 데이터 샘플에 맞는 **최적의 선형 함수**를 구하고 이를 통해 (데이터 특성으로) 예측값을 산출하는 알고리즘입니다. 선형 회귀는 **\"우리가 예측하고자 하는 데이터는 선형적인 특성을 갖고 있다. 즉, 선형 모델($H(x_i) = Wx_i$)에 적합하다.\"** 라는 가정을 갖습니다. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "위의 그림에서 빨간색 점들은 데이터 샘플을 의미하고 이는 오직 1가지 특성($x$)을 가지고 있습니다. 데이터 샘플의 특성은 무조건 1가지 일 필요는 없습니다. 2가지 이상의 특성을 갖을 수 있으며 그 데이터 샘플은 다차원 공간에서 표현됩니다. 파란색 그래프는 **최적의 선형 함수**를 의미하며 이를 통해 예측값($H(x_i)$)을 구합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 **최적의 선형 함수**를 어떻게 구할 수 있는지 알아봅시다.\n",
    "**최적의 선형 함수**는 데이터 샘플의 패턴과 매우 흡사해야 합니다. 그렇기 위해서는 데이터 샘플과 선형 함수간의 거리가 최소어야 합니다.\n",
    "즉, 데이터 샘플의 타겟값($y_i$)과 데이터 샘플의 함수값($H(x_i)$)의 차가 최소로 하는 함수가 **최적의 선형 함수**입나다. $y_i$와 $H(x_i)$의 차를 식을 다음과 같이 나타낼 수 있습니다.\n",
    "\n",
    "$Cost Function = \\frac{1}{m}\\sum_{i = 1}^N (H(x_i)-y_i)^2$\n",
    "\n",
    "이 식을 선형 회귀의 손실 함수(Cost Function)라 하고 이 값이 최소가 되는 함수가 **최적의 선형 함수**입니다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 선형 회귀의 파라미터: 기울기($W$)\n",
    "선형 회귀의 파리미터는 기울기($W$) 한 가지입니다. 이를 이해해봅시다.\n",
    "선형 함수의 모양은 오직 기울기($W$)에만 영향을 받기에 오직 기울기만이 선형 회귀의 파라미터입니다. 위에서 설명한 것처럼 선형 함수는 오직 기울기에 의해서만 모양이 바뀌기에 **최적의 선형 함수를 구한다**와 **최적의 $W$를 구한다**는 동치가 됩니다. 이제 **최적의 $W$**를 어떻게 구할 수 있는 알아봅시다. $W$가 **최적의 $W$**가 되는 지점은 손실 함수값이 0이 되는 지점입니다. 다른 말로 하자면 (손실 함수는 $W$를 독립변수로 갖는 2차함수이기에) 미분값이 0인 지점을 의미하기도 합니다. 손실 함수의 미분값이 양수($+$)일 시 $W$가 마이너스($-$)쪽으로 갈 때 손실 함수값은 작아지고 플러스($+$)으로 갈 때 손실 함수는 커집니다. 반대로 음수($-$)일 시 $W$가 플러스($+$)쪽으로 갈 때 손실 함수값은 작아지고 마이너스($-$)으로 갈 때 손실 함수는 커집니다. 그러므로 **최적의 $W$**를 구하는 알고리즘은 다음과 같습니다. 마지막으로 손실 함수의 미분값이 0과 (개발자가 직접 정한)가까운 숫자($\\epsilon$)와 작거나 같을 때 학습이 종료됩니다.\n",
    "\n",
    "$W : W - \\alpha\\frac{\\partial}{\\partial W}cost(W)$ ($\\alpha = $*learining rate*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 선형 회귀의 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 선형 회귀의 장점\n",
    "- 선형 회귀의 원리 즉, 알고리즘을 쉽게 이해할 수 있다.\n",
    "- 선형적인 데이터 셋이 있으면 이를 큰 효과를 볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 선형 회귀의 단점\n",
    "- 비정상적인 데이터가 존재할 시 큰 영향을 받는다.\n",
    "- 비선형적인 데이터 셋이 있으면 이는 큰 효과를 볼 수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 로지스틱 회귀(logistic regression) 알고리즘 이란?\n",
    "이번에 알려드릴 로지스틱 회귀(logistic regression) 알고리즘은 선형 회귀 다음으로 간단한 분류, 회귀 알고리즘입니다. 로지스틱 회귀 알고리즘은 데이터 샘플에 맞는 최적의 로지스틱 함수를 구하고 이를 통해 (데이터 특성으로) 예측값을 추출하는 알고리즘입니다. 선형 회귀와 함수 모양이 다를 뿐 원리는 비슷하다고 볼 수 있습니다. 선형 회귀와 비슷하게 로지스틱 회귀 또한 **\"우리가 예측하고자 하는 데이터는 로지스틱 모델($H(x_i) = \\frac{1}{(1 + e^-WX)}$)에 적합하다.\"** 라는 가정을 갖습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "위의 그림에서 흰 색 점들은 데이터 샘플을 의미하고 이는 오직 1가지 특성($x_i$)을 가지고 있습니다. 오른쪽 그래프는 **최적의 로지스틱 함수**를 의미하며 이를 통해 예측값($H(x_i)$)을 구합니다. \n",
    "선형 회귀 사용할 때 비정상적인 데이터 샘플이 있을 시 선형 회귀는 이 데이터 샘플에 영향을 크게 받아 그림과 같이 데이터 샘플을 완벽하게 구분하지 못하지만 로지스틱 회귀는 함수 모양 자체가 계단 모양이기에 이러한 데이터 샘플이 있을 때도 샘플을 완벽하게 구분하는 모습을 그림에서와 같이 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 **최적의 로지스틱 함수**를 어떻게 구할 수 있는지 알아봅시다.\n",
    "**최적의 로지스틱 함수** 또한 선형 회귀와 같이 데이터 샘플의 패턴과 매우 흡사해야 합니다. 즉, 데이터 샘플과 로지스틱 함수간의 거리가 최소어야 합니다.\n",
    "즉, 데이터 샘플의 타겟값($y_i$)와 데이터 샘플의 함수값($H(x_i)$)의 차가 최소로 하는 함수가 **최적의 로지스틱 함수**이다. 그러므로 손실 함수(Cost Function)를 다음과 같다고 예상할 것이다. (하지만 실제로 이와 같지 않다. 그 이유는 뒤에 나온다.)\n",
    "\n",
    "$Cost Function = \\frac{1}{m}\\sum_{i = 1}^N (\\frac{1}{(1 + e^-Wx_i)}-y_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 로지스틱 회귀의 파라미터: 기울기($W$)\n",
    "로지스틱 회귀의 파리미터는 $W$ 한 가지입니다. 이를 이해해봅시다.\n",
    "로지스틱 함수의 모양은 오직 $W$에만 영향을 받기에 오직 $W$만이 로지스틱 회귀의 파라미터이다. 위에서 설명한 것처럼 로지스틱 함수는 오직  $W$에서만 모양이 바뀌기에 **최적의 로지스틱 함수를 구한다**와 **최적의 $W$를 구한다**는 동치가 됩니다. 이제 **최적의 $W$**를 어떻게 구할 수 있는 알아봅시다. $W$가 **최적의 $W$**가 되는 지점은 손실 함수값이 0이 되는 지점입니다. 다른 말로 하자면 (손실 함수는 $W$를 독립변수로 갖는 2차함수이기에) 미분값이 0인 지점을 의미하기도 합니다. 하지만 미분값이 0이라고 해서 무조건 최적의 $W$가 되는 것은 아닙니다. 왜냐하면 손실 함수는 울퉁불퉁하여 미분값이 0인 곳이 최소값 지점 이외에도 수많이 존재할 수 있기 때문입니다. 그렇기 때문에 선형 회귀와는 다른 손실함수를 사용해야 합니다. 그것은 다음과 같습니다\n",
    "\n",
    "$H(x) = \\frac{1}{1 + e^-Wx_i}$\n",
    "\n",
    "$C(H(x), y)=\n",
    "\\begin{cases}\n",
    "-log(H(x)), & \\mbox{if }n\\mbox{ y == 1} \\\\\n",
    "-log(1-H(x)), & \\mbox{if }n\\mbox{ y == 0}\n",
    "\\end{cases}$\n",
    "\n",
    "$Cost Function = \\frac{1}{m}\\sum_{i = 1}^N (C(H(x_i), y_i))^2$\n",
    "\n",
    "다음과 같은 손실 함수는 로지스틱 함수 즉, 지수 함수와 반대 성향을 갖고 있는 로그 함수를 넣으므로써 손실 함수를 울퉁불퉁하지 않고 매끄럽게 만들어질 가능성이 큽니다. 즉, 미분값이 0인 지점이 최소값일 가능성이 높습니다. 이 뿐만 아니라 데이터 샘플의 타겟 값이 1(혹은 0)일 때 데이터 샘플의 함수 값($H(X)$)이 0(혹은 1)이면 손실 함수값은 무한으로 발산하며 데이터 샘플의 함수값이 1(혹은 0)라면 손실 함수값이 0으로 수렴하여 데이터 샘플의 특성을 잘 학습할 수 있습니다.그러므로 **최적의 $W$**를 구하는 알고리즘은 다음과 같습니다. 마지막으로 손실 함수의 미분값이 0과 (계발자가 직접 정한)가까운 숫자($\\epsilon$)와 작거나 같을 때 학습이 종료됩니다.\n",
    "\n",
    "\n",
    "\n",
    "$W : W - \\alpha\\frac{\\partial}{\\partial W}Cost(H(x), y)$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 로지스틱 회귀 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 로지스틱 회귀 장점\n",
    "- 로지스틱 회귀의 원리 즉, 알고리즘을 쉽게 이해할 수 있다.\n",
    "- 선형 회귀보다 더 큰 효과를 기대해 볼 수 있다. 즉, 비정상적인 데이터 샘플에 큰 영향을 받지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 로지스틱 회귀 단점\n",
    "- 복잡한 문제를 해결하는데 큰 어려움을 겪는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
