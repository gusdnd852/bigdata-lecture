{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가방법 및 오버피팅\n",
    "> 머신러닝 모델을 평가하는 방법과 오버피팅에 대해 알아봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 4]\n",
    "- permalink: /model_evaluation_overfitting\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### 1. 모델 평가방법과 오버피팅이란?\n",
    "\n",
    "![](img/bao_0.png)\n",
    "\n",
    "지난 시간까지 우리는 다양한 모델에 대해 학습하였고 모델의 결과물에 대해 Accuracy를 사용하여 평가하였습니다. 하지만 정말 모든 경우에서 Accuracy를 사용하는 것이 정답일까요?<br>\n",
    "이번 시간에는 Accuracy를 비롯한 Precision, Recall, F1 Score등의 모델 평가방법에 대해 알아보고 오버피팅이 무엇인지에 대해 학습하도록 하겠습니다.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Accuracy\n",
    "\n",
    "Accuracy는 모델이 예측하여 맞은 개수를 전체 개수로 나눈 값입니다.\n",
    "\n",
    "예를 들어, 양성과 음성이 각각 500명인 질병 환자에 대해 감염 여부를 예측하는 모델을 실행하였을때 아래와 같은 결과가 나왔다고 가정해봅시다.\n",
    "\n",
    "- 예측 #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>390</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>10</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos  Neg\n",
       "Pos  390  110\n",
       "Neg   10  490"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "import pandas as pd\n",
    "\n",
    "case1_df = pd.DataFrame(\n",
    "    columns=['Pos', 'Neg'],\n",
    "    index=['Pos', 'Neg'],\n",
    "    data=[[390, 110],\n",
    "          [10, 490]]\n",
    ")\n",
    "\n",
    "case1_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 결과가 양성 400명, 음성 600명으로 예측하였고, Accuracy는 (390+490)/1000 = 0.88이 됩니다.\n",
    "\n",
    "하지만 다음과 같은 모델은 어떨까요?\n",
    "\n",
    "- 예측 #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos  Neg\n",
       "Pos    0  100\n",
       "Neg    0  900"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "import pandas as pd\n",
    "\n",
    "case2_df= pd.DataFrame(\n",
    "    columns=['Pos', 'Neg'],\n",
    "    index=['Pos', 'Neg'],\n",
    "    data=[[0, 100],\n",
    "          [0, 900]]\n",
    ")\n",
    "\n",
    "case2_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일하게 1000명을 대상으로 예측을 진행했지만, 이번에는 실제 양성의 수가 100명, 음성의 수가 900명입니다.\n",
    "모델의 예측 결과가 1000명 모두 음성으로 판단하였기 때문에 Accuracy는 900/1000 = 0.90이 됩니다.\n",
    "\n",
    "두 번째 예측이 첫 번째 예측보다 0.02 높은 Accuracy를 기록했습니다. 그러므로, Accuracy에 기반하여 판단한다면 두 번째 모델이 더 좋은 모델이라고 판단할 수 있겠습니다.<br>\n",
    "하지만, 두 번째 예측은 양성을 단 한명도 예측해내지 못했습니다. 양성을 단 한명도 예측하지 못하는 모델이 Accuracy가 높다는 것만을 이유로 더 좋다고 판단하는 것이 옳은 선택일까요?<br>\n",
    "이처럼 불균형한 데이터에서는 Accuracy를 사용할 경우 잘못된 성능 예측 결과를 가져올 수 있음을 꼭 기억해야 합니다.<br>\n",
    "\n",
    "문제가 생겼다면 이를 해결할 방법을 찾아야합니다. Accuracy의 문제를 해결하기 위해 사용되는 Precision, Recall과 이를 이용한 F1 Score에 대해 알아보도록 하겠습니다.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision은 모델이 True라고 분류한 항목에서 실제 True인 것의 비율로, 모델의 관점에서 보는 평가 방법입니다.\n",
    "\n",
    "![](img/precision.png)\n",
    "\n",
    "두 번째 모델을 이용하여 Precision을 계산해보도록 하겠습니다.<br>\n",
    "모델이 양성(Pos)이라고 분류한 항목의 합은 0입니다. 그러므로 P(Pos) =  0입니다.<br>\n",
    "모델이 음성(Neg)이라고 분류한 항목의 합은 1000이고, 실제 음성은 900입니다. 그러므로 P(Neg) = 900/(900+100) = 0.90입니다.\n",
    "\n",
    "이제 두 Presicion의 평균을 구하면 두 번째 모델의 Presicion을 알아낼 수 있습니다.<br>\n",
    "모든 항목의 Presicion을 더하고 항목의 개수만큼 나눠주면 됩니다. 두 번째 모델의 Precision은 (0 + 0.9) / 2 = 0.45입니다.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall은 실제 True인 항목에서 모델이 True라고 예측한 것의 비율로, 데이터 관점에서 보는 평가 방법입니다.\n",
    "\n",
    "![](img/recall.png)\n",
    "\n",
    "마찬가지로 두 번째 모델을 이용하여 Recall을 계산해 보겠습니다.<br>\n",
    "실제 양성인 항목에서 모델이 양성으로 예측한 값은 0, 음성으로 예측한 값은 100 입니다. 그러므로 R(Pos) = 0/100 = 0입니다. <br>\n",
    "실제 음성인 항목에서 모델이 음성으로 예측한 값은 900, 양성으로 예측한 값은 0 입니다. 그러므로 R(Neg) = 900/900 = 1입니다.\n",
    "\n",
    "두 Recall의 평균을 구하면 두 번째 모델의 Recall을 알아낼 수 있습니다.<br>\n",
    "Presicion과 마찬가지로 모든 Recall을 더하고 항목의 개수로 나눠주면 됩니다. 두 번째 모델의 Recall은 (0 + 1) / 2 = 0.5입니다.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 구한 Presicion과 Recall을 이용하여 F1 Score를 계산할 수 있습니다.\n",
    "\n",
    "F1 Score를 구하는 식은 2 * (Presicion * Recall) / (Presicion + Recall)입니다.\n",
    "이를 이용하여 두 번째 모델의 F1 Score를 계산하면 2 * (0.45 * 0.5) / (0.45 + 0.5) = 0.473입니다.\n",
    "\n",
    "![](img/evaluation_result.png)<br>\n",
    "두 번째 모델의 평가가 끝났습니다. Accuracy는 0.90이 나왔지만, F1 Score는 0.473이 나왔습니다. 이 모델을 Accuracy만 믿고 사용했다면 부정확한 예측를 대량으로 만들어버리는 좋지 못한 결과를 초래하였을 겁니다.<br>\n",
    "이를 방지하기 위해서는 아래 그림처럼 Accuracy 이외에도 Precision, Recall, F1 Score를 이용한 평가를 진행하여 모델의 성능을 정확하게 알아내야 합니다.\n",
    "\n",
    "![](img/kochat_evaluation.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting은 공통적인 특징을 학습하는 것에 그치지 않고, 더욱 자세한 특징까지 학습하여 나타나는 현상입니다.\n",
    "\n",
    "![](img/bao_1.png)<br>\n",
    "예를 들어, 지폐를 구분하는 모델을 만들고 학습 데이터로 원화, 달러, 엔화를 사용하여 학습을 시켜봅시다.\n",
    "\n",
    "![](img/bao_2.png)<br>\n",
    "모델은 직사각형 사이즈에, 금액이 적혀있고, 일련번호가 적힌 것들을 지폐라고 분류할 수 있게 되었습니다.\n",
    "\n",
    "여기서 그치지 않고 새로운 특징으로 \"사람의 얼굴이 있다\"는 것을 학습시켜 보았습니다. 더 자세한 특징을 학습시켰으니 모델이 더 정확한 판단을 내릴수 있다고 생각할 것입니다.<br>\n",
    "하지만, 아래와 같이 결과는 더 나빠집니다. 사람의 얼굴이 없는 지폐는 지폐가 아니라고 판단해버리게 되었습니다.\n",
    "\n",
    "![](img/bao_3.png)<br>\n",
    "\n",
    "이처럼, 샘플 데이터만 가지고 너무 자세한 특징까지 학습하게 되어 새로운 데이터에 대해 예측하지 못하는 모델을 overfitting 모델이라고 합니다.<br> 이를 해결하기 위한 가장 기본적인 방법은 샘플 데이터를 충분히 사용하여 다양한 테스트 데이터에도 예측할 수 있도록 하는 것이며, 이외에도 Regularization과 Dropout등의 방법을 통해 해결할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
