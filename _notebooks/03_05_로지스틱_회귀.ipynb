{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. 로지스틱 회귀 알고리즘\n",
    "> 가장 기초적인 분류 알고리즘인 로지스틱회귀에 대해 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 3]\n",
    "- permalink: /logistic_regression\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 기초적인 머신러닝 분류 알고리즘인 로지스틱 회귀에 대해 배워보고, 실습을 진행해보겠습니다.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 로지스틱 회귀(logistic regression) 알고리즘 이란?\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99F325485C7B76BC2B)\n",
    "\n",
    "이번에 알려드릴 로지스틱 회귀(logistic regression) 알고리즘은 선형 회귀 다음으로 간단한 분류, 회귀 알고리즘입니다. 로지스틱 회귀 알고리즘은 데이터 샘플에 맞는 최적의 로지스틱 함수를 구하고 이를 통해 (데이터 특성으로) 예측값을 추출하는 알고리즘입니다. 선형 회귀와 함수 모양이 다를 뿐 원리는 비슷하다고 볼 수 있습니다. **이름은 회귀이지만, 본 강의에서는 분류 알고리즘으로 설명**하겠습니다. (원래는 오직 분류목적으로 쓰이는 것이 아니라서 이름이 그런것입니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. 회귀를 해봤으니, 이제 분류를 해보자\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/15.png?raw=true)\n",
    "\n",
    "이전 시간에 선형회귀에 대해 배웠으니 이번엔 분류를 해봅시다. 선형회귀 모델의 출력은 연속형입니다. 선형회귀의 출력값이 전체의 절반을 넘었으면 그냥 반을림해서 출력을 1로, 절반을 못넘겼으면 출력을 0으로 생각해서 분류 할 수도 있어보입니다. 예를 들자면, 위 그림(x:공부시간, y:합격여부)처럼 시험공부를 1시간, 2시간, 3시간한 친구들은 시험에서 떨어졌고, 시험공부를 4시간, 5시간, 6시간 한 친구들은 시험에 붙었다고 합시다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/16.png?raw=true)\n",
    "\n",
    "그러면 이전에 했던것과 비슷하게 선형회귀 모델을 사용해서 직선을 하나 찾고, 직선의 중간쯤을 넘어가면 불합격, 직선의 중간쯤을 넘어가면 합격이라고 예상할 수도 있지 않을까요? \n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/17.png?raw=true)\n",
    "\n",
    "뭔가 그럴싸 해보입니다. 출력이 0.5를 넘어가면 1(합격), 0.5를 넘지 못하면 0(불합격)이라고 생각할 수 있을 것 같습니다. 그러나 여기에는 커다란 문제가 있습니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/18.png?raw=true)\n",
    "\n",
    "만약 빨간 네모칸에 있는 데이터가 입력되었다고 해봅시다. 이 학생은 공부를 상당히 오래했기 때문에 x축의 가장 끝쪽에 위치합니다. 하지만 여기에서 문제가 발생합니다. 이 데이터가 들어오면 선형회귀 직선이 크게 영향을 받게 됩니다. \n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/20.png?raw=true)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/19.png?raw=true)\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/21.png?raw=true)\n",
    "\n",
    "선형회귀 모델은 데이터($y$)와 직선($\\hat{y}$)간의 차이의 **평균**을 최소화 하게 학습했습니다. 때문에 가장 끝쪽에 들어온 데이터는 직선과 차이가 크기 때문에 직선이 직접 옆으로 움직여서 이 데이터와 차이를 좁히려고 할 것입니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/22.png?raw=true)\n",
    "\n",
    "직선이 변했기 때문에, 함수값이 0.5를 지나는 구간도 변경되었습니다. \n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/23.png?raw=true)\n",
    "\n",
    "함수값이 0.5를 지나는 구간이 변경되었기 때문에 이에 다른 데이터들의 정답 여부가 바뀝니다.\n",
    "빨간색 동그라미 친 데이터는 원래 정답으로 맞췄지만 기준선이 지나치게 오른쪽으로 움직여버려서\n",
    "오답처리되었습니다. 즉, 선형회귀의 경우 기본적으로 회귀모델이기 때문에 분류에는 적합하지 않습니다.\n",
    "그러면 어떻게 해야할까요? 우리는 **선형회귀 모델의 출력에 베이즈 정리를 적용**하므로써 \n",
    "같은 모델을 분류모델로 탈바꿈 시킬 수 있습니다.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Hello! 시그모이드(Sigmoid)\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F275BAD4F577B669920)\n",
    "\n",
    "통계시간에 시그모이드 함수는 곧 베이즈 정리라고 배웠습니다. 베이즈 정리는 $P(D|M) = \\frac{P(M|D) P(D)}{P(M)}$입니다. 즉, 모델이 주어졌을때, 해당 데이터를 맞출 확률입니다. 기존에 우리가 배웠던 회귀는 데이터를 맞추는 것이 아니라 데이터의 분포를 흉내냅니다. 그러나 분류는 데이터가 구분되는 시점을 분리내서 데이터가 무엇인지 맞춥니다. 시그모이드를 사용하면 아까와 같이 멀리 떨어진 데이터들도 잘 맞춰낼 수 있습니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/24.png?raw=true)\n",
    "\n",
    "기존에 사용했던 선형회귀 모델의 출력을 그대로 시그모이드 함수의 입력으로 넣으면 0 혹은 1의 값을 출력하게 되고, 이 값으로 예측을 수행하면 됩니다. 기존의 출력은 $y = wx + b$였습니다. 이 출력을 시그모이드 함수 $sigmoid(z)$의 입력으로 넣으면 y = $\\frac{1}{1 + e^{-(wx + b)}}$가 되고, 이 값은 0 또는 1에 가까운 분류값이 됩니다.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cross Entropy Loss 함수\n",
    "\n",
    "선형회귀의 경우는 MSE(Mean Squared Error)라는 Loss 함수를 사용했습니다. MSE는 직선과 데이터의 차이를 제곱한 값의 평균, 즉 분산을 최소화시키는 방향으로 학습했습니다. 그런데 분류모델을 과연 MSE로 학습해도 될까요? \n",
    "<br><br>\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*_P2Y7wi_B7o8MERuabnE9Q@2x.png)\n",
    "\n",
    "위 그림을 봅시다. 우리가 이전에 했던 Regression(회귀)의 경우에는 직선이 데이터를 따라가야합니다. 따라서 직선과 데이터 사이의 차이가 줄어들어야합니다. 따라서 MSE는 이런 경우 꽤나 괜찮은 Loss함수처럼 보입니다. 그런데 분류의 경우는 두 데이터의 사이를 가로질러야합니다. 위 그림만 봐도 직선의 방향이 반대 방향인데 만약 Classification(분류)를 위한 Loss함수로 MSE를 쓰면 아래처럼 수행될 것입니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day3/25.png?raw=true)\n",
    "\n",
    "인공지능이 이렇게 작업을 수행하면 느낌이 맞는 것이 아니라 깡통을 한대 맞아야합니다. MSE를 분류작업에 쓰게 되면 전혀 분류작업을 할 수 없습니다. 따라서 여기에서는 이전에 정보이론 시간에 배웠던 Cross Entropy를 Loss함수로 사용하여 KLD를 최소화시키는 방향으로 모델을 학습시킵니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
