{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 정보이론 기초\n",
    "> 머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 2]\n",
    "- permalink: /information_theory\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝의 학습의핵심 아이디어 중 한가지인 정보이론의 Entropy, Cross Entropy, KL-Divergence 등의 개념에 대해 배우고, One-hot 인코딩 등의 몇가지 팁을 배워봅시다. 정보이론은 원래는 정보를 효율적으로 인코딩하기 위해 고안되었으나 머신러닝/딥러닝 모델을 학습시킬 때, 정답과 예측사이의 차이를 계산할 때 매우 유용한 Tool로 사용됩니다. 매우 간단한 예제로 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### 1. 엔트로피? 무질서?\n",
    "\n",
    "보통 엔트로피라고 하면 열역학에 나오는 엔트로피를 생각합니다. 열역학에서 엔트로피는 무질서도입니다. 정보이론에서도 마찬가지로 엔트로피는 무질서라는 의미를 가지고 있긴합니다만 그 쓰임이 조금 다릅니다. 그 차이에 대해 알아봅시다.\n",
    "<br><br>\n",
    "\n",
    "#### 1.1. 열역학 엔트로피와 정보이론 엔트로피\n",
    "\n",
    "![](http://study.zumst.com/upload/00-d33-00-22-05/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80%20%ED%99%95%EB%A5%A01.png)\n",
    "\n",
    "잘 아시다시피 열역학에서 **엔트로피**는 **무질서**를 의미하고, 계는 시간이 지나면서 자연스레 무질서해집니다. 집에서 청소를 안하면 집이 계속 더러워지는 것 처럼요. 이 것이 바로 열역학 제 2법칙이고, 이에 의해 자연계에서 엔트로피 증가량은 항상 0보다 큽니다. 만약 엔트로피를 감소시켜서 다시 질서있게 만들려면 그만큼의 힘이 듭니다. 우리가 어지럽힐땐 힘들지 않아도 청소할때는 힘든 것 처럼 말이에요.\n",
    "<br><br>\n",
    "\n",
    "그렇다면 이것과 정보와 과연 무슨 관련이 있을까요? **정보이론**에서 엔트로피의 정의는 **정보량의 기댓값(평균)** 을 의미합니다. 정보이론에서도 엔트로피가 높으면 정보가 많다는 것이고, 그러면 정보들이 더 많이 뒤섞이기 때문에 **무질서**한 것입니다. 반면에, 엔트로피가 낮으면 정보의 양이 적다는 것이고 정보들은 덜 뒤섞이기 때문에 덜 무질서 한 것입니다. 또한 정보의 양은 항상 증가하기 때문에 열역학 제 2법칙과도 어느정도 잘 맞습니다.<br><br>\n",
    "\n",
    "가령 단어가 4개를 말할 수 있는 아이와 단어 1000개를 말할 수 있는 어른이 있다고 합시다. 어른이 훨씬 많은 단어를 알고 있기 때문에 훨씬 다양한 대화가 가능할 것이고, 훨씬 다양한 대화를 만들어 낼 수 있습니다. 따라서 정보 입장에서 보면 어른의 대화가 훨씬 무질서하고, 아이의 대화는 매우 단순합니다. <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.2. 정보의 단위 : Bit\n",
    "\n",
    "그렇다면 정보의 양을 어떻게 측정할까요? 한국어와 영어와 독일어 등 모든 언어가 다른데 말이죠. 정보이론을 연구하던 학자들은 여러 언어의 말을 **0 or 1의 단위인 Bit로 변환해서 언어등과 무관하게 정보의 양을 측정**할 수 있었습니다. 그런데 정보를 Bit로 변환할 때 중요한 것이 있습니다. 최소한의 통신으로 최대한 많은 정보를 전송해야하기 때문에 단어를 Bit로 변환할 때, **자주나오는 말은 짧게 변환하고 드물게 나오는 말은 그것보다는 길게 변환**해야했습니다.<br><br>\n",
    "\n",
    "#### 1.2. 카톡 대화를 Bit로 변환하기\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/118.png?raw=true)\n",
    "\n",
    "흔한 연인들의 대화입니다. 대화의 절반이 하트로 이루어진 것을 볼 수 있습니다. 그러다가 실수로 친구에게 보낼 ㅗㅗ을 연인에게 보내게 되었습니다. 대화의 절반정도가 하트이기 때문에 단어를 쓸 때 하트가 나올 확률 $P(♥) = 0.5$이고, 잘못보낸 ㅗㅗ를 보낼 확률은 $P(ㅗㅗ) = 0.001$ 정도 된다고 해봅시다. <br><br>\n",
    "\n",
    "정보의 양을 측정하기 위해 이 두 단어(♥, ㅗㅗ)를 2진수로 변환해야 한다고 해보겠습니다. 예를 들면 최대 4비트 패턴까지 할당할 수 있는데 하는데 만약 자주 나오는 단어인 ♥에 0000을 할당하고, 자주 나오지 않는 단어인 ㅗㅗ에 00을 할당했다고 해봅시다. 그러면 단어 전체를 전송하기 위해 훨씬 많은 비트가 필요해져서 매우 비효율적입니다. 반대로 ♥에 00을 ㅗㅗ에 0000을 할당하는 편이 훨씬 적은 비트로 많은 데이터를 보낼 수 있겠죠.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.3. 정보이론에서 엔트로피란?\n",
    "엔트로피는 위에서 말했듯이 정보량의 기대값입니다. 그리고 그 정보의 양은 Bit로 측정합니다. 우리는 앞선 통계시간에 기대값에 대해 배웠습니다. 기대값은 아래와 같습니다.\n",
    "\n",
    "<br>\n",
    "$$E[x] = \\sum_{i} x_i \\cdot P(x_i)$$\n",
    "<br>\n",
    "\n",
    "평균이 아니라 기대값이여야하는 이유는 **각 단어가 등장할 확률이 모두 다르기 때문**입니다. 만약 단어가 균등분포의 형태로 등장한다면, 평균으로 측정해도 상관이 없지만, 위의 예시처럼 어떤 단어는 자주 등장하고, 어떤 단어는 잘 등장하지 않기 때문에 단어의 등장 확률을 고려하여 기대값으로 평가하는 것이 정확합니다.\n",
    "<br><br>\n",
    "\n",
    "그런데 이 때, 단어는 단순한 숫자 (0, 01, 010 등)이므로 의미가 없습니다. 따라서 **정보의 양을 측정하기 위해서는 몇 비트가 사용되었는지로 평가**해야합니다. 여기에서는 아래와 같은 그래프를 사용합니다.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/ko/thumb/7/71/Entropy_kor.jpg/330px-Entropy_kor.jpg)\n",
    "\n",
    "위 그래프는 $y= -\\log_2 P(x)$ 그래프입니다. 단어가 등장할 확률(y축)이 높다면, 길이(x축)가 짧아지고 단어가 등장할 확률(y축)이 낮다면 길이(x축)이 길어집니다. 따라서 길이에 따라 필요한 Bit의 수를 잘 모델링했다고 할 수 있습니다. 우리는 단어 $x$가 $P(x)$의 확률로 나타날 때, $-\\log_2 P(x)$의 기대값을 구할 것이기 때문에 엔트로피는 아래와 같아집니다.\n",
    "\n",
    "<br>\n",
    "$$Entropy = \\sum_{i} (-\\log_2 P(x_i)) \\cdot P(x_i)$$\n",
    "<br>\n",
    "\n",
    "이는 기대값 $E[x] = \\sum_{i} x_i \\cdot P(x_i)$에서 $x_i$를 $(-\\log_2 P(x))$로 치환한 것입니다. 식을 조금 더 깔끔하게 정리해서 쓰면 아래와 동일합니다.\n",
    "\n",
    "<br>\n",
    "$$Entropy = - \\sum_{i} P(x_i) \\cdot \\log_2 P(x_i) $$\n",
    "<br>\n",
    "\n",
    "이 것이 정보이론에서의 엔트로피에 대한 정의입니다. 즉, 엔트로피는 **정보량의 기댓값(평균)** 을 의미하며 정보량을 측정하기 위해 필요한 Bit의 길이를 $(-\\log_2 P(x))$라고 모델링하여 그 기대값을 측정하는 것을 의미합니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
