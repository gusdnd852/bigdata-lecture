{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 정보이론 기초\n",
    "> 머신러닝에 반드시 필요한 정보이론의 기초 이론을 배웁니다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 2]\n",
    "- permalink: /information_theory\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝의 학습의핵심 아이디어 중 한가지인 정보이론의 Entropy, Cross Entropy, KL-Divergence 등의 개념에 대해 배우고, One-hot 인코딩 등의 몇가지 팁을 배워봅시다. 정보이론은 원래는 정보를 효율적으로 인코딩하기 위해 고안되었으나 머신러닝/딥러닝 모델을 학습시킬 때, 정답과 예측사이의 차이를 계산할 때 매우 유용한 Tool로 사용됩니다. 매우 간단한 예제로 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### 1. 엔트로피? 무질서?\n",
    "\n",
    "보통 엔트로피라고 하면 열역학에 나오는 엔트로피를 생각합니다. 열역학에서 엔트로피는 무질서도입니다. 정보이론에서도 마찬가지로 엔트로피는 무질서라는 의미를 가지고 있긴합니다만 그 쓰임이 조금 다릅니다. 그 차이에 대해 알아봅시다.\n",
    "<br><br>\n",
    "\n",
    "#### 1.1. 열역학 엔트로피와 정보이론 엔트로피\n",
    "\n",
    "![](http://study.zumst.com/upload/00-d33-00-22-05/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80%20%ED%99%95%EB%A5%A01.png)\n",
    "\n",
    "잘 아시다시피 열역학에서 **엔트로피**는 **무질서**를 의미하고, 계는 시간이 지나면서 자연스레 무질서해집니다. 집에서 청소를 안하면 집이 계속 더러워지는 것 처럼요. 이 것이 바로 열역학 제 2법칙이고, 이에 의해 자연계에서 엔트로피 증가량은 항상 0보다 큽니다. 만약 엔트로피를 감소시켜서 다시 질서있게 만들려면 그만큼의 힘이 듭니다. 우리가 어지럽힐땐 힘들지 않아도 청소할때는 힘든 것 처럼 말이에요.\n",
    "<br><br>\n",
    "\n",
    "그렇다면 이것과 정보와 과연 무슨 관련이 있을까요? **정보이론**에서 엔트로피의 정의는 **전체 정보를 표현하는데 필요한 최소 자원량(기대값)**을 의미합니다. 정보이론에서도 엔트로피가 높으면 정보가 많다는 것이고, 그러면 정보들이 더 많이 뒤섞이기 때문에 **무질서**한 것입니다. 반면에, 엔트로피가 낮으면 정보의 양이 적다는 것이고 정보들은 덜 뒤섞이기 때문에 덜 무질서 한 것입니다. 또한 정보의 양은 항상 증가하기 때문에 열역학 제 2법칙과도 어느정도 잘 맞습니다.<br><br>\n",
    "\n",
    "가령 총 4개의 단어를 말할 수 있는 아이와 총 1000개의 단어를 말할 수 있는 어른이 있다고 합시다. 어른이 훨씬 많은 단어를 알고 있기 때문에 훨씬 다양한 대화가 가능할 것이고, 훨씬 다양한 대화를 만들어 낼 수 있습니다. 따라서 정보 입장에서 보면 어른의 대화가 훨씬 무질서하고, 아이의 대화는 매우 단순합니다. <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.2. 정보의 단위 : Bit\n",
    "\n",
    "그렇다면 전체 정보를 표현하기 위한 최소 자원량을 어떻게 측정할까요? 한국어와 영어와 독일어 등 모든 언어가 다른데 말이죠. 정보이론을 연구하던 학자들은 여러 언어의 말을 **0 or 1의 단위인 Bit로 인코딩해서 언어등과 무관하게 정보의 양을 측정**할 수 있었습니다. 그런데 정보를 Bit로 변환할 때 중요한 것이 있습니다. 최소한의 통신으로 최대한 많은 정보를 전송해야하기 때문에 단어를 Bit로 인코딩할 때, **자주나오는 말은 짧게 인코딩하고 드물게 나오는 말은 그것보다는 길게 인코딩**해야했습니다.<br><br>\n",
    "\n",
    "#### 1.2. 예제 : 카톡 대화를 Bit로 변환하기\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/118.png?raw=true)\n",
    "\n",
    "흔한 연인들의 대화입니다. 대화의 절반이 하트로 이루어진 것을 볼 수 있습니다. 대화의 절반정도가 하트이기 때문에 단어를 쓸 때 하트가 나올 확률 $P(♥) = 0.5$이고, ㅗ를 보내면 싸우게 되므로, 자주 안보내기 때문에 $P(ㅗ) = 0.1$ 정도 된다고 해봅시다. <br><br>\n",
    "\n",
    "정보의 양을 측정하기 위해 이 두 단어(♥와 ㅗ)를 2진수로 인코딩해야 한다고 해보겠습니다. 예를 들면 단어 하나를 최소 2비트 ~ 최대 4비트까지 할당할 수 있는데 하는데 만약 자주 나오는 단어인 ♥를 0000로 인코딩하고, 자주 나오지 않는 단어인 ㅗ를 00로 인코딩했다고 해봅시다. 이 커플은 ㅗ보다 ♥를 훨씬 자주보냅니다. ♥를 5배정도 많이 보내기 때문에 ♥를 4비트로 인코딩하면 보낼때 마다 4비트씩 보내야하므로 너무 데이터의 낭비가 심합니다. 그에 비해 자주 쓰이지 않는 ㅗ는 2비트에 할당되어있으니 이 둘을 바꿔서 ㅗ를 4비트로, ♥를 2비트로 인코딩해주는 것이 훨씬 효율적일 것입니다.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.3. 정보이론에서 엔트로피란?\n",
    "엔트로피는 위에서 말했듯이 **전체 정보를 표현하는데 필요한 최소 자원량(기대값)**입니다. 정말 모든 단어들이 최소의 자원량, 최소의 비트만큼만 할당되게 인코딩한다면 정말 좋겠죠. \n",
    "<br><br>\n",
    "\n",
    "단어들이 매우 많으므로 평균적으로 몇비트 정도로 인코딩 되었는지 알기 위해 기대값을 계산합니다. 물론 그 값은 적으면 적을수록 좋겠죠? (평균적으로 단어들의 비트가 짧다는 것) 우리는 앞선 통계시간에 기대값에 대해 배웠습니다. 기대값은 아래와 같습니다.\n",
    "\n",
    "<br>\n",
    "$$E[x] = \\sum_{i} x_i \\cdot P(x_i)$$\n",
    "<br>\n",
    "\n",
    "\n",
    "평균이 아니라 기대값이여야하는 이유는 **각 단어가 등장할 확률이 모두 다르기 때문**입니다. 만약 단어가 균등분포의 형태로 등장한다면, 평균으로 측정해도 상관이 없지만, 위의 예시처럼 어떤 단어는 자주 등장하고, 어떤 단어는 잘 등장하지 않기 때문에 단어의 등장 확률을 고려하여 기대값으로 평가하는 것이 정확합니다.\n",
    "<br><br>\n",
    "\n",
    "그렇다면 **전체 정보를 표현하는데 필요한 최소의 비트수(기대값)**을 갖게끔 단어들을 인코딩하려면 각 단어가 얼마의 길이를 갖게끔 인코딩 해야할까요? 정보이론의 아버지인 클라우드 섀넌는 아래와 같은 그래프를 사용하면 각 단어를 이론상으로 최소 비트 수(단어 길이)로 인코딩 할 수 있다고 했습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/119.png?raw=true)\n",
    "\n",
    "위 그래프는 $y= -\\log_2 P_i$, 즉 등장 확률에 따른 비트 수의 그래프입니다. 여기에서 $P_i$는 $i$번째 단어의 등장 확률을 의미합니다. 단어가 등장할 확률(x축)이 높다면, 길이(y축)를 짧게 인코딩하고, 단어가 등장할 확률(x축)이 낮다면 길이(y축)를 길게 인코딩합니다. 우리는 단어 $x_i$가 $P_i$의 확률로 나타날 때, 비트의 길이인 $-\\log_2 P_i$의 기대값을 계산할 것이기 때문에 엔트로피는 아래와 같아집니다.\n",
    "\n",
    "<br>\n",
    "$$Entropy = \\sum_{i} (-\\log_2 P_i) \\cdot P_i$$\n",
    "<br>\n",
    "\n",
    "이는 기대값 $E[x] = \\sum_{i} x_i \\cdot P_i$에서 $x_i$를 $(-\\log_2 P_i)$로 치환한 것입니다. 식을 조금 더 깔끔하게 정리해서 쓰면 아래와 동일합니다.\n",
    "\n",
    "<br>\n",
    "$$Entropy = - \\sum_{i} P_i \\cdot \\log_2 P_i $$\n",
    "<br>\n",
    "\n",
    "이 것이 정보이론에서의 엔트로피에 대한 정의입니다. 즉, 엔트로피는 **전체 정보를 표현하는데 필요한 최소 자원량(기대값)** 을 의미하며 이론상으로 가장 최소의 비트만을 사용해서 데이터를 전송하는 인코딩 방식입니다. 이러한 엔트로피의 정의는 **우리가 일정량의 데이터를 가지고 있을 때, 어떤 방식으로 인코딩한다고 해도 절대 이 것보다 적은 비트로 인코딩 할수는 없다는 것을 의미**하기도 합니다. 결론적으로 다시 한번 말하자면, 일정한 만큼의 데이터가 있을 때, 각 단어를 가능한 최소의 비트만 써서 인코딩하는 길이가 $ -\\log_2 P_i $라는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. Cross Entropy\n",
    "\n",
    "크로스 엔트로피란 각 단어의 인코딩 (비트)길이를 섀넌의 방식에서 우리의 방식으로 바꿨을 때의 엔트로피를 이야기합니다. 섀넌이 제안한 엔트로피는 말 그대로 **이론상의 최소 비트 수**입니다. 어떻게 인코딩해도 섀넌의 인코딩보다는 비효율적일 수 밖에 없습니다. 기존에 섀넌의 이론에서 제안된 엔트로피는 다음과 같습니다.\n",
    "\n",
    "<br>\n",
    "$$Entropy = - \\sum_{i} P_i \\cdot \\log_2 P_i$$\n",
    "<br>\n",
    "\n",
    "여기에서 우리는 $\\log_2 P_i$가 아니라 $\\log_2 Q_i$를 쓰게 됩니다. ($Q_i$는 실제 단어의 등장 확률이 아니라 우리가 직접 예측한 단어 등장 확률입니다.) 위에서 말했다시피 $\\log_2 P_i$를 사용하여 인코딩 하는 것은 정말 이론적으로 최상의 경우지만, 우리가 실제로 인코딩 할때는 현실적인 제약 등을 고려하여 $Q_i$로 생각하고 인코딩합니다. 그래서 우리가 실질적으로 적용하는 각 단어별 인코딩 길이는 $\\log_2 Q_i$입니다. 따라서, 즉, 섀넌의 방법보다는 비효율적인, 그러나 현실적인 **우리의 가설에 의해 인코딩한 뒤 계산한 Entropy가 바로 CrossEntropy**입니다.\n",
    "\n",
    "<br>\n",
    "$$CrossEntropy = - \\sum_{i} P_i \\cdot \\log_2 Q_i$$\n",
    "<br>\n",
    "\n",
    "이론상의 최소 비트수인 Entropy와 현실적으로 적용한 우리의 비트 할당량은 Cross Entropy. 이 두 개념 확실히 이해 가셨나요?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3. KL (Kullback–Leibler) Divergence\n",
    "\n",
    "정보이론에서 쿨백 라이블러 발산(KL-Divergence)은 크로스 엔트로피와 엔트로피의 차이입니다. 즉, **우리가 직접 인코딩한 방식(크로스엔트로피)이 이론상의 최소 비트 인코딩 방식(엔트로피)에 비해 어느정도의 차이가 있는지**를 나타냅니다. \n",
    "\n",
    "<br>\n",
    "$$KLD = CrossEntropy - Entropy$$\n",
    "<br>\n",
    "$$KLD = - \\sum_{i} P_i \\cdot \\log_2 Q_i - \\sum_{i} P_i \\cdot \\log_2 P_i$$\n",
    "<br>\n",
    "$$KLD = - \\sum_{i} P_i \\cdot \\log_2 \\frac{Q_i}{P_i}$$\n",
    "<br>\n",
    "\n",
    "즉, KL Divergence는 이론상 최소 비트을 할당하는 방식에 비해 우리의 방식이 얼마나 못하는지를 나타냅니다. 둘 사이의 차이가 작으면 거의 완벽하게 해낸 것이고, 둘 사이가 크면 최소 방식에 성능이 별로 좋지 못한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4. 딥러닝에 적용하기\n",
    "\n",
    "그래서 이걸 왜 공부한걸까요? 로지스틱 회귀모형(회귀이지만 분류모델입니다)과 거의 대부분의 딥러닝 분류(Classification) 알고리즘은 현재 $KLD$를 최소화하는 방식으로 학습하고 있으며, 거의 대부분의 트리기반의 머신러닝 알고리즘은 Entropy로 정보의 양을 비교하여 트리를 설계합니다. 대부분 머신러닝 알고리즘 학습의 주축이기 때문에 소개한 것입니다. (이후 수업때 계속 써먹어야 해서 설명한거긴 합니다)<br><br>\n",
    "\n",
    "분류모델 학습시에 우리의 데이터에는 정답열이 있습니다. 이러한 정답열은 섀넌의 Entropy에 해당합니다. 전부 맞추면 100%의 성능을 보이겠지만 이론상으로만 가능하고 현실적으로 불가능합니다. 그에 비해 우리의 예측은 CrossEntropy입니다. 정답과는 다르지만 나름 잘 모델링해서 만든 예측입니다. <br><br>\n",
    "\n",
    "그리고 $KLD$는 정답데이터와 모델 예측간의 차이입니다. 만약 $KLD$가 크다면 모델의 예측과 실제 정답의 차이가 큰 것으로 모델을 더 강하게 학습시켜야하고, 만약 $KLD$가 작다면 모델의 예측과 실제 정답의 차이가 작은 것으로 모델을 더 약하게 학습시켜야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4.1. One - Hot 인코딩\n",
    "그렇다면 어떻게 $KLD$를 적용할까요? 분류데이터에는 구체적으로 나뉘어진 정답열이 있기 때문에 정답열에서 모델의 예측을 빼면 $KLD$가 됩니다.\n",
    "(사실은 그냥 빼는 것이 아니라 log를 적용해서 계산해야하지만, 정답 - 예측이라는 컨셉을 보여주기 위해 그냥 뺄셈으로 계산하겠습니다.) <br><br>\n",
    "\n",
    "우리가 딥러닝 혹은 로지스틱모형등의 분류모델을 쓰면 최종적인 출력값이 클래스의 갯수로 출력됩니다. 이해가 잘 안갈테니 예를 들어 보여드리겠습니다.\n",
    "만약 생체 정보를 바탕으로 질병을 나타내는 데이터가 있다고 가정합시다. 우리의 데이터셋은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>혈압</th>\n",
       "      <th>몸무게</th>\n",
       "      <th>혈중 콜레스테롤 농도</th>\n",
       "      <th>질병(라벨)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>68</td>\n",
       "      <td>180</td>\n",
       "      <td>정상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>82</td>\n",
       "      <td>216</td>\n",
       "      <td>당뇨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122</td>\n",
       "      <td>55</td>\n",
       "      <td>160</td>\n",
       "      <td>정상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133</td>\n",
       "      <td>82</td>\n",
       "      <td>220</td>\n",
       "      <td>고혈압</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>83</td>\n",
       "      <td>220</td>\n",
       "      <td>당뇨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118</td>\n",
       "      <td>58</td>\n",
       "      <td>166</td>\n",
       "      <td>정상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>143</td>\n",
       "      <td>86</td>\n",
       "      <td>220</td>\n",
       "      <td>고혈압</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>151</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "      <td>심장병</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    혈압  몸무게  혈중 콜레스테롤 농도 질병(라벨)\n",
       "0  120   68          180     정상\n",
       "1  144   82          216     당뇨\n",
       "2  122   55          160     정상\n",
       "3  133   82          220    고혈압\n",
       "4  133   83          220     당뇨\n",
       "5  118   58          166     정상\n",
       "6  143   86          220    고혈압\n",
       "7  151   40          200    심장병"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.DataFrame(\n",
    "    columns=['혈압', '몸무게', '혈중 콜레스테롤 농도', '질병(라벨)'],\n",
    "\n",
    "    data=[\n",
    "        [120, 68, 180, '정상'], \n",
    "        [144, 82, 216, '당뇨'],\n",
    "        [122, 55, 160, '정상'],\n",
    "        [133, 82, 220, '고혈압'], \n",
    "        [133, 83, 220, '당뇨'], \n",
    "        [118, 58, 166, '정상'], \n",
    "        [143, 86, 220, '고혈압'],\n",
    "        [151, 40, 200, '심장병'],  \n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴퓨터는 자연어를 모릅니다. 때문에 질병(라벨)을 숫자로 맵핑해줍니다.\n",
    "정상은 0번 질병, 당뇨는 1번 질병, 고혈압은 2번 질병, 심장병은 3번 질병이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>혈압</th>\n",
       "      <th>몸무게</th>\n",
       "      <th>혈중 콜레스테롤 농도</th>\n",
       "      <th>질병(라벨)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>68</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>82</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122</td>\n",
       "      <td>55</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133</td>\n",
       "      <td>82</td>\n",
       "      <td>220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>83</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118</td>\n",
       "      <td>58</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>143</td>\n",
       "      <td>86</td>\n",
       "      <td>220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>151</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    혈압  몸무게  혈중 콜레스테롤 농도  질병(라벨)\n",
       "0  120   68          180       0\n",
       "1  144   82          216       1\n",
       "2  122   55          160       0\n",
       "3  133   82          220       2\n",
       "4  133   83          220       1\n",
       "5  118   58          166       0\n",
       "6  143   86          220       2\n",
       "7  151   40          200       3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "label_map = {'정상':0, '당뇨':1, '고혈압':2, '심장병':3}\n",
    "dataset['질병(라벨)'] = dataset['질병(라벨)'].map(label_map)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "그러면 데이터 샘플들은 다음과 같은 라벨 세트를 가지게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 2, 1, 0, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "dataset['질병(라벨)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "실제 정답이 위와 같을 때, 우리의 예측 정답이 다음과 같았다고 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 0, 1, 2, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.flip(dataset['질병(라벨)'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "이 때, 실제 정답에서 예측값을 빼서 KLD를 구해보면 아래처럼 됩니다. (위에서 말한대로 실제로는 log를 이용해 계산해야하지만 간단하게 정답 - 예측이라는 컨셉을 보여드리기 위해 그냥 뺄셈으로 연산하였습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "predict =  np.flip(dataset['질병(라벨)'].values)\n",
    "label = dataset['질병(라벨)'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1,  0, -1,  1,  0, -1, -3], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict - label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "만약 정답을 맞췄다면 같은 라벨끼리 (e.g. 2번 질병 - 2번 질병 = 0) 뺄셈을 수행하기 때문에 결과는 0일것입니다. 그런데 **문제는 오답**에 있습니다.\n",
    "어떤 질병은 차이가 -3이고 어떤질병은 차이가 3이고 어떤 질병은 차이가 1입니다. 이런문제는 다음과 같은 이유로 생깁니다. \n",
    "\n",
    "- (정답 case) 정답 1번 / 예측 1번 : '정답(1) - 예측(1)' = 0\n",
    "- (오답 case) 정답 0번 / 예측 3번 : '정답(0) - 예측(3)' = -3\n",
    "- (오답 case) 정답 2번 / 예측 1번 : '정답(2) - 예측(1)' = -1 \n",
    "\n",
    "<br>\n",
    "\n",
    "무언가 잘못된 것 같습니다. 차이가 3이 나는 질병이라면 더 크게 틀린것이고, 차이가 1이 나는 질병은 더 적게 틀린것일까요? \n",
    "절대 그렇지 않습니다. 예측한 오답 중에 **더 많이 틀리고, 덜 틀리고 한 것**은 없습니다. \n",
    "이러한 문제를 해결하기 위해서 우리는 One-Hot 인코딩 방법을 기본으로 사용합니다.\n",
    "One-Hot 인코딩은 각 라벨을 다음과 같이 변경합니다.\n",
    "\n",
    "- 0 : [1, 0, 0, 0]\n",
    "- 1 : [0, 1, 0, 0]\n",
    "- 2 : [0, 0, 1, 0]\n",
    "- 3 : [0, 0, 0, 1]\n",
    "\n",
    "<br>\n",
    "\n",
    "One-Hot 인코딩을 적용하면 아래와 같은 결과를 확인할 수 있습니다.\n",
    "\n",
    "- (정답 case) 정답 1번 / 예측 1번 : '[0, 1, 0, 0] - [0, 1, 0, 0]' = [0, 0, 0, 0]\n",
    "- (오답 case) 정답 0번 / 예측 3번 : '[1, 0, 0, 0] - [0, 0, 0, 1]' = [1, 0, 0, -1]\n",
    "- (오답 case) 정답 2번 / 예측 1번 : '[0, 0, 1, 0] - [0, 1, 0, 0]' = [0, -1, 1, 0]\n",
    "\n",
    "<br>\n",
    "\n",
    "정답인 경우 결과가 [0, 0, 0, 0]이 되지만, 오답의 경우 리스트에 1과 -1이 반드시 생깁니다. 그래서 이 리스트 원소들을 모두 제곱해서 음수를 없애주면 아래와 같습니다.\n",
    "\n",
    "- (오답 case) 정답 0번 / 예측 3번 : [1, 0, 0, -1]$^2$ → [1, 0, 0, 1]\n",
    "- (오답 case) 정답 2번 / 예측 1번 : [0, -1, 1, 0]$^2$ → [0, 1, 1, 0]\n",
    "\n",
    "<br>\n",
    "\n",
    "그리고 나서 리스트의 원소 합을 모두 더해주면 반드시 2가 됩니다.\n",
    "\n",
    "- (오답 case) 정답 0번 / 예측 3번 : [1, 0, 0, 1] → 2\n",
    "- (오답 case) 정답 2번 / 예측 1번 : [0, 1, 1, 0] → 2\n",
    "\n",
    "<br>\n",
    "\n",
    "이런 방식으로 '정답 - 예측'을 구하면 정답인 경우 합이 0, 오답인 경우 합이 반드시 2가 나오게 되고, 어떻게 틀리던지 출력값이 동일하게 됩니다.\n",
    "(실제 출력은 log값을 사용하기 때문에 2가 나오진 않습니다만, 어떻게 틀리던지간에 오답이라면 동일한 출력값이 나오게 됩니다.)\n",
    "Tensorflow, Sklearn 등의 유명 머신러닝, 딥러닝 패키지의 분류 모델들은 기본적으로 이러한 One hot 인코딩 기능이 자동으로 구현되어있습니다. \n",
    "따라서 사용자가 직접 인코딩 하지 않아도 무방하지만, 태스크에 따라서는 직접 One-Hot 인코딩을 해야하는 경우도 존재하니 잘 숙지하시길 바랍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
