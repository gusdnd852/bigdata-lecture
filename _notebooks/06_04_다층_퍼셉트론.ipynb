{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 다층 퍼셉트론\n",
    "> 인공신경망 알고리즘인 다층 퍼셉트론에 대해 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 6]\n",
    "- permalink: /mlp\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1. 다층 퍼셉트론\n",
    "\n",
    "퍼셉트론에서 결과값을 내놓는 부분은 결국 활성 함수(activation function)인데, 단층 퍼셉트론에서는 이 활성 함수가 1개밖에 없는 구조입니다. 아래 그림에서 출력층의 활성 함수를 a로 표시했습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/15.png?raw=true)\n",
    " \n",
    "단층 퍼셉트론은 한계가 있는데, 비선형적으로 분리되는 데이터에 대해서는 제대로 된 학습이 불가능하다는 것입니다. 예를 들면 단층 퍼셉트론으로 AND연산에 대해서는 학습이 가능하지만, XOR에 대해서는 학습이 불가능하다는 것이 증명되었습니다.\n",
    "<br><br>\n",
    "\n",
    "이를 극복하기 위한 방안으로 입력층과 출력층 사이에 하나 이상의 중간층을 두어 비선형적으로 분리되는 데이터에 대해서도 학습이 가능하도록 다층 퍼셉트론(줄여서 MLP)이 고안되었습니다. 아래 그림은 다층 퍼셉트론의 구조의 한 예를 보인 것입니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/16.png?raw=true)\n",
    "\n",
    "입력층과 출력층 사이에 존재하는 중간층을 숨어 있는 층이라 해서 은닉층이라 부릅니다. 입력층과 출력층 사이에 여러개의 은닉층이 있는 인공 신경망을 심층 신경망(deep neural network)이라 부르며, 심층 신경망을 학습하기 위해 고안된 특별한 알고리즘들을 딥러닝(deep learning)이라 부릅니다. 따라서 딥러닝을 제대로 이해하기 위해서는 심층 신경망의 가장 기초적인 다층 퍼셉트론에 대해 제대로 알고 있어야 합니다.\n",
    "<br><br>\n",
    "\n",
    "다층 퍼셉트론에서는 입력층에서 전달되는 값이 은닉층의 모든 노드로 전달되며 은닉층의 모든 노드의 출력값 역시 출력층의 모든 노드로 전달됩니다. 이런 형식으로 값이 전달되는 것을 **순전파(feedforward)**라 합니다. 입력층과 은닉층에 있는 한개의 노드만 볼 때, 하나의 단층 퍼셉트론으로 생각할 수 있습니다. 따라서 은닉층에 있는 각각의 노드는 퍼셉트론의 활성 함수라고 볼 수 있습니다. 은닉층에 있는 각 노드에 이름을 $a_n$으로 이름을 붙여서 그림을 그려보면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/17.png?raw=true)\n",
    "\n",
    "마찬가지로 은닉층과 츨력층에 있는 한개의 노드만 고려해보면 이 역시 하나의 단층 퍼셉트론이며 출력층의 각 노드에 $A_n$로 이름을 붙여서 그림을 그려보면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/18.png?raw=true)\n",
    "\n",
    "은닉층과 출력층의 노드에 이름 붙여진 $a_n$과 $A_n$를 소문자 $a$로 통일되게끔 표현하기 위해 위첨자와 아래첨자를 도입해 봅니다. 입력층, 은닉층, 출력층은 각각 1번째, 2번째, 3번째 층이므로 a의 위첨자에 층을 표시하도록 해봅니다. 또한 은닉층에도 입력층에서와 같이 바이어스 $a_0$를 추가하여 다층 퍼셉트론을 표현하면 아래 그림과 같이 됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/21.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 다층 퍼셉트론이 동작하는 원리에 대해 살펴봅니다. 우리는 단층 퍼셉트론이 동작하는 원리는 다 알고 있습니다. 단층 퍼셉트론은 활성 함수가 내놓는 결과값이 실제값과 오류가 최소가 되도록 입력층에서 전달되는 값들에 곱해지는 가중치의 값을 결정하는 것입니다. <br><br>\n",
    "\n",
    "다층 퍼셉트론의 동작 원리 역시 단층 퍼셉트론의 동작 원리와 크게 다를 것이 없습니다. 차이점은 단층 퍼셉트론은 활성 함수가 1개인 반면, 다층 퍼셉트론은 은닉층과 출력층에 존재하는 활성 함수가 여러개이고, 이에 따른 가중치도 여러개인 것이지요. 다층 퍼셉트론은 아래와 같이 동작합니다. <br><br>\n",
    "\n",
    "- 각 층에서의 가중치(w)를 임의의 값(보통 0으로 설정함)으로 설정합니다. 각 층에서 bias 값은 1로 설정합니다.\n",
    "- 하나의 트레이닝 데이터에 대해서 각 층에서의 순입력 함수값을 계산하고 최종적으로 활성 함수에 의한 출력값을 계산합니다.\n",
    "- 출력층의 활성 함수에 의한 결과값과 실제값이 허용 오차 이내가 되도록 각층에서의 가중치를 업데이트합니다.\n",
    "- 모든 트레이닝 데이터에 대해서 출력층의 활성 함수에 의한 결과값과 실제값이 허용 오차 이내가 되면 학습을 종료합니다.\n",
    "<br><br>\n",
    "\n",
    "그런데, 여기서 한가지 어려움이 있습니다. 단층 퍼셉트론에서는 은닉층이 존재하지 않고, 입력층과 출력층만 존재하기 때문에 출력층의 결과값을 우리가 확보한 실제값과 비교하여 오차가 최소가 되도록 가중치를 업데이트하고 결정하면 됩니다. 하지만 다층 퍼셉트론에서는 입력층과 출력층 사이에 은닉층이 존재하고, 은닉층의 출력값에 대한 기준값을 정의할 수 없습니다. 은닉층에서 어떤 값이 출력되어야 맞다 틀리다하는 기준이 없다는 것이지요. 즉, 오차로 출력되는 값은 하나의 스칼라인데, 업데이트할 가중치는 여러개이기 때문에, 어느 가중치를 얼마나 업데이트 해야하는지 모른다는 것입니다. <br><br>\n",
    "\n",
    "\n",
    "이러한 문제점을 해결하기 위해 다층 퍼셉트론에서는 출력층에서 발생하는 오차값을 이용하여 은닉층으로 역전파(backpropagation)시켜 은닉층에서 발생하는 오차값에 따라 은닉층의 가중치를 업데이트하게 됩니다. 역전파에 대한 내용은 나중에 자세히 다루도록 하겠습니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/22.png?raw=true)\n",
    "\n",
    "다층 퍼셉트론의 동작 원리를 이해하기 위해 좀 더 구체적으로 들어가 보겠습니다. $l$층의 $k$번째 노드와 $l+1$층의 $j$번째 노드를 연결하는 가중치 $w$를 위와 같이 정의해봅니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/23.png?raw=true)\n",
    "\n",
    "우리의 인공 신경망에 대입해서 생각해보면 위와 같습니다. 그렇다면 은닉층의 $j$번째 노드에 입력되는 순입력 함수의 값은 다음과 같습니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/24.png?raw=true)\n",
    "\n",
    "순입력 함수값은 해당 노드의 활성 함수에 입력되는 값입니다. 우리가 여태까지 배웠던 활성 함수를 되새겨 보면, 단순 퍼셉트론에서는 활성 함수로 계단 함수(step function)를, 아달라인에서는 1차 항등 함수를, 로지스틱 회귀에서는 시그모이드 함수를 적용했습니다. <br><br>\n",
    "\n",
    "다층 퍼셉트론의 활성 함수로 시그모이드 함수를 적용하게 되면 분석이 복잡해지긴 하지만 시그모이드 함수의 결과가 미분가능한 꼴이어서 역전파 알고리즘을 적용할 수 있게 되고, 나아가 이미지 분류 등과 같은 복잡한 비선형 문제를 풀 수 있는 잠재력을 지니게 됩니다.\n",
    "<br><br>\n",
    "\n",
    "여기서 잠깐, 퍼셉트론의 활성 함수는 시그모이드가 아닌데.. 시그모이드를 활성 함수로 적용하게 되면 이는 로지스틱 회귀를 겹겹히 쌓아 놓은 구조가 되므로, 엄밀히 말하면 다층 퍼셉트론이라는 말은 좀 어폐가 있어 보이네요. 아무튼 그냥 다층 퍼셉트론이라 부르기로 합니다. <br><br>\n",
    "\n",
    "시그모이드 함수를 적용한 활성 함수를 φ라 하면 은닉층의 j번째 노드의 활성 함수는 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/25.png?raw=true)\n",
    "\n",
    "이와 같이 각 층에서 순입력 함수값을 계산하고, 시그모이드 활성 함수에 의해 출력값을 계산하여, 최종적으로 출력층에서 활성 함수에 의한 결과값이 나오면 실제값과 오차를 계산한 후 가중치를 업데이트하는 행위를 반복하여 학습을 수행합니다. 학습의 결과가 허용된 오차 이내가 되면 각 층의 신경망에 곱해지는 가중치가 결정되는 것이고, 최종적으로 학습을 종료하게 됩니다. 이런 과정이 결국 딥러닝의 핵심 개념이라 보면 됩니다.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "다층 퍼셉트론은 순전파(feedforward) 인공 신경망의 전형적인 예입니다. 순전파라는 말은 루프를 돌지 않으면서 각 층의 출력값이 그 다음층의 입력값이 된다는 의미입니다. 나중에 다룰 또 다른 인공 신경망인 RNN(Recurrent Neural Network)은 순전파와는 대비되는 개념이 적용됩니다. 단층 퍼셉트론과는 달리 다층 퍼셉트론의 출력층의 출력 노드는 여러개가 될 수 있습니다. 만약 3개 품종을 지닌 아이리스를 분류하는 다층 퍼셉트론은 출력층의 출력노드를 3개로 구성하고 그 결과값에 따라 품종 분류는 아래와 그림과 같은 형식으로 하면 될 것입니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/26.png?raw=true)\n",
    "\n",
    "출력층을 3개의 노드로 구성하고, 아이리스의 3개 품종 setosa, versicolor, verginica에 대한 실제값을 (1, 0, 0), (0, 1, 0), (0, 0, 1)로 정의합니다. 출력층의 1번째 노드 a1(3), 2번째 노드 a2(3), 3번째 노드 a3(3)의 값이 (1, 0, 0) 스러우면 아이리스 품종을 setosa로, (0, 1, 0) 스러우면 versicolor로 분류하는 식입니다.\n",
    "<br><br>\n",
    "\n",
    "활성 함수가 시그모이드 함수이므로 실제 출력층에서의 출력값은 0~1사이의 값을 갖게 됩니다. 따라서 출력값은 (0.9, 0.01, 0.19)과 같은 꼴로 될 것이며, 이는 (1, 0, 0) 스러우므로 setosa로 분류하면 됩니다. 만약 0에서 9까지 숫자를 인식하는 다층 퍼셉트론이라면 출력층의 노드 개수를 10개로 하면 될 것입니다. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 다층 퍼셉트론의 학습 : 역전파 알고리즘\n",
    "\n",
    "1958년 퍼셉트론이 발표된 후 같은 해 7월 8일자 뉴욕타임즈는 앞으로 조만간 걷고, 말하고 자아를 인식하는 단계에 이르는 컴퓨터 세상이 도래할 것이라는 다소 과격한 기사를 냈습니다. 하지만 1969년, 단순 퍼셉트론은 XOR 문제도 풀 수 없다는 사실을 MIT AI 랩 창시자인 Marvin Minsky 교수가 증명하였고, 다층 퍼셉트론(MLP)으로 신경망을 구성하면 XOR 문제를 풀 수 있으나, 이러한 MLP를 학습시키는 방법은 존재하지 않는다고 단정해버렸습니다. 이로 인해 떠들석했던 인공신경망과 관련된 학문과 기술은 더 이상 발전되지 않고 침체기를 겪게 됩니다. <br><br>\n",
    "\n",
    "그런데 1974년, 당시 하버드 대학교 박사과정이었던 Paul Werbos는 MLP를 학습시키는 방법을 찾게 되는데, 이 방법을 Minsky 교수에게 설명하지만 냉랭한 분위기속에 무시되버립니다. Paul Werbos가 Minsky 교수에게 설명한 MLP를 학습시킬 수 있는 획기적인 방법이 바로 오류 역전파 (Backpropagation of errors)라는 개념입니다. 이제 오류 역전파(앞으로 그냥 역전파라고 부르겠습니다)가 무엇인지 살펴보도록 합니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/27.png?raw=true)\n",
    "\n",
    "먼저 위와 같은 인풋:2 히든:2 출력:2 다층 퍼셉트론이 있다고 생각해봅니다. 여기서는 역전파의 개념만 살펴볼 예정이므로 각 층에 존재하는 바이어스는 생략했습니다. 또한 용어의 통일성을 위해 입력층의 $x_1$, $x_2$는 $a_1^{(1)}$, $a_2^{(1)}$로 표기하기로 합니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/28.png?raw=true)\n",
    "\n",
    "이 구조에서 적용되는 활성 함수는 시그모이드 함수 φ입니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/29.png?raw=true)\n",
    "\n",
    "입력층 -> 은닉층 사이의 값의 흐름은 위와 같습니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/30.png?raw=true)\n",
    "\n",
    "은닉층 -> 출력층 사이의 값의 흐름은 위와 같습니다. 이와 같이 입력층의 $a_1^{(1)}$, $a_2^{(1)}$을 시작으로 출력층의 $a_1^{(3)}$, $a_2^{(3)}$ 값이 출력되는 과정을 **순전파(feedforward)**라고 부릅니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/31.png?raw=true)\n",
    "\n",
    "우리는 아달라인을 다룰 때 오차제곱합을 오차함수로 도입해서, 이 오차함수가 최소가 되도록 가중치를 결정했습니다. 물론 가중치를 결정하는 방법은 경사하강법이었죠. 여기서도 오차함수 **J**를 오차제곱합으로 정의를 해보면 위와 같이 될 겁니다. <br><br>\n",
    "\n",
    "여기서 $y_1$, $y_2$는 트레이닝 데이터에 대한 각 노드에 대응되는 실제값입니다. 순전파 1회가 마무리되면 $J_1$과 $J_2$의 값이 결정되겠죠. 입력값 $a_1^{(1)}$, $a_2^{(1)}$과 실제값 $y_1$, $y_2$는 고정된 값이므로 $J_1$과 $J_2$는 결국 가중치 $w1,1(1)$ ~ $w2,2(2)$를 변수로 가지는 함수가 됩니다. 우리의 목표는 $J1$과 $J2$의 값이 최소가 되도록 $w1,1(1)$ ~ $w2,2(2)$ 를 결정해야 합니다. <br><br>\n",
    "\n",
    "아달라인에서와 마찬가지로 다층 퍼셉트론에서도 비용함수의 최소값을 찾아가는 방법으로 경사하강법을 활용한다고 했습니다. 각 층에서 가중치를 업데이트하기 위해서는 결국 각 층에서의 오차함수의 미분값이 필요하게 되는 것이지요. 이를 매우 효율적으로 해결하기 위한 방법이 바로 역전파 알고리즘입니다. 역전파란 역방향으로 오차를 전파시키면서 각층의 가중치를 업데이트하고 최적의 학습 결과를 찾아가는 방법입니다. <br><br>\n",
    "\n",
    "자, 이 경우를 한번 생각해봅니다. 순전파에 있어서 가중치의 값을 매우 미세하게 변화시키면 비용함수 $J1$, $J2$도 매우 미세하게 변화될 겁니다. 매우 미세하게 변화시킨다는 것은 미분을 한다는 이야기입니다. 그런데, 매우 미세한 영역으로 국한할 때 가중치의 미세변화와 이에 따른 비용함수의 미세변화가 선형적인 관계가 된다고 알려져 있습니다. 선형적인 관계라는 이야기는 결국 비용함수를 매우 미세하게 변화시키면 이에 따라 가중치도 매우 미세하게 변화되는데 이 역시 선형적인 관계라는 이야기입니다. <br><br>\n",
    "\n",
    "이런 원리에 의해, 순전파를 통해 출력층에서 계산된 오차 J1, J2의 각 가중치에 따른 미세변화를 입력층 방향으로 역전파시키면서 가중치를 업데이트하고, 또 다시 입력값을 이용해 순전파시켜 출력층에서 새로운 오차를 계산하고, 이 오차를 또다시 역전파시켜 가중치를 업데이트하는 식으로 반복합니다. <br><br>\n",
    "\n",
    "역전파를 이용한 가중치 업데이트 절차는 아래와 같이 요약될 수 있습니다. \n",
    "\n",
    "- 주어진 가중치 값을 이용해 출력층의 출력값을 계산함(순전파를 통해 이루어짐) \n",
    "- 오차를 각 가중치의 뱡향으로 미분한 값(실제로는 learning rate을 곱한 값)을 기존 가중치에서 빼줌(경사하강법을 적용하는 것이며, 역전파를 통해 이루어짐) \n",
    "- 2번 단계는 모든 가중치에 대해 이루어짐 \n",
    "- 1~3단계를 주어진 학습회수만큼 또는 주어진 허용오차값에 도달할 때가지 반복함\n",
    "\n",
    "2번 단계의 가중치 업데이트 식을 수식으로 표현하면 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/32.png?raw=true)\n",
    "\n",
    "여기서 Jtotal은 해당 노드가 영향을 미치는 오차의 총합입니다. 예를 들면 출력층의 노드 $a_1^{(3)}$에서는 다른 노드의 오차에 영향을 전혀 미치지 않으므로 해당 노드에서 오차인 $J1$만 따지면 되지만, 은닉층의 $a_1^{(2)}$ 노드는 출력층의 $a_1^{(3)}$, $a_2^{(3)}$ 노드의 오차에 영향을 미치므로 $J1$, $J2$ 모두 더한 것이 $Jtotal$ 값이 됩니다. <br><br>\n",
    "\n",
    "이번에는 역전파를 통한 가중치 업데이트 메커니즘을 살펴볼 것이므로 편의상 learning rate $\\eta$는 1로 둡니다. 자, 그러면 실제로 역전파를 이용해 가중치를 업데이트 해보도록 하죠. 먼저 가중치 $w1,1(2)$에 대해서 계산을 해봅니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/33.png?raw=true)\n",
    "\n",
    "w1,1(2)에 대한 업데이트 식은 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/34.png?raw=true)\n",
    "\n",
    "미분의 연쇄법칙에 의해 오차의 가중치에 대한 미분값은 아래의 식으로 표현될 수 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/35.png?raw=true)\n",
    "\n",
    "이제 위 식의 오른쪽 항에 있는 3개의 미분값을 하나하나 구해보도록 하지요~ 참고로 이 글의 첫부분에서 서술한 순전파를 통한 값의 흐름을 요약한 수식을 참고하기 바랍니다. 역전파의 출발노드인 $a_1^{(3)}$에서 $Jtotal$의 값은 $J1$입니다. 따라서 위 식 오른쪽 항은 아래와 같이 계산됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/36.png?raw=true)\n",
    "\n",
    "따라서 역전파에서 가중치 업데이트를 위해 사용되는 오차의 가중치에 대한 미분값이 결국 역전파의 출발 노드의 활성 함수 값과 도착 노드의 활성 함수 값, 그리고 실제값만으로 표현되는 것을 알 수 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/37.png?raw=true)\n",
    "\n",
    "위 식의 오른쪽 항에서 처음 두 식의 값을 아래와 같이 δ1(3)으로 둡니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/38.png?raw=true)\n",
    "\n",
    "역전파에 의해 업데이트 되는 w1,1(2)는 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/39.png?raw=true)\n",
    "\n",
    "마찬가지 방법으로 w1,2(2)에 대한 업데이트 수식도 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/40.png?raw=true)\n",
    "\n",
    "δ2(3)을 아래와 같은 수식으로 정의하고,\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/41.png?raw=true)\n",
    "\n",
    "w2,1(2), w2,2(2)에 대해서도 계산을 해보면\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/42.png?raw=true)\n",
    "\n",
    "여기까지 서술해보니 뭔가 규칙성이 보입니다. 출력층에서 은닉층으로 역전파를 통해 전달되는 값이 결국 δ1(3) 과 δ2(3) 뿐이더라도 관련된 가중치를 업데이트하는데 충분하다는 것을 알 수 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/43.png?raw=true)\n",
    "\n",
    "이제, 은닉층에서 입력층으로 향하는 역전파를 통해 w1,1(1)의 값을 업데이트 해보겠습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/44.png?raw=true)\n",
    "\n",
    "w1,1(1)을 업데이트 하기 위해서 a1(2)에서 Jtotal을 w1,1(1)로 편미분한 값을 계산해야겠지요? 앞에서 적용한 것과 마찬가지로 미분의 연쇄법칙을 활용하면 다음과 같은 식이 됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/45.png?raw=true)\n",
    "\n",
    "위에서 언급했듯이 a1(2)에서 Jtotal은 J1과 J2를 더한 값이 됩니다. 따라서 위 식 오른쪽 항의 첫 번째 편미분값은 아래와 같이 계산됩니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/46.png?raw=true)\n",
    "\n",
    "그런데 출력층 -> 은닉층으로 역전파를 통한 가중치 계산시에 등장한 식들을 참고하여 계산해보면 다음과 같은 결과가 나옵니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/47.png?raw=true)\n",
    "\n",
    "나머지 편미분 값은 이미 해본 계산이므로 쉽게 계산됩니다.  w1,1(1)을 업데이트 하기 위한 편미분 값은 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/48.png?raw=true)\n",
    "\n",
    "위 식의 오른쪽 항에서 처음 두 식을 δ1(2)로 둡니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/49.png?raw=true)\n",
    "\n",
    "최종적으로 w1,1(1)의 업데이트 식은 아래와 같게 됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/50.png?raw=true)\n",
    "\n",
    "동일하게 w1,2(1)의 업데이트 식은 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/51.png?raw=true)\n",
    "\n",
    "마찬가지로, δ2(2)를 다음과 같이 두고, \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/52.png?raw=true)\n",
    "\n",
    "w2,1(1), w2,2(2)에 대한 업데이트 식을 나타내면 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/53.png?raw=true)\n",
    "\n",
    "따라서 모든 가중치에 대한 업데이트 식은 아래와 같이 표현할 수 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/54.png?raw=true)\n",
    "\n",
    "여태까지 다룬 역전파 알고리즘 개념을 그림으로 도식화하여 나타내보면 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/55.png?raw=true)\n",
    "\n",
    "이와 같이 순전파 -> 역전파 -> 가중치 업데이트 -> 순전파로 계속 반복해나가면 오차값이 0에 가까워지게 됩니다. 이상으로 역전파 알고리즘 및 가중치 업데이트 원리를 살펴보았습니다. 역전파 알고리즘을 이해하기가 쬐금은 어렵지만, 이 부분이 딥러닝의 핵심적인 역할을 하는 부분이며, 완전한 이해가 힘들더라도 그 개념만이라도 알고 있으면 많은 도움이 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### 3. ReLU 활성화 함수\n",
    "\n",
    "위의 글에서 역전파를 이해하기 위해 도입한 활성함수는 시그모이드 함수입니다. 시그모이드 함수는 활성함수로서 여러가지 장점이 있는 함수임에는 틀림이 없으나 은닉층이 매우 많은 심층신경망에서는 단점이 존재하는데, 신경망의 은닉층이 많아질수록 역전파에 의한 가중치 보정이 의미가 없어지는 gradient vanishing 문제가 발생합니다. <br><br>\n",
    "\n",
    "gradient vanishing은 역전파를 수행하는 은닉층의 개수가 많아지면 J(w)의 w에 대한 편미분값이 0에 가까워져서 가중치 업데이트 식에 의한 가중치 갱신이 거의 이루어지지 않기 때문에 발생하는 현상입니다. 이는 시그모이드의 도함수의 함수값이 거의 0에 가깝게 작기 때문인데요. 따라서 새로운 활성화 함수인 ReLU를 도입합니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/55.png?raw=true)\n",
    "\n",
    "심층신경망에서 활성함수로써 ReLU는 시그모이드보다 훨씬 나은 성능을 보이며, 대부분의 심층신경망에서 활성함수로 널리 사용되고 있습니다. 따라서 이전에 보여드린 시그모이드를 활성함수로 한 예시는 역전파의 개념을 이해하는데 만족하시고 실제 응용시에는 ReLU를 활성함수로 적용하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4. Tensorflow로 신경망 구현하기\n",
    "\n",
    "아래코드를 참고하여 Tensorflow로 인공 신경망을 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "feature = iris.data\n",
    "label = iris.target\n",
    "label = np.expand_dims(label, axis=1)\n",
    "label = np.float32(label)\n",
    "\n",
    "iris = np.concatenate([feature, label], axis=1)\n",
    "np.random.shuffle(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120, 1)\n",
      "(30, 4) (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 학습, 테스트 데이터셋 분할\n",
    "\n",
    "feature = iris[:, :4]\n",
    "label = iris[:, 4:]\n",
    "\n",
    "split_point = int(0.8 * len(feature))\n",
    "train_feature, train_label = feature[:split_point], label[:split_point]\n",
    "test_feature, test_label = feature[split_point:], label[split_point:]\n",
    "\n",
    "print(train_feature.shape, train_label.shape)\n",
    "print(test_feature.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구현 (Class)\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class NeuralNetwork(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = Dense(512, activation='relu')\n",
    "        self.output_layer = Dense(3, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2772 - accuracy: 0.3167\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0127 - accuracy: 0.3167\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9336 - accuracy: 0.7167\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9032 - accuracy: 0.6833\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8749 - accuracy: 0.6833\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8525 - accuracy: 0.6833\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8360 - accuracy: 0.6833\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8119 - accuracy: 0.6833\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7920 - accuracy: 0.7167\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7766 - accuracy: 0.6833\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7578 - accuracy: 0.6833\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7462 - accuracy: 0.7083\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7323 - accuracy: 0.6833\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7188 - accuracy: 0.6833\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7067 - accuracy: 0.6917\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6940 - accuracy: 0.6833\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6800 - accuracy: 0.7333\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6692 - accuracy: 0.6833\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.7000\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.6833\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6833\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6352 - accuracy: 0.7667\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6294 - accuracy: 0.7167\n",
      "Epoch 24/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.6833\n",
      "Epoch 25/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7667\n",
      "Epoch 26/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6058 - accuracy: 0.7083\n",
      "Epoch 27/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.7333\n",
      "Epoch 28/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.8333\n",
      "Epoch 29/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.7667\n",
      "Epoch 30/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.7750\n",
      "Epoch 31/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7417\n",
      "Epoch 32/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.8250\n",
      "Epoch 33/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.7250\n",
      "Epoch 34/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.7667\n",
      "Epoch 35/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5567 - accuracy: 0.7000\n",
      "Epoch 36/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5514 - accuracy: 0.8333\n",
      "Epoch 37/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5464 - accuracy: 0.8333\n",
      "Epoch 38/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5421 - accuracy: 0.8083\n",
      "Epoch 39/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.7750\n",
      "Epoch 40/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7917\n",
      "Epoch 41/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.8083\n",
      "Epoch 42/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7167\n",
      "Epoch 43/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.8583\n",
      "Epoch 44/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.8500\n",
      "Epoch 45/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7417\n",
      "Epoch 46/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.8667\n",
      "Epoch 47/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5085 - accuracy: 0.8083\n",
      "Epoch 48/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.8000\n",
      "Epoch 49/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.8417\n",
      "Epoch 50/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.9000\n",
      "Epoch 51/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.8333\n",
      "Epoch 52/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.8667\n",
      "Epoch 53/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8833\n",
      "Epoch 54/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.8417\n",
      "Epoch 55/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.8500\n",
      "Epoch 56/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.8167\n",
      "Epoch 57/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7917\n",
      "Epoch 58/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.8833\n",
      "Epoch 59/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.8667\n",
      "Epoch 60/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.8583\n",
      "Epoch 61/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8667\n",
      "Epoch 62/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.8750\n",
      "Epoch 63/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.8250\n",
      "Epoch 64/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.8917\n",
      "Epoch 65/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.8667\n",
      "Epoch 66/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.8917\n",
      "Epoch 67/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.8833\n",
      "Epoch 68/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.9000\n",
      "Epoch 69/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.8833\n",
      "Epoch 70/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.8750\n",
      "Epoch 71/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.8667\n",
      "Epoch 72/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8667\n",
      "Epoch 73/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.8833\n",
      "Epoch 74/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.9167\n",
      "Epoch 75/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4395 - accuracy: 0.8917\n",
      "Epoch 76/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.9000\n",
      "Epoch 77/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8917\n",
      "Epoch 78/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.8667\n",
      "Epoch 79/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8750\n",
      "Epoch 80/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.9500\n",
      "Epoch 81/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.8917\n",
      "Epoch 82/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8750\n",
      "Epoch 83/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.9000\n",
      "Epoch 84/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.9333\n",
      "Epoch 85/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8833\n",
      "Epoch 86/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.9583\n",
      "Epoch 87/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.9000\n",
      "Epoch 88/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4186 - accuracy: 0.8917\n",
      "Epoch 89/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.9000\n",
      "Epoch 90/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8750\n",
      "Epoch 91/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.9250\n",
      "Epoch 92/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.9083\n",
      "Epoch 93/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.8833\n",
      "Epoch 94/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4070 - accuracy: 0.9417\n",
      "Epoch 95/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.9083\n",
      "Epoch 96/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.9250\n",
      "Epoch 97/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.9250\n",
      "Epoch 98/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.9417\n",
      "Epoch 99/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.9000\n",
      "Epoch 100/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.9500\n",
      "Epoch 101/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.9417\n",
      "Epoch 102/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8750\n",
      "Epoch 103/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.9500\n",
      "Epoch 104/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.9167\n",
      "Epoch 105/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3917 - accuracy: 0.9583\n",
      "Epoch 106/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.9083\n",
      "Epoch 107/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.9583\n",
      "Epoch 108/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.9333\n",
      "Epoch 109/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.9500\n",
      "Epoch 110/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.9333\n",
      "Epoch 111/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.9083\n",
      "Epoch 112/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.9417\n",
      "Epoch 113/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.9250\n",
      "Epoch 114/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3793 - accuracy: 0.9250\n",
      "Epoch 115/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.9333\n",
      "Epoch 116/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.9167\n",
      "Epoch 117/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.9667\n",
      "Epoch 118/150\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3757 - accuracy: 0.9250\n",
      "Epoch 119/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.9583\n",
      "Epoch 120/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.9083\n",
      "Epoch 121/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.9083\n",
      "Epoch 122/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.9583\n",
      "Epoch 123/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.9417\n",
      "Epoch 124/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.9333\n",
      "Epoch 125/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.9250\n",
      "Epoch 126/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.9417\n",
      "Epoch 127/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.9500\n",
      "Epoch 128/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.9667\n",
      "Epoch 129/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.9500\n",
      "Epoch 130/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.9583\n",
      "Epoch 131/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.9250\n",
      "Epoch 132/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3589 - accuracy: 0.9583\n",
      "Epoch 133/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.9500\n",
      "Epoch 134/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.9583\n",
      "Epoch 135/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.9583\n",
      "Epoch 136/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.9583\n",
      "Epoch 137/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.9500\n",
      "Epoch 138/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.9583\n",
      "Epoch 139/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.9583\n",
      "Epoch 140/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.9750\n",
      "Epoch 141/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3517 - accuracy: 0.9500\n",
      "Epoch 142/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.9417\n",
      "Epoch 143/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.9667\n",
      "Epoch 144/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.9583\n",
      "Epoch 145/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.9417\n",
      "Epoch 146/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.9167\n",
      "Epoch 147/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.9667\n",
      "Epoch 148/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.9667\n",
      "Epoch 149/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.9500\n",
      "Epoch 150/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25281842518>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컴파일 및 학습 진행\n",
    "\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "net = NeuralNetwork()\n",
    "net.compile(optimizer=optimizers.SGD(lr=0.005),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "net.fit(train_feature, train_label, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33121922612190247, 0.9333333373069763]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 과정\n",
    "\n",
    "net.evaluate(test_feature, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 2, 2, 1, 2, 0, 1, 0,\n",
       "       0, 1, 2, 1, 0, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추론 과정\n",
    "\n",
    "pred = net.predict(test_feature)\n",
    "pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 2., 0., 0., 1., 0., 1., 1., 2., 1., 0., 1., 0., 0., 2.,\n",
       "        1., 1., 2., 0., 1., 0., 0., 1., 1., 1., 0., 2., 2., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 라벨과 비교\n",
    "\n",
    "test_label.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "이번엔 Sequential API를 이용해 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구현 (Sequential)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "seq_net = Sequential([\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dense(3, activation='softmax')\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.9417\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.9000\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.9500\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.9333\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3661 - accuracy: 0.9417\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3637 - accuracy: 0.9167\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3637 - accuracy: 0.9417\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.9417\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.9583\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.9333\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.9417\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3573 - accuracy: 0.9417\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.9417\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.9333\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.9417\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3524 - accuracy: 0.9500\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.9667\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.9333\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.9417\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3496 - accuracy: 0.9500\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.9500\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.9417\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.9333\n",
      "Epoch 24/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.9417\n",
      "Epoch 25/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.9417\n",
      "Epoch 26/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.9500\n",
      "Epoch 27/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.9417\n",
      "Epoch 28/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3411 - accuracy: 0.9583\n",
      "Epoch 29/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.9417\n",
      "Epoch 30/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.9417\n",
      "Epoch 31/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.9583\n",
      "Epoch 32/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.9500\n",
      "Epoch 33/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.9583\n",
      "Epoch 34/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.9500\n",
      "Epoch 35/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.9583\n",
      "Epoch 36/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.9333\n",
      "Epoch 37/150\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.96 - 0s 2ms/step - loss: 0.3326 - accuracy: 0.9583\n",
      "Epoch 38/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.9583\n",
      "Epoch 39/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.9417\n",
      "Epoch 40/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.9417\n",
      "Epoch 41/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.9583\n",
      "Epoch 42/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.9583\n",
      "Epoch 43/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.9583\n",
      "Epoch 44/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.9583\n",
      "Epoch 45/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.9583\n",
      "Epoch 46/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.9583\n",
      "Epoch 47/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.9500\n",
      "Epoch 48/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.9583\n",
      "Epoch 49/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.9667\n",
      "Epoch 50/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.9500\n",
      "Epoch 51/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.9417\n",
      "Epoch 52/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3189 - accuracy: 0.9667\n",
      "Epoch 53/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.9583\n",
      "Epoch 54/150\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.9667\n",
      "Epoch 55/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.9583\n",
      "Epoch 56/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3157 - accuracy: 0.9667\n",
      "Epoch 57/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.9417\n",
      "Epoch 58/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.9667\n",
      "Epoch 59/150\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.9667\n",
      "Epoch 60/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.9417\n",
      "Epoch 61/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3127 - accuracy: 0.9667\n",
      "Epoch 62/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.9667\n",
      "Epoch 63/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.9667\n",
      "Epoch 64/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.9667\n",
      "Epoch 65/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.9667\n",
      "Epoch 66/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3070 - accuracy: 0.9667\n",
      "Epoch 67/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.9583\n",
      "Epoch 68/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.9500\n",
      "Epoch 69/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.9667\n",
      "Epoch 70/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.9583\n",
      "Epoch 71/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.9500\n",
      "Epoch 72/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3044 - accuracy: 0.9500\n",
      "Epoch 73/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.9667\n",
      "Epoch 74/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.9500\n",
      "Epoch 75/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.9667\n",
      "Epoch 76/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2997 - accuracy: 0.9667\n",
      "Epoch 77/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3004 - accuracy: 0.9500\n",
      "Epoch 78/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2997 - accuracy: 0.9750\n",
      "Epoch 79/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.9583\n",
      "Epoch 80/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2976 - accuracy: 0.9583\n",
      "Epoch 81/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.9667\n",
      "Epoch 82/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.9583\n",
      "Epoch 83/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.9667\n",
      "Epoch 84/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.9667\n",
      "Epoch 85/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.9667\n",
      "Epoch 86/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.9667\n",
      "Epoch 87/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.9750\n",
      "Epoch 88/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.9667\n",
      "Epoch 89/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9583\n",
      "Epoch 90/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.9750\n",
      "Epoch 91/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2883 - accuracy: 0.9583\n",
      "Epoch 92/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.9667\n",
      "Epoch 93/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2862 - accuracy: 0.9667\n",
      "Epoch 94/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.9667\n",
      "Epoch 95/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2854 - accuracy: 0.9750\n",
      "Epoch 96/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.9667\n",
      "Epoch 97/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.9667\n",
      "Epoch 98/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.9750\n",
      "Epoch 99/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2817 - accuracy: 0.9667\n",
      "Epoch 100/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2825 - accuracy: 0.9667\n",
      "Epoch 101/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.9583\n",
      "Epoch 102/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.9667\n",
      "Epoch 103/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.9583\n",
      "Epoch 104/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.9667\n",
      "Epoch 105/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.9667\n",
      "Epoch 106/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.9583\n",
      "Epoch 107/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.9667\n",
      "Epoch 108/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.9750\n",
      "Epoch 109/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9750\n",
      "Epoch 110/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.9667\n",
      "Epoch 111/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.9583\n",
      "Epoch 112/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.9750\n",
      "Epoch 113/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.9667\n",
      "Epoch 114/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2713 - accuracy: 0.9667\n",
      "Epoch 115/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.9667\n",
      "Epoch 116/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.9667\n",
      "Epoch 117/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2703 - accuracy: 0.9750\n",
      "Epoch 118/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.9583\n",
      "Epoch 119/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.9667\n",
      "Epoch 120/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2712 - accuracy: 0.9667\n",
      "Epoch 121/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.9750\n",
      "Epoch 122/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.9667\n",
      "Epoch 123/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.9667\n",
      "Epoch 124/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.9500\n",
      "Epoch 125/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.9750\n",
      "Epoch 126/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.9667\n",
      "Epoch 127/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.9750\n",
      "Epoch 128/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2635 - accuracy: 0.9750\n",
      "Epoch 129/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.9667\n",
      "Epoch 130/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.9750\n",
      "Epoch 131/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.9750\n",
      "Epoch 132/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.9750\n",
      "Epoch 133/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.9667\n",
      "Epoch 134/150\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.9667\n",
      "Epoch 135/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.9667\n",
      "Epoch 136/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.9750\n",
      "Epoch 137/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.9667\n",
      "Epoch 138/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.9750\n",
      "Epoch 139/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.9667\n",
      "Epoch 140/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9750\n",
      "Epoch 141/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.9750\n",
      "Epoch 142/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.9750\n",
      "Epoch 143/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.9667\n",
      "Epoch 144/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9667\n",
      "Epoch 145/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9667\n",
      "Epoch 146/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.9667\n",
      "Epoch 147/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511 - accuracy: 0.9667\n",
      "Epoch 148/150\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.96 - 0s 2ms/step - loss: 0.2510 - accuracy: 0.9667\n",
      "Epoch 149/150\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2528 - accuracy: 0.9750\n",
      "Epoch 150/150\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25286515b70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컴파일 및 학습 진행\n",
    "\n",
    "seq_net.compile(optimizer=optimizers.SGD(lr=0.005),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "seq_net.fit(train_feature, train_label, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 998us/step - loss: 0.2334 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23344847559928894, 0.9666666388511658]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 과정\n",
    "seq_net.evaluate(test_feature, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 2, 1, 1, 2, 0, 1, 0,\n",
       "       0, 1, 2, 1, 0, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추론 과정\n",
    "pred = seq_net.predict(test_feature)\n",
    "pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 2., 0., 0., 1., 0., 1., 1., 2., 1., 0., 1., 0., 0., 2.,\n",
       "        1., 1., 2., 0., 1., 0., 0., 1., 1., 1., 0., 2., 2., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 라벨과 비교\n",
    "test_label.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
