{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 다층 퍼셉트론\n",
    "> 인공신경망 알고리즘인 다층 퍼셉트론에 대해 배워봅시다.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Day 6]\n",
    "- permalink: /mlp\n",
    "- exec: colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1. 다층 퍼셉트론\n",
    "\n",
    "퍼셉트론에서 결과값을 내놓는 부분은 결국 활성 함수(activation function)인데, 단층 퍼셉트론에서는 이 활성 함수가 1개밖에 없는 구조입니다. 아래 그림에서 출력층의 활성 함수를 a로 표시했습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/15.png?raw=true)\n",
    " \n",
    "단층 퍼셉트론은 한계가 있는데, 비선형적으로 분리되는 데이터에 대해서는 제대로 된 학습이 불가능하다는 것입니다. 예를 들면 단층 퍼셉트론으로 AND연산에 대해서는 학습이 가능하지만, XOR에 대해서는 학습이 불가능하다는 것이 증명되었습니다.\n",
    "<br><br>\n",
    "\n",
    "이를 극복하기 위한 방안으로 입력층과 출력층 사이에 하나 이상의 중간층을 두어 비선형적으로 분리되는 데이터에 대해서도 학습이 가능하도록 다층 퍼셉트론(줄여서 MLP)이 고안되었습니다. 아래 그림은 다층 퍼셉트론의 구조의 한 예를 보인 것입니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/16.png?raw=true)\n",
    "\n",
    "입력층과 출력층 사이에 존재하는 중간층을 숨어 있는 층이라 해서 은닉층이라 부릅니다. 입력층과 출력층 사이에 여러개의 은닉층이 있는 인공 신경망을 심층 신경망(deep neural network)이라 부르며, 심층 신경망을 학습하기 위해 고안된 특별한 알고리즘들을 딥러닝(deep learning)이라 부릅니다. 따라서 딥러닝을 제대로 이해하기 위해서는 심층 신경망의 가장 기초적인 다층 퍼셉트론에 대해 제대로 알고 있어야 합니다.\n",
    "<br><br>\n",
    "\n",
    "다층 퍼셉트론에서는 입력층에서 전달되는 값이 은닉층의 모든 노드로 전달되며 은닉층의 모든 노드의 출력값 역시 출력층의 모든 노드로 전달됩니다. 이런 형식으로 값이 전달되는 것을 **순전파(feedforward)**라 합니다. 입력층과 은닉층에 있는 한개의 노드만 볼 때, 하나의 단층 퍼셉트론으로 생각할 수 있습니다. 따라서 은닉층에 있는 각각의 노드는 퍼셉트론의 활성 함수라고 볼 수 있습니다. 은닉층에 있는 각 노드에 이름을 $a_n$으로 이름을 붙여서 그림을 그려보면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/17.png?raw=true)\n",
    "\n",
    "마찬가지로 은닉층과 츨력층에 있는 한개의 노드만 고려해보면 이 역시 하나의 단층 퍼셉트론이며 출력층의 각 노드에 $A_n$로 이름을 붙여서 그림을 그려보면 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/18.png?raw=true)\n",
    "\n",
    "은닉층과 출력층의 노드에 이름 붙여진 $a_n$과 $A_n$를 소문자 $a$로 통일되게끔 표현하기 위해 위첨자와 아래첨자를 도입해 봅니다. 입력층, 은닉층, 출력층은 각각 1번째, 2번째, 3번째 층이므로 a의 위첨자에 층을 표시하도록 해봅니다. 또한 은닉층에도 입력층에서와 같이 바이어스 $a_0$를 추가하여 다층 퍼셉트론을 표현하면 아래 그림과 같이 됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/21.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 다층 퍼셉트론이 동작하는 원리에 대해 살펴봅니다. 우리는 단층 퍼셉트론이 동작하는 원리는 다 알고 있습니다. 단층 퍼셉트론은 활성 함수가 내놓는 결과값이 실제값과 오류가 최소가 되도록 입력층에서 전달되는 값들에 곱해지는 가중치의 값을 결정하는 것입니다. <br><br>\n",
    "\n",
    "다층 퍼셉트론의 동작 원리 역시 단층 퍼셉트론의 동작 원리와 크게 다를 것이 없습니다. 차이점은 단층 퍼셉트론은 활성 함수가 1개인 반면, 다층 퍼셉트론은 은닉층과 출력층에 존재하는 활성 함수가 여러개이고, 이에 따른 가중치도 여러개인 것이지요. 다층 퍼셉트론은 아래와 같이 동작합니다. <br><br>\n",
    "\n",
    "- 각 층에서의 가중치(w)를 임의의 값(보통 0으로 설정함)으로 설정합니다. 각 층에서 bias 값은 1로 설정합니다.\n",
    "- 하나의 트레이닝 데이터에 대해서 각 층에서의 순입력 함수값을 계산하고 최종적으로 활성 함수에 의한 출력값을 계산합니다.\n",
    "- 출력층의 활성 함수에 의한 결과값과 실제값이 허용 오차 이내가 되도록 각층에서의 가중치를 업데이트합니다.\n",
    "- 모든 트레이닝 데이터에 대해서 출력층의 활성 함수에 의한 결과값과 실제값이 허용 오차 이내가 되면 학습을 종료합니다.\n",
    "<br><br>\n",
    "\n",
    "그런데, 여기서 한가지 어려움이 있습니다. 단층 퍼셉트론에서는 은닉층이 존재하지 않고, 입력층과 출력층만 존재하기 때문에 출력층의 결과값을 우리가 확보한 실제값과 비교하여 오차가 최소가 되도록 가중치를 업데이트하고 결정하면 됩니다. 하지만 다층 퍼셉트론에서는 입력층과 출력층 사이에 은닉층이 존재하고, 은닉층의 출력값에 대한 기준값을 정의할 수 없습니다. 은닉층에서 어떤 값이 출력되어야 맞다 틀리다하는 기준이 없다는 것이지요. 즉, 오차로 출력되는 값은 하나의 스칼라인데, 업데이트할 가중치는 여러개이기 때문에, 어느 가중치를 얼마나 업데이트 해야하는지 모른다는 것입니다. <br><br>\n",
    "\n",
    "\n",
    "이러한 문제점을 해결하기 위해 다층 퍼셉트론에서는 출력층에서 발생하는 오차값을 이용하여 은닉층으로 역전파(backpropagation)시켜 은닉층에서 발생하는 오차값에 따라 은닉층의 가중치를 업데이트하게 됩니다. 역전파에 대한 내용은 나중에 자세히 다루도록 하겠습니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/22.png?raw=true)\n",
    "\n",
    "다층 퍼셉트론의 동작 원리를 이해하기 위해 좀 더 구체적으로 들어가 보겠습니다. $l$층의 $k$번째 노드와 $l+1$층의 $j$번째 노드를 연결하는 가중치 $w$를 위와 같이 정의해봅니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/23.png?raw=true)\n",
    "\n",
    "우리의 인공 신경망에 대입해서 생각해보면 위와 같습니다. 그렇다면 은닉층의 $j$번째 노드에 입력되는 순입력 함수의 값은 다음과 같습니다. \n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/24.png?raw=true)\n",
    "\n",
    "순입력 함수값은 해당 노드의 활성 함수에 입력되는 값입니다. 우리가 여태까지 배웠던 활성 함수를 되새겨 보면, 단순 퍼셉트론에서는 활성 함수로 계단 함수(step function)를, 아달라인에서는 1차 항등 함수를, 로지스틱 회귀에서는 시그모이드 함수를 적용했습니다. <br><br>\n",
    "\n",
    "다층 퍼셉트론의 활성 함수로 시그모이드 함수를 적용하게 되면 분석이 복잡해지긴 하지만 시그모이드 함수의 결과가 미분가능한 꼴이어서 역전파 알고리즘을 적용할 수 있게 되고, 나아가 이미지 분류 등과 같은 복잡한 비선형 문제를 풀 수 있는 잠재력을 지니게 됩니다.\n",
    "<br><br>\n",
    "\n",
    "여기서 잠깐, 퍼셉트론의 활성 함수는 시그모이드가 아닌데.. 시그모이드를 활성 함수로 적용하게 되면 이는 로지스틱 회귀를 겹겹히 쌓아 놓은 구조가 되므로, 엄밀히 말하면 다층 퍼셉트론이라는 말은 좀 어폐가 있어 보이네요. 아무튼 그냥 다층 퍼셉트론이라 부르기로 합니다. <br><br>\n",
    "\n",
    "시그모이드 함수를 적용한 활성 함수를 φ라 하면 은닉층의 j번째 노드의 활성 함수는 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/25.png?raw=true)\n",
    "\n",
    "이와 같이 각 층에서 순입력 함수값을 계산하고, 시그모이드 활성 함수에 의해 출력값을 계산하여, 최종적으로 출력층에서 활성 함수에 의한 결과값이 나오면 실제값과 오차를 계산한 후 가중치를 업데이트하는 행위를 반복하여 학습을 수행합니다. 학습의 결과가 허용된 오차 이내가 되면 각 층의 신경망에 곱해지는 가중치가 결정되는 것이고, 최종적으로 학습을 종료하게 됩니다. 이런 과정이 결국 딥러닝의 핵심 개념이라 보면 됩니다.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "다층 퍼셉트론은 순전파(feedforward) 인공 신경망의 전형적인 예입니다. 순전파라는 말은 루프를 돌지 않으면서 각 층의 출력값이 그 다음층의 입력값이 된다는 의미입니다. 나중에 다룰 또 다른 인공 신경망인 RNN(Recurrent Neural Network)은 순전파와는 대비되는 개념이 적용됩니다. 단층 퍼셉트론과는 달리 다층 퍼셉트론의 출력층의 출력 노드는 여러개가 될 수 있습니다. 만약 3개 품종을 지닌 아이리스를 분류하는 다층 퍼셉트론은 출력층의 출력노드를 3개로 구성하고 그 결과값에 따라 품종 분류는 아래와 그림과 같은 형식으로 하면 될 것입니다.\n",
    "<br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/26.png?raw=true)\n",
    "\n",
    "출력층을 3개의 노드로 구성하고, 아이리스의 3개 품종 setosa, versicolor, verginica에 대한 실제값을 (1, 0, 0), (0, 1, 0), (0, 0, 1)로 정의합니다. 출력층의 1번째 노드 a1(3), 2번째 노드 a2(3), 3번째 노드 a3(3)의 값이 (1, 0, 0) 스러우면 아이리스 품종을 setosa로, (0, 1, 0) 스러우면 versicolor로 분류하는 식입니다.\n",
    "<br><br>\n",
    "\n",
    "활성 함수가 시그모이드 함수이므로 실제 출력층에서의 출력값은 0~1사이의 값을 갖게 됩니다. 따라서 출력값은 (0.9, 0.01, 0.19)과 같은 꼴로 될 것이며, 이는 (1, 0, 0) 스러우므로 setosa로 분류하면 됩니다. 만약 0에서 9까지 숫자를 인식하는 다층 퍼셉트론이라면 출력층의 노드 개수를 10개로 하면 될 것입니다. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 다층 퍼셉트론의 학습 : 역전파 알고리즘\n",
    "\n",
    "1958년 퍼셉트론이 발표된 후 같은 해 7월 8일자 뉴욕타임즈는 앞으로 조만간 걷고, 말하고 자아를 인식하는 단계에 이르는 컴퓨터 세상이 도래할 것이라는 다소 과격한 기사를 냈습니다. 하지만 1969년, 단순 퍼셉트론은 XOR 문제도 풀 수 없다는 사실을 MIT AI 랩 창시자인 Marvin Minsky 교수가 증명하였고, 다층 퍼셉트론(MLP)으로 신경망을 구성하면 XOR 문제를 풀 수 있으나, 이러한 MLP를 학습시키는 방법은 존재하지 않는다고 단정해버렸습니다. 이로 인해 떠들석했던 인공신경망과 관련된 학문과 기술은 더 이상 발전되지 않고 침체기를 겪게 됩니다. <br><br>\n",
    "\n",
    "그런데 1974년, 당시 하버드 대학교 박사과정이었던 Paul Werbos는 MLP를 학습시키는 방법을 찾게 되는데, 이 방법을 Minsky 교수에게 설명하지만 냉랭한 분위기속에 무시되버립니다. Paul Werbos가 Minsky 교수에게 설명한 MLP를 학습시킬 수 있는 획기적인 방법이 바로 오류 역전파 (Backpropagation of errors)라는 개념입니다. 이제 오류 역전파(앞으로 그냥 역전파라고 부르겠습니다)가 무엇인지 살펴보도록 합니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/27.png?raw=true)\n",
    "\n",
    "먼저 위와 같은 인풋:2 히든:2 출력:2 다층 퍼셉트론이 있다고 생각해봅니다. 여기서는 역전파의 개념만 살펴볼 예정이므로 각 층에 존재하는 바이어스는 생략했습니다. 또한 용어의 통일성을 위해 입력층의 $x_1$, $x_2$는 $a_1^{(1)}$, $a_2^{(1)}$로 표기하기로 합니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/28.png?raw=true)\n",
    "\n",
    "이 구조에서 적용되는 활성 함수는 시그모이드 함수 φ입니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/29.png?raw=true)\n",
    "\n",
    "입력층 -> 은닉층 사이의 값의 흐름은 위와 같습니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/30.png?raw=true)\n",
    "\n",
    "은닉층 -> 출력층 사이의 값의 흐름은 위와 같습니다. 이와 같이 입력층의 $a_1^{(1)}$, $a_2^{(1)}$을 시작으로 출력층의 $a_1^{(3)}$, $a_2^{(3)}$ 값이 출력되는 과정을 **순전파(feedforward)**라고 부릅니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/31.png?raw=true)\n",
    "\n",
    "우리는 아달라인을 다룰 때 오차제곱합을 오차함수로 도입해서, 이 오차함수가 최소가 되도록 가중치를 결정했습니다. 물론 가중치를 결정하는 방법은 경사하강법이었죠. 여기서도 오차함수 **J**를 오차제곱합으로 정의를 해보면 위와 같이 될 겁니다. <br><br>\n",
    "\n",
    "여기서 $y_1$, $y_2$는 트레이닝 데이터에 대한 각 노드에 대응되는 실제값입니다. 순전파 1회가 마무리되면 $J_1$과 $J_2$의 값이 결정되겠죠. 입력값 $a_1^{(1)}$, $a_2^{(1)}$과 실제값 $y_1$, $y_2$는 고정된 값이므로 $J_1$과 $J_2$는 결국 가중치 $w1,1(1)$ ~ $w2,2(2)$를 변수로 가지는 함수가 됩니다. 우리의 목표는 $J1$과 $J2$의 값이 최소가 되도록 $w1,1(1)$ ~ $w2,2(2)$ 를 결정해야 합니다. <br><br>\n",
    "\n",
    "아달라인에서와 마찬가지로 다층 퍼셉트론에서도 비용함수의 최소값을 찾아가는 방법으로 경사하강법을 활용한다고 했습니다. 각 층에서 가중치를 업데이트하기 위해서는 결국 각 층에서의 오차함수의 미분값이 필요하게 되는 것이지요. 이를 매우 효율적으로 해결하기 위한 방법이 바로 역전파 알고리즘입니다. 역전파란 역방향으로 오차를 전파시키면서 각층의 가중치를 업데이트하고 최적의 학습 결과를 찾아가는 방법입니다. <br><br>\n",
    "\n",
    "자, 이 경우를 한번 생각해봅니다. 순전파에 있어서 가중치의 값을 매우 미세하게 변화시키면 비용함수 $J1$, $J2$도 매우 미세하게 변화될 겁니다. 매우 미세하게 변화시킨다는 것은 미분을 한다는 이야기입니다. 그런데, 매우 미세한 영역으로 국한할 때 가중치의 미세변화와 이에 따른 비용함수의 미세변화가 선형적인 관계가 된다고 알려져 있습니다. 선형적인 관계라는 이야기는 결국 비용함수를 매우 미세하게 변화시키면 이에 따라 가중치도 매우 미세하게 변화되는데 이 역시 선형적인 관계라는 이야기입니다. <br><br>\n",
    "\n",
    "이런 원리에 의해, 순전파를 통해 출력층에서 계산된 오차 J1, J2의 각 가중치에 따른 미세변화를 입력층 방향으로 역전파시키면서 가중치를 업데이트하고, 또 다시 입력값을 이용해 순전파시켜 출력층에서 새로운 오차를 계산하고, 이 오차를 또다시 역전파시켜 가중치를 업데이트하는 식으로 반복합니다. <br><br>\n",
    "\n",
    "역전파를 이용한 가중치 업데이트 절차는 아래와 같이 요약될 수 있습니다. \n",
    "\n",
    "- 주어진 가중치 값을 이용해 출력층의 출력값을 계산함(순전파를 통해 이루어짐) \n",
    "- 오차를 각 가중치의 뱡향으로 미분한 값(실제로는 learning rate을 곱한 값)을 기존 가중치에서 빼줌(경사하강법을 적용하는 것이며, 역전파를 통해 이루어짐) \n",
    "- 2번 단계는 모든 가중치에 대해 이루어짐 \n",
    "- 1~3단계를 주어진 학습회수만큼 또는 주어진 허용오차값에 도달할 때가지 반복함\n",
    "\n",
    "2번 단계의 가중치 업데이트 식을 수식으로 표현하면 다음과 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/32.png?raw=true)\n",
    "\n",
    "여기서 Jtotal은 해당 노드가 영향을 미치는 오차의 총합입니다. 예를 들면 출력층의 노드 $a_1^{(3)}$에서는 다른 노드의 오차에 영향을 전혀 미치지 않으므로 해당 노드에서 오차인 $J1$만 따지면 되지만, 은닉층의 $a_1^{(2)}$ 노드는 출력층의 $a_1^{(3)}$, $a_2^{(3)}$ 노드의 오차에 영향을 미치므로 $J1$, $J2$ 모두 더한 것이 $Jtotal$ 값이 됩니다. <br><br>\n",
    "\n",
    "이번에는 역전파를 통한 가중치 업데이트 메커니즘을 살펴볼 것이므로 편의상 learning rate $\\eta$는 1로 둡니다. 자, 그러면 실제로 역전파를 이용해 가중치를 업데이트 해보도록 하죠. 먼저 가중치 $w1,1(2)$에 대해서 계산을 해봅니다. <br><br>\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/33.png?raw=true)\n",
    "\n",
    "w1,1(2)에 대한 업데이트 식은 아래와 같습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/34.png?raw=true)\n",
    "\n",
    "미분의 연쇄법칙에 의해 오차의 가중치에 대한 미분값은 아래의 식으로 표현될 수 있습니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/35.png?raw=true)\n",
    "\n",
    "이제 위 식의 오른쪽 항에 있는 3개의 미분값을 하나하나 구해보도록 하지요~ 참고로 이 글의 첫부분에서 서술한 순전파를 통한 값의 흐름을 요약한 수식을 참고하기 바랍니다. 역전파의 출발노드인 $a_1^{(3)}$에서 $Jtotal$의 값은 $J1$입니다. 따라서 위 식 오른쪽 항은 아래와 같이 계산됩니다.\n",
    "\n",
    "![](https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day6/36.png?raw=true)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
