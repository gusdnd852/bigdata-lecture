<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/bigdata-lecture/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>통계학 기초 | TensorFlow로 시작하는 머신러닝 &amp; 딥러닝</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="통계학 기초" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다." />
<meta property="og:description" content="머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다." />
<link rel="canonical" href="https://gusdnd852.github.io/bigdata-lecture/statics_theory" />
<meta property="og:url" content="https://gusdnd852.github.io/bigdata-lecture/statics_theory" />
<meta property="og:site_name" content="TensorFlow로 시작하는 머신러닝 &amp; 딥러닝" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-18T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gusdnd852.github.io/bigdata-lecture/statics_theory","@type":"BlogPosting","headline":"통계학 기초","dateModified":"2020-07-18T00:00:00-05:00","datePublished":"2020-07-18T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gusdnd852.github.io/bigdata-lecture/statics_theory"},"description":"머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="/bigdata-lecture/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://gusdnd852.github.io/bigdata-lecture/feed.xml" title="TensorFlow로 시작하는 머신러닝 & 딥러닝" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>

     @media screen and (max-width : 511px) {
         body { 
		font-size: 0.84rem; 
	}pre, code, blockquote {
		font-size: 0.84rem !important;
	}
	.katex, .mord {
		font: normal 1.02em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.90em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 0.9rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}
	.mobile_hide{
		display: none;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 

     @media screen and (min-width : 512px) and (max-width : 767px) {
         body { 
		font-size: 0.92rem; 
	}pre, code, blockquote {
		font-size: 0.92rem !important;
	}
	.katex, .mord {
		font: normal 1.04em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.92em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.0rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}    
	.mobile_hide{
		display: none !important;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 
     @media screen and (min-width : 768px) and (max-width : 1024px) {
         body {
		 font-size: 1.0rem; 
	}pre, code, blockquote {
		font-size: 1.0rem !important;
	}
	.katex, .mord {
		font: normal 1.05em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.93em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.1rem !important;
	}
	.contents-right, .contents-left {
		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}  
	.contents-left{
		margin-left: 20px !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

     @media screen and (min-width : 1025px) {
         body { 
		font-size: 1.08rem; 
	}pre, code, blockquote {
		font-size: 1.08rem !important;
	}
     	.katex, .mord {
		font: normal 1.06em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.94em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.2rem !important;
	}
	.contents-right, .contents-left {
    		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}
	.contents-right > h2, .contents-left > h2 {
    		margin-botton: 2px
	}  
	.contents-left{
    		margin-left: 0 !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

    </style>
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" style="font-size:1.2rem;" href="/bigdata-lecture/">TensorFlow로 시작하는 머신러닝 &amp; 딥러닝 </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bigdata-lecture/lecture">Lecture</a><a class="page-link" href="/bigdata-lecture/orientation">Orientation</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content" style="flex: none;" >
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h3 class="post-title p-name" itemprop="name headline">03. 통계학 기초</h3><p class="page-description">머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다.</p
      <i class="fas fa-tags category-tags-icon"></i><p class="category-tags"> 
      
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
	
	<div class="px-1">
    <a href="https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_03_통계학_기초.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bigdata-lecture/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
	
	<div class="px-1">
<a href="https://github.com/gusdnd852/bigdata-lecture/tree/master/_notebooks/02_03_통계학_기초.ipynb" role="button">
    <img class="notebook-badge-image" src="https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d">
</a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-통계학이란?">1. 통계학이란? </a></li>
<li class="toc-entry toc-h3"><a href="#2.-빈도주의(Frequentist)-VS-베이지안(Bayesian)">2. 빈도주의(Frequentist) VS 베이지안(Bayesian) </a>
<ul>
<li class="toc-entry toc-h4"><a href="#2.1.-빈도주의-확률-(Frequentist)">2.1. 빈도주의 확률 (Frequentist) </a></li>
<li class="toc-entry toc-h4"><a href="#2.2.-베이지안-확률-(Bayesian)">2.2. 베이지안 확률 (Bayesian) </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#3.-베이즈-정리">3. 베이즈 정리 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#3.1.-베이즈-정리의-직관적-이해-(1)">3.1. 베이즈 정리의 직관적 이해 (1) </a></li>
<li class="toc-entry toc-h4"><a href="#3.2.-베이즈-정리의-직관적-이해-(2)">3.2. 베이즈 정리의 직관적 이해 (2) </a></li>
<li class="toc-entry toc-h4"><a href="#3.3.-언제-베이즈-정리를-사용할까?">3.3. 언제 베이즈 정리를 사용할까? </a></li>
<li class="toc-entry toc-h4"><a href="#3.1.-베이즈-정리의-유도">3.1. 베이즈 정리의 유도 </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/02_03_통계학_기초.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>본격적으로 머신러닝의 How에 해당하며, 데이터의 불확실성을 모델링 하는 방법인 통계에 대해 배워봅시다. 물론 통계 이론 전체를 배우려면 엄청나게 방대한 양의 내용을 배워야하지만, 시간이 한정적이므로 현재 머신러닝 모델들을 이해하는 수준까지만 공부하도록합니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h3 id="1.-통계학이란?">
<a class="anchor" href="#1.-%ED%86%B5%EA%B3%84%ED%95%99%EC%9D%B4%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. 통계학이란?<a class="anchor-link" href="#1.-%ED%86%B5%EA%B3%84%ED%95%99%EC%9D%B4%EB%9E%80?"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/90.jpg?raw=true" alt="img"></p>
<p>통계란 무엇일까요? <strong>통계는 다양한 데이터를 가지고 다양한 가설을 세우는 과정</strong>입니다. 그러나 100% 정확한 가설은 거의 존재하지 않습니다. 때문에 우리는 이러한 가설의 불확실성에 대해 생각해봐야합니다. 통계에서는 이러한 <strong>불확실성을 '확률'이라는 도구를 사용하여 표현</strong>합니다. 그런데 문제는 이러한 <strong>'확률'을 바라보는 관점이 학파마다 달랐다</strong> 는 것입니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="2.-빈도주의(Frequentist)-VS-베이지안(Bayesian)">
<a class="anchor" href="#2.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98(Frequentist)-VS-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88(Bayesian)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. 빈도주의(Frequentist) VS 베이지안(Bayesian)<a class="anchor-link" href="#2.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98(Frequentist)-VS-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88(Bayesian)"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/91.png?raw=true" alt="img"></p>
<p>확률을 바라보는 관점에는 크게 두가지가 있습니다. 첫번째는 빈도주의, 두번째는 베이지안입니다. 이들에 대해 알아봅시다.
<br><br></p>
<h4 id="2.1.-빈도주의-확률-(Frequentist)">
<a class="anchor" href="#2.1.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98-%ED%99%95%EB%A5%A0-(Frequentist)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1. 빈도주의 확률 (Frequentist)<a class="anchor-link" href="#2.1.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98-%ED%99%95%EB%A5%A0-(Frequentist)"> </a>
</h4>
<p>빈도주의는 보통 <strong>데이터의 관점</strong>에서, 확률을 생각합니다. 이들 빈도주의자들에게는 확률이란 <strong>사건의 발생 빈도</strong>를 의미합니다. 간단하게 예를 들어 설명하겠습니다. 먼저 "동전을 던졌을 때 앞면이 나올 확률은 $0.5$이다." 라는 가설을 세웁니다. 이때 이 확률 $0.5$는 앞면이라는 <strong>사건이 발생할 확률</strong>을 의미합니다.
<br><br></p>
<p>그리고 나서 동전을 100번, 1000번 던져보고 앞면이 나올 확률이 약 $0.5$와 매우 비슷하다는 것을 확인하고 <strong>가설을 채택</strong>합니다. 빈도주의 학파가 집중하는 불확실성은 Aleatory Uncertainty라고 하는데, 이는 <strong>사건 발생의 Random함이 불확실성을 만든다</strong>라는 관점으로 해석할 수 있습니다. 이러한 Random함을 타파하는 방법은 그저 많이 시행해보는 방법밖에는 없습니다.
<br><br></p>
<h4 id="2.2.-베이지안-확률-(Bayesian)">
<a class="anchor" href="#2.2.-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%99%95%EB%A5%A0-(Bayesian)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2. 베이지안 확률 (Bayesian)<a class="anchor-link" href="#2.2.-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%99%95%EB%A5%A0-(Bayesian)"> </a>
</h4>
<p>베이지안은 보통 <strong>가설의 관점</strong>에서 확률을 생각합니다. 이들 베이지안들에게는 확률이란 <strong>가설의 신뢰도</strong>를 의미합니다. 간단하게 예를 들면, 먼저 "동전을 던졌을 때 앞면이 나올 확률이 절반 정도 일것이라는 가설의 신뢰도는 $0.5$이다"라고 생각합니다. 이 때, 확률 $0.5$는 <strong>주장하는 가설에 대한 신뢰도</strong>입니다. <br><br></p>
<p>그리고 나서 실험을 진행하고 앞면이 나올 확률이 절반정도 된다는 것을 확인하고 <strong>기존 가설의 신뢰도를 업데이트</strong>합니다. 계속해서 확인하면 계속 절반정도만 앞면이 나올 것이고 가설의 신뢰도는 거의 <strong>1.0</strong>에 가까워질 것입니다. <br><br></p>
<p>베이지안 학파가 집중하는 불확실성은 Epidemic Uncertainty라고 하는데, 이는 <strong>가설의 신뢰도가 불확실성을 만든다</strong>라는 관점으로 해석할 수 있습니다. 이러한 관점의 차이를 이해하고 베이즈 정리에 대해 이해해봅시다.<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="3.-베이즈-정리">
<a class="anchor" href="#3.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. 베이즈 정리<a class="anchor-link" href="#3.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/92.jpg?raw=true" alt="">
<br><br></p>
<h4 id="3.1.-베이즈-정리의-직관적-이해-(1)">
<a class="anchor" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(1)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1. 베이즈 정리의 직관적 이해 (1)<a class="anchor-link" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(1)"> </a>
</h4>
<p>베이즈 정리의 수식을 곧바로 이해하는 것은 아직 어려울 것입니다. 따라서 직관적으로 이해할 수 있는 예제를 봅시다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/93.gif?raw=true" alt=""></p>
<p>Steve라는 미국인이 있었습니다. 그는 수줍지만 그래도 남들을 잘 도와주는 편이고 순하고 깨끗한 영혼을 가졌습니다. 그의 성격이 주어졌을 때 그가 농부일지, 도서관 사서일까요?
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/94.gif?raw=true" alt=""></p>
<p>대부분의 여러분이 농부보다는 도서관 사서와 어울린다고 생각하셨을 것입니다. 실제로 조사 결과 사서들의 대부분인 40%가 그러한 성격을 가지고 있었고, 농부들의 10%가 해당 성격을 가지고 있었다고 합시다. 그렇다면 스티브는 사서일까요?<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/95.gif?raw=true" alt=""></p>
<p>그럴싸하지만 아닙니다. 미국에는 일반적으로 1 : 20의 비율로 농부의 수가 도서관 사서보다 훨씬 많습니다. 따라서 Steve는 농부일 수도 있습니다.  <br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/96.gif?raw=true" alt=""></p>
<p>그렇다면 숫자를 10배 늘려서 직접 계산해봅시다. 아직 Steve의 성격이 무엇인지는 모르지만, Steve를 포함해서 미국에서 사서 10명과 농부 200명을 데려왔습니다. 왼쪽에 있는 10명의 사람은 도서관 사서이고, 오른쪽에 있는 200명의 사람은 농부입니다. Steve의 성격을 알기 전까지, Steve가 사서일거라는 주장의 신뢰도는 $\frac{10}{210} = 4.7$퍼센트가 됩니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/97.gif?raw=true" alt=""></p>
<p>그러다가, Steve가 해당 성격을 가졌다는 것을 알게 되었다고 합시다. 이 순간, Steve는 사서의 40%와 농부의 10%에 포함되게 되었습니다. 그래서 그 이외의 나머지 사람들(사서의 60%, 농부의 90%)은 의미가 없어졌습니다.<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/98.gif?raw=true" alt=""></p>
<p>Steve가 해당 성격을 가졌기 때문에, 해당 성격을 가지지 않은 나머지 사람들은 전부 제외하고, P(사서 확률 | 해당 성격을 가졌을 때)을 구합니다. <strong>이 부분이 베이즈 정리의 핵심입니다.</strong> 베이즈 정리는 결국 조건부확률이기 때문에 조건에 해당하지 않는 부분은 전부 제외할 수 있습니다. 따라서 표본공간이 변하게 되고, 계산 결과로 $\frac{4}{4 + 20} = 16.7$퍼센트가 나옵니다. 생각보다는 저조하죠? 그래도 성격에 대한 설명을 읽기 전인 $\frac{10}{210} = 4.7$퍼센트 보다는 높아졌습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/99.gif?raw=true" alt=""></p>
<p>Steve의 성격을 알게 된 것이 그 자체만으로 Steve가 사서일 것이라는 주장에 대한 신뢰도가 되는건 아닙니다. 베이지안 통계에서는 Epidemic Uncertainty를 다루기 때문에 데이터가 곧바로 어떤 분류일지에 대해 생각하는게 아니라, 그 데이터를 확인함으로서 기존 가설을 채택/기각하는 것이 아니라, 가설에 대한 신뢰도를 계속해서 업데이트 합니다. 
<br><br></p>
<h4 id="3.2.-베이즈-정리의-직관적-이해-(2)">
<a class="anchor" href="#3.2.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(2)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2. 베이즈 정리의 직관적 이해 (2)<a class="anchor-link" href="#3.2.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(2)"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/99.gif?raw=true" alt=""></p>
<p>또 다른 예시를 하나 봅시다. 수학자 W씨는 발렌타인 데이 때 좋아하던 여성분에게 초콜릿을 선물 받았습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/100.gif?raw=true" alt=""></p>
<p>W씨는 머릿속에 온갖 생각이 다 들었지만, 그냥 예의상 초콜릿을 준 것 일수도 있습니다. W씨는 수학을 이용해서 초콜릿을 받았을 때, 자신을 얼마나 좋아할지 계산해보기로 했습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/101.gif?raw=true" alt=""></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/102.gif?raw=true" alt=""></p>
<p>직접 물어보기 전에는 상대가 얼만큼 자신을 좋아하는지 알 수 없기 때문에, 좋아하는 것과 싫어하는 것을 50대 50으로 가정하고, 계산을 하기로 했습니다. 50대 50으로 생각하는 것은 "이유 불충분의 원리"라고 부릅니다.</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/103.gif?raw=true" alt=""></p>
<p>직접 몇가지 통계자료를 조사해본 결과 좋아하는 사람에게 초콜릿을 줄 확률은 40%, 주지 않을 확률은 60%이며, 좋아하지 않는 사람에게 초콜릿을 줄 확률은 30%, 주지 않을 확률은 70%라는 것을 알았습니다.</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/104.gif?raw=true" alt=""></p>
<p>예시를 들기 위해 100명의 사람이 있다고 해봅시다. 이유 불충분의 원리에 따라 50명의 사람은 누군가에게 사랑을 받고 있고, 나머지 50명은 받지 못하고 있습니다. 이 때, 사랑을 받고 있으면서 초콜릿을 줄 확률은 40%, 주지 않을 확률은 60% 입니다. 따라서 P(초콜릿 | 사랑받음) = 0.4입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/104.gif?raw=true" alt=""></p>
<p>그리고, 예의상 초콜릿을 받은 사람 중에서 30%는 초콜릿을 받았고, 70%는 초콜릿을 받지 못했습니다. 따라서 P(초콜릿 | 사랑받지않음) = 0.3입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/106.gif?raw=true" alt=""></p>
<p>우리가 원하는 것은 초콜릿을 받았을 때, 사랑받고 있을 확률입니다. 그러나 우리가 가진 데이터는 사랑받고 있을 때, 초콜릿을 가진 데이터입니다. 이 둘은 약간은 상반되는 개념입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/107.gif?raw=true" alt=""></p>
<p>수학자 W씨는 이미 초콜릿을 받았고, 초콜릿을 받은 상황만 고려하면 됩니다. 따라서 조건에 해당되는 부분만 남기고 모두 지웁니다. 이 과정에서 표본공간이 변하게 됩니다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/108.gif?raw=true" alt="">
초콜릿을 받은 전체 35명의 사람 중, 20명이 사랑받고 있으니 P(사랑받고 있음 | 초콜릿)은 $\frac{20}{35}$로 총 57%가 됩니다.</p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/109.gif?raw=true" alt=""></p>
<p>따라서 수학자 W씨는 여성분이 자신을 좋아할 확률을 기존에 생각했던 50%에서 57%로 업데이트합니다. 
<br><br></p>
<h4 id="3.3.-언제-베이즈-정리를-사용할까?">
<a class="anchor" href="#3.3.-%EC%96%B8%EC%A0%9C-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%A0%EA%B9%8C?" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3. 언제 베이즈 정리를 사용할까?<a class="anchor-link" href="#3.3.-%EC%96%B8%EC%A0%9C-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%A0%EA%B9%8C?"> </a>
</h4>
<p>빈도주의자들의 방법은 시행을 계속하며 데이터를 계속 변화시킵니다. 그러나 베이즈 정리는 데이터가 고정되었을 때 사용합니다. 머신러닝에서 데이터는 정말 귀한 자원입니다. 따라서 아무</p>
<h4 id="3.1.-베이즈-정리의-유도">
<a class="anchor" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%9C%A0%EB%8F%84" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1. 베이즈 정리의 유도<a class="anchor-link" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%9C%A0%EB%8F%84"> </a>
</h4>
<p>(1) 우리는 이전 확률시간에 조건부 확률에 대해 배웠습니다. 베이즈정리는 곧 조건부 확률과 동일합니다. 아래와 같은 식을 이용해 베이즈 정리를 유도할 수 있습니다.</p>
<ul>
<li>B가 일어날 때, A가 일어날 사건에 대한 조건부 확률은 $P(A|B) = \frac{P(A \cap B)}{P(B)}$입니다. </li>
<li>A가 일어날 때, B가 일어날 사건에 대한 조건부 확률은 $P(B|A) = \frac{P(B \cap A)}{P(A)}$입니다. 
<br><br>
</li>
</ul>
<p>(2) 두 식의 분모를 없게 하기 위해 분모를 곱한다면 아래와 같아집니다.</p>
<ul>
<li>$P(A|B) \cdot P(B) = P(A \cap B)$</li>
<li>$P(B|A) \cdot P(A) = P(B \cap A)$
<br><br>
</li>
</ul>
<p>(3) 이 때, $P(A \cap B)$와 $P(B \cap A)$는 동일합니다. 따라서, 아래와 같은 식의 유도가 가능해집니다.</p>
<ul>
<li>$P(A|B) \cdot P(B) = P(B|A) \cdot P(A)$ 
<br><br>
</li>
</ul>
<p>(4) 양 변을 $P(B)$로 나누면 베이즈 정리가 완성됩니다.</p>
<ul>
<li>$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$</li>
</ul>

</div>
</div>
</div>
</div>





    <br><br>
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = “https://gusdnd852.github.io/statics_theory“;
this.page.identifier = 03. 통계학 기초;
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://gusdnd852.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<a class="u-url" href="/bigdata-lecture/statics_theory" hidden></a>
</article>

<script>
	function run_exec(){
		if("colab" === "binder"){
			if(confirm("이 노트북은 CPU환경인 Binder에서 소스코드를 실행합니다.")){
				var binder_path = "https://mybinder.org/v2/gh/gusdnd852/bigdata-lecture/master?filepath=_notebooks%2F02_03_%ED%86%B5%EA%B3%84%ED%95%99_%EA%B8%B0%EC%B4%88.ipynb";
				alert("Binder는 Docker기반의 컨테이너를 직접 빌드하기 때문에 시간이 다소 소요됩니다. Build logs의 show버튼을 눌러서 진행상황을 확인하세요");
				window.open(binder_path, "_blank");
							
			}
			
		}else if("colab" === "colab"){
			if(confirm("이 노트북은 GPU환경인 Google Colab에서 소스코드를 실행합니다.")){
				var colab_path = "https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_03_통계학_기초.ipynb";
				alert("GPU 설정을 위해 Colab 상단 메뉴중, 런타임 → 런타임 유형변경에서 GPU를 선택해주세요. (미선택시 CPU에서 실행됨)");
				window.open(colab_path, "_blank");
			}			
		}else{
			alert("해당 실행환경은 구동이 불가능합니다. Binder와 Google Colab 중 한가지를 선택해주세요.");
		}
	}

    function code_listener() {
	var input_area = document.getElementsByClassName("input_area");

	for(var i = 0 ; i < input_area.length; i++){
		input_area[i].onclick = function(){
			run_exec();
    		}
	}

    } 

    code_listener();
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bigdata-lecture/"></data>

  <div class="wrapper">
      <div class="footer-col">
	<footer-elem class=px-2>
		<a href="https://github.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#github"></use>
			</svg>
			<span class="username">Github</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://youtube.com/channel/UCb7DJAuj1LulbY8WEr2EDUw">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#youtube"></use>
			</svg>
			<span class="username">Youtube</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://www.facebook.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#facebook"></use>
			</svg>
			<span class="username">Facebook</span>
		</a>
	</footer-elem>
	<br>
	<div class="copyright">
		© Copyright 2020 Hyunwoong Go. All rights reserved.
	</div>
      </div>
  </div>
</footer>
</body>
</html>
