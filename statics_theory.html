<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/bigdata-lecture/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>통계학 기초 | TensorFlow로 시작하는 머신러닝 &amp; 딥러닝</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="통계학 기초" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다." />
<meta property="og:description" content="머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다." />
<link rel="canonical" href="https://gusdnd852.github.io/bigdata-lecture/statics_theory" />
<meta property="og:url" content="https://gusdnd852.github.io/bigdata-lecture/statics_theory" />
<meta property="og:site_name" content="TensorFlow로 시작하는 머신러닝 &amp; 딥러닝" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-18T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gusdnd852.github.io/bigdata-lecture/statics_theory","@type":"BlogPosting","headline":"통계학 기초","dateModified":"2020-07-18T00:00:00-05:00","datePublished":"2020-07-18T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gusdnd852.github.io/bigdata-lecture/statics_theory"},"description":"머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="/bigdata-lecture/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://gusdnd852.github.io/bigdata-lecture/feed.xml" title="TensorFlow로 시작하는 머신러닝 & 딥러닝" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>

     @media screen and (max-width : 511px) {
         body { 
		font-size: 0.84rem; 
	}pre, code, blockquote {
		font-size: 0.84rem !important;
	}
	.katex, .mord {
		font: normal 1.02em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.90em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 0.9rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}
	.mobile_hide{
		display: none;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 

     @media screen and (min-width : 512px) and (max-width : 767px) {
         body { 
		font-size: 0.92rem; 
	}pre, code, blockquote {
		font-size: 0.92rem !important;
	}
	.katex, .mord {
		font: normal 1.04em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.92em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.0rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}    
	.mobile_hide{
		display: none !important;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 
     @media screen and (min-width : 768px) and (max-width : 1024px) {
         body {
		 font-size: 1.0rem; 
	}pre, code, blockquote {
		font-size: 1.0rem !important;
	}
	.katex, .mord {
		font: normal 1.05em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.93em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.1rem !important;
	}
	.contents-right, .contents-left {
		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}  
	.contents-left{
		margin-left: 20px !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

     @media screen and (min-width : 1025px) {
         body { 
		font-size: 1.08rem; 
	}pre, code, blockquote {
		font-size: 1.08rem !important;
	}
     	.katex, .mord {
		font: normal 1.06em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.94em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.2rem !important;
	}
	.contents-right, .contents-left {
    		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}
	.contents-right > h2, .contents-left > h2 {
    		margin-botton: 2px
	}  
	.contents-left{
    		margin-left: 0 !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

    </style>
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" style="font-size:1.2rem;" href="/bigdata-lecture/">TensorFlow로 시작하는 머신러닝 &amp; 딥러닝 </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bigdata-lecture/lecture">Lecture</a><a class="page-link" href="/bigdata-lecture/orientation">Orientation</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content" style="flex: none;" >
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h3 class="post-title p-name" itemprop="name headline">03. 통계학 기초</h3><p class="page-description">머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다.</p
      <i class="fas fa-tags category-tags-icon"></i><p class="category-tags"> 
      
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
	
	<div class="px-1">
    <a href="https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_03_통계학_기초.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bigdata-lecture/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
	
	<div class="px-1">
<a href="https://github.com/gusdnd852/bigdata-lecture/tree/master/_notebooks/02_03_통계학_기초.ipynb" role="button">
    <img class="notebook-badge-image" src="https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d">
</a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-통계학이란?">1. 통계학이란? </a></li>
<li class="toc-entry toc-h3"><a href="#2.-빈도주의(Frequentist)-VS-베이지안(Bayesian)">2. 빈도주의(Frequentist) VS 베이지안(Bayesian) </a>
<ul>
<li class="toc-entry toc-h4"><a href="#2.1.-빈도주의-확률-(Frequentist)">2.1. 빈도주의 확률 (Frequentist) </a></li>
<li class="toc-entry toc-h4"><a href="#2.2.-베이지안-확률-(Bayesian)">2.2. 베이지안 확률 (Bayesian) </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#3.-베이즈-정리">3. 베이즈 정리 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#3.1.-베이즈-정리의-직관적-이해-(1)">3.1. 베이즈 정리의 직관적 이해 (1) </a></li>
<li class="toc-entry toc-h4"><a href="#3.2.-베이즈-정리의-직관적-이해-(2)">3.2. 베이즈 정리의 직관적 이해 (2) </a></li>
<li class="toc-entry toc-h4"><a href="#3.3.-왜-머신러닝은-베이지안인가?">3.3. 왜 머신러닝은 베이지안인가? </a></li>
<li class="toc-entry toc-h4"><a href="#3.4.-베이즈-정리-속-각-부분의-의미">3.4. 베이즈 정리 속 각 부분의 의미 </a></li>
<li class="toc-entry toc-h4"><a href="#3.5.-베이즈-정리의-유도">3.5. 베이즈 정리의 유도 </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#4.-Softmax와-Sigmoid-함수">4. Softmax와 Sigmoid 함수 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#4.1.-Softmax-함수">4.1. Softmax 함수 </a></li>
<li class="toc-entry toc-h4"><a href="#4.2.-Sigmoid-함수">4.2. Sigmoid 함수 </a></li>
<li class="toc-entry toc-h4"><a href="#4.3.-...-어렵나요?-그래도-기억해야할-것!">4.3. ... 어렵나요? 그래도 기억해야할 것! </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/02_03_통계학_기초.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>본격적으로 머신러닝의 How에 해당하며, 데이터의 불확실성을 모델링 하는 방법인 통계에 대해 배워봅시다. 물론 통계 이론 전체를 배우려면 엄청나게 방대한 양의 내용을 배워야하지만, 시간이 한정적이므로 현재 머신러닝 모델들을 이해하는 수준까지만 공부하도록합니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h3 id="1.-통계학이란?">
<a class="anchor" href="#1.-%ED%86%B5%EA%B3%84%ED%95%99%EC%9D%B4%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. 통계학이란?<a class="anchor-link" href="#1.-%ED%86%B5%EA%B3%84%ED%95%99%EC%9D%B4%EB%9E%80?"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/90.jpg?raw=true" alt="img"></p>
<p>통계란 무엇일까요? <strong>통계는 다양한 데이터를 가지고 다양한 가설을 세우는 과정</strong>입니다. 그러나 100% 정확한 가설은 거의 존재하지 않습니다. 때문에 우리는 이러한 가설의 불확실성에 대해 생각해봐야합니다. 통계에서는 이러한 <strong>불확실성을 '확률'이라는 도구를 사용하여 표현</strong>합니다. 그런데 문제는 이러한 <strong>'확률'을 바라보는 관점이 학파마다 달랐다</strong> 는 것입니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="2.-빈도주의(Frequentist)-VS-베이지안(Bayesian)">
<a class="anchor" href="#2.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98(Frequentist)-VS-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88(Bayesian)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. 빈도주의(Frequentist) VS 베이지안(Bayesian)<a class="anchor-link" href="#2.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98(Frequentist)-VS-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88(Bayesian)"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/91.png?raw=true" alt="img"></p>
<p>확률을 바라보는 관점에는 크게 두가지가 있습니다. 첫번째는 빈도주의, 두번째는 베이지안입니다. 이들에 대해 알아봅시다.
<br><br></p>
<h4 id="2.1.-빈도주의-확률-(Frequentist)">
<a class="anchor" href="#2.1.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98-%ED%99%95%EB%A5%A0-(Frequentist)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1. 빈도주의 확률 (Frequentist)<a class="anchor-link" href="#2.1.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98-%ED%99%95%EB%A5%A0-(Frequentist)"> </a>
</h4>
<p>빈도주의는 보통 <strong>데이터의 관점</strong>에서, 확률을 생각합니다. 이들 빈도주의자들에게는 확률이란 <strong>사건의 발생 빈도</strong>를 의미합니다. 간단하게 예를 들어 설명하겠습니다. 먼저 "동전을 던졌을 때 앞면이 나올 확률은 $0.5$이다." 라는 가설을 세웁니다. 이때 이 확률 $0.5$는 앞면이라는 <strong>사건이 발생할 확률</strong>을 의미합니다.
<br><br></p>
<p>그리고 나서 동전을 100번, 1000번 던져보고 앞면이 나올 확률이 약 $0.5$와 매우 비슷하다는 것을 확인하고 <strong>가설을 채택</strong>합니다. 즉, 빈도주의자들은 어떤 시스템을 모델링할 때, 가설(모델)을 고정해둔 상태에서 데이터를 계속 실험해봐서 가설이 맞는지 안맞는지 보고, 가설을 채택 혹은 기각합니다. 빈도주의 학파가 집중하는 불확실성은 Aleatory Uncertainty라고 하는데, 이는 <strong>사건 발생의 Random함이 불확실성을 만든다</strong>라는 관점으로 해석할 수 있습니다. 그들은 그들의 가설을 믿습니다. 그리고 데이터를 불신합니다.
<br><br></p>
<h4 id="2.2.-베이지안-확률-(Bayesian)">
<a class="anchor" href="#2.2.-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%99%95%EB%A5%A0-(Bayesian)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2. 베이지안 확률 (Bayesian)<a class="anchor-link" href="#2.2.-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%99%95%EB%A5%A0-(Bayesian)"> </a>
</h4>
<p>베이지안은 보통 <strong>가설의 관점</strong>에서 확률을 생각합니다. 이들 베이지안들에게는 확률이란 <strong>가설의 신뢰도</strong>를 의미합니다. 간단하게 예를 들면, 먼저 "동전을 던졌을 때 앞면이 나올 확률이 절반 정도 일것이라는 가설의 신뢰도는 $0.5$이다"라고 생각합니다. 이 때, 확률 $0.5$는 <strong>주장하는 가설에 대한 신뢰도</strong>입니다. <br><br></p>
<p>그리고 나서 실험을 진행하고 앞면이 나올 확률이 절반정도 된다는 것을 확인하고 <strong>기존 가설의 신뢰도를 업데이트</strong>합니다. 계속해서 확인하면 계속 절반정도만 앞면이 나올 것이고 가설의 신뢰도는 거의 <strong>1.0</strong>에 가까워질 것입니다. 즉, 베이지안 학파는 어떤 시스템을 모델링할 때, 데이터를 고정한 상태에서 가설을 점점 잘 맞게 변화시켜나갑니다. 베이지안 학파가 집중하는 불확실성은 Epidemic Uncertainty라고 하는데, 이는 <strong>가설의 신뢰도가 불확실성을 만든다</strong>라는 관점으로 해석할 수 있습니다. 그들은 그들의 모델을 불신합니다. 그리고 데이터를 믿습니다. 그리고 이 것이 베이지안이 빈도주의를 꺾고 세상을 지배하게 된 이유입니다. 이러한 관점의 차이를 이해하고 베이즈 정리에 대해 이해해봅시다.<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="3.-베이즈-정리">
<a class="anchor" href="#3.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. 베이즈 정리<a class="anchor-link" href="#3.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/113.gif?raw=true" alt=""></p>
<p><br><br></p>
<h4 id="3.1.-베이즈-정리의-직관적-이해-(1)">
<a class="anchor" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(1)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1. 베이즈 정리의 직관적 이해 (1)<a class="anchor-link" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(1)"> </a>
</h4>
<p>베이즈 정리의 수식을 곧바로 이해하는 것은 아직 어려울 것입니다. 따라서 직관적으로 이해할 수 있는 예제를 봅시다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/93.gif?raw=true" alt=""></p>
<p>Steve라는 미국인이 있었습니다. 그는 수줍지만 그래도 남들을 잘 도와주는 편이고 순하고 깨끗한 영혼을 가졌습니다. Steve의 성격이 주어졌을 때, 그는 농부일까요? 도서관 사서일까요?
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/94.gif?raw=true" alt=""></p>
<p>대부분의 여러분이 농부보다는 도서관 사서와 어울린다고 생각하셨을 것입니다. 실제로 조사 결과 사서들의 대부분인 40%가 그러한 성격을 가지고 있었고, 농부들의 10%가 해당 성격을 가지고 있었다고 가정 해봅시다. 그렇다면 스티브는 사서일까요?<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/95.gif?raw=true" alt=""></p>
<p>그럴싸하지만 아닙니다. 미국에는 일반적으로 1 : 20의 비율로 농부의 수가 도서관 사서보다 훨씬 많습니다. 따라서 Steve는 농부일 수도 있습니다.  <br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/96.gif?raw=true" alt=""></p>
<p>그렇다면 숫자를 10배 늘려서 직접 계산해봅시다. 아직 Steve의 성격이 무엇인지는 모르지만, Steve를 포함해서 미국에서 사서 10명과 농부 200명을 데려왔습니다. 왼쪽에 있는 10명의 사람은 도서관 사서이고, 오른쪽에 있는 200명의 사람은 농부입니다. Steve의 성격을 알기 전까지, Steve가 사서일거라는 주장의 신뢰도는 $\frac{10}{210} = 4.7$퍼센트가 됩니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/97.gif?raw=true" alt=""></p>
<p>그러다가, Steve가 해당 성격을 가졌다는 것을 알게 되었다고 합시다. 이 순간, Steve는 사서의 40%와 농부의 10%에 포함되게 되었습니다. 그래서 그 이외의 나머지 사람들(사서의 60%, 농부의 90%)은 의미가 없어졌습니다.<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/98.gif?raw=true" alt=""></p>
<p>Steve가 해당 성격을 가졌기 때문에, 해당 성격을 가지지 않은 나머지 사람들은 전부 제외하고, P(사서 확률 | 해당 성격을 가졌을 때)을 구합니다. Steve의 직업이 무엇이든, 성격이 그러한 것은 사실이므로 조건에 부합하지 않는 부분은 전부 제거합니다. <strong>이 부분이 베이즈 정리의 핵심입니다.</strong> 베이즈 정리는 결국 조건부확률이기 때문에 조건에 해당하지 않는 부분은 전부 제외할 수 있습니다. 따라서 표본공간이 변하게 되고, 계산 결과로 $\frac{4}{4 + 20} = 16.7$퍼센트가 나옵니다. 생각보다는 저조하죠? 그래도 성격에 대한 설명을 읽기 전인 $\frac{10}{210} = 4.7$퍼센트 보다는 높아졌습니다.
<br><br></p>
<p>Steve의 성격을 알게 된 것이 그 자체만으로 Steve가 사서일 것이라는 주장에 대한 신뢰도가 되는건 아닙니다. 베이지안 통계에서는 Epidemic Uncertainty를 다루기 때문에 데이터가 곧바로 어떤 분류일지에 대해 생각하는게 아니라, 그 데이터를 확인함으로서 기존 가설을 채택/기각하는 것이 아니라, 가설에 대한 신뢰도를 계속해서 업데이트 합니다. 
<br><br></p>
<h4 id="3.2.-베이즈-정리의-직관적-이해-(2)">
<a class="anchor" href="#3.2.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(2)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2. 베이즈 정리의 직관적 이해 (2)<a class="anchor-link" href="#3.2.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(2)"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/99.gif?raw=true" alt=""></p>
<p>또 다른 예시를 하나 봅시다. 수학자 W씨는 발렌타인 데이 때 좋아하던 여성분에게 초콜릿을 선물 받았습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/100.gif?raw=true" alt=""></p>
<p>W씨는 머릿속에 온갖 생각이 다 들었지만, 여성분이 그냥 별뜻 없이 초콜릿을 준 것 일수도 있습니다. W씨는 수학을 이용해서 초콜릿을 받았을 때, 자신을 얼마나 좋아할지 계산해보기로 했습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/101.gif?raw=true" alt=""></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/102.gif?raw=true" alt=""></p>
<p>직접 물어보기 전에는 상대가 얼만큼 자신을 좋아하는지 알 수 없기 때문에, 좋아하는 것과 싫어하는 것을 50대 50으로 가정하고, 계산을 하기로 했습니다. 50대 50으로 생각하는 것은 "이유 불충분의 원리"라고 부릅니다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/103.gif?raw=true" alt=""></p>
<p>직접 몇가지 통계자료를 조사해본 결과 좋아하는 사람에게 초콜릿을 줄 확률은 40%, 주지 않을 확률은 60%이며, 좋아하지 않는 사람에게 초콜릿을 줄 확률은 30%, 주지 않을 확률은 70%라는 것을 알았습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/104.gif?raw=true" alt=""></p>
<p>예시를 들기 위해 100명의 사람이 있다고 해봅시다. 이유 불충분의 원리에 따라 50명의 사람은 누군가에게 사랑을 받고 있고, 나머지 50명은 받지 못하고 있습니다. 이 때, 사랑을 받고 있으면서 초콜릿을 줄 확률은 40%, 주지 않을 확률은 60% 입니다. 따라서 P(초콜릿 | 사랑받음) = 0.4입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/105.gif?raw=true" alt=""></p>
<p>그리고, 예의상 초콜릿을 받은 사람 중에서 30%는 초콜릿을 받았고, 70%는 초콜릿을 받지 못했습니다. 따라서 P(초콜릿 | 사랑받지않음) = 0.3입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/106.gif?raw=true" alt=""></p>
<p>우리가 원하는 것은 초콜릿을 받았을 때, 사랑받고 있을 확률입니다. 그러나 우리가 가진 데이터는 사랑받고 있을 때, 초콜릿을 가진 데이터입니다. 이 둘은 약간은 상반되는 개념입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/107.gif?raw=true" alt=""></p>
<p>수학자 W씨는 이미 초콜릿을 받았고, 초콜릿을 받은 상황만 고려하면 됩니다. 따라서 조건에 해당되는 부분만 남기고 모두 지웁니다. 이 과정에서 표본공간이 변하게 됩니다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/108.gif?raw=true" alt=""></p>
<p>초콜릿을 받은 전체 35명의 사람 중, 20명이 사랑받고 있으니 P(사랑받고 있음 | 초콜릿)은 $\frac{20}{35}$로 총 57%가 됩니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/109.gif?raw=true" alt=""></p>
<p>따라서 수학자 W씨는 여성분이 자신을 좋아할 확률을 기존에 생각했던 50%에서 57%로 업데이트합니다. 그러나 57%는 너무 낮습니다. 수학자 W씨는 이대로 포기해야할까요?
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/112.gif?raw=true" alt=""></p>
<p>그렇지 않습니다. 베이즈 정리는 데이터가 축적되면 축적될수록 훨씬 강해집니다. 수학자 W씨는 초콜릿을 받은 이후, 같이 밥을 먹은 사건, 밤새 전화를 했던 사건 등을 토대로 점점 사후확률을 더욱 높게 업데이트해 나갑니다. 그리고, 이러한 과정은 우리가 배우고 있는 머신러닝/딥러닝 알고리즘과 정확히 일치합니다.
<br><br></p>
<h4 id="3.3.-왜-머신러닝은-베이지안인가?">
<a class="anchor" href="#3.3.-%EC%99%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9D%80-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%EC%9D%B8%EA%B0%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3. 왜 머신러닝은 베이지안인가?<a class="anchor-link" href="#3.3.-%EC%99%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9D%80-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%EC%9D%B8%EA%B0%80?"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/110.png?raw=true" alt="">
<br><br></p>
<ul>
<li>머신러닝에 쓰이는 데이터셋은 고정적이다.</li>
</ul>
<p>빈도주의자들은 가설을 고정해놓고, 시행을 계속하며 랜덤한 데이터를 계속 만들어냅니다. 그러나 베이지안은 데이터를 고정해놓고, 시행을 계속하며 기존 가설을 점점 더 좋게 업데이트 합니다. 머신러닝에 있어서 데이터는 정말 귀중한 자원이며, 일정한 패턴을 띄고 있어야합니다. 따라서 무제한적으로 반복시행하여 만들어낸 랜덤 데이터 등이 머신러닝 데이터셋을 대체할 수 없습니다. 랜덤생성을 하지 않는다면 기존에 가진 데이터 셋으로 학습해야하는데, 무제한적으로 반복실험 할 만큼의 데이터를 구하는 것 자체가 너무나 어려운 일입니다.
<br><br></p>
<ul>
<li>머신러닝의 '학습'은 사실 사후확률 업데이트이다.</li>
</ul>
<p>몇몇 알고리즘(K-NN, 트리기반 등등)은 해당하지 않는 이야기이긴 합니다만, 그 외의 많은 머신러닝 분류 알고리즘과 모든 신경망 기반 딥러닝 분류 알고리즘들은 최종 출력값을 Sigmoid나 Softmax 함수 등에 연결하여 최종적인 사후확률을 구해냅니다. 이때 사용되는 <strong>Sigmoid 함수와 Softmax 함수가 바로 베이즈 정리 공식과 100% 동일한 함수</strong>입니다.
<br><br></p>
<p>즉, 대부분의 머신러닝/딥러닝 알고리즘의 출력값은 해당 데이터가 맞을 확률보다는, 해당 가설의 신뢰도를 의미합니다. 이렇게 구해진 <strong>사후확률은 다시 사전확률이 되고, 다시 한번 관측을 수행함으로서 더 나은 사후확률을 계속해서 만들어냅니다</strong>. 그래서 몇몇 전문가들은 머신러닝/딥러닝을 중/고급 통계응용이라고도 부릅니다.
<br><br></p>
<ul>
<li>머신러닝은 베이지안들의 철학, 그 자체이다.</li>
</ul>
<p>빈도주의자들은 가설(모델)을 아예 고정해놓고 데이터를 지속적으로 변화해서 실험한 뒤, 가설(모델)이 맞는 가설(모델)인지, 아닌지 평가합니다. 이러한 방식을 쓰면, 가설(모델)은 계속 고정되어있습니다. 만약 가설이 틀리면 사람이 직접 가설을 바꾸고 다시 테스트합니다. 따라서 가설 자체를 사람이 수동적으로 설정하고 데이터를 넣어보는 것이 빈도주의자들의 방식입니다. 이게 바로 Rule based 방식입니다. 규칙(if문)을 사람이 정합니다. 그리고 데이터를 넣어서 결과를 봅니다. 
<br><br></p>
<p>베이지안은 데이터셋을 고정해놓고, 가설(모델)을 지속적으로 변화시킵니다. 데이터를 설명할 분류/회귀를 위한 선이나 면(모델)을 계속 찾아나가면서 모델에 대한 신뢰도(정확도)를 계속 개선(업데이트)합니다. 이러한 방식을 쓰면, 데이터는 계속 고정되어있지만 모델이 계속 변합니다. 즉, 데이터는 고정하되, 모델이 계속 변하게 파라미터를 학습시키는 것이 베이지안들의 방식입니다. 그리고 이게 바로 머신러닝입니다. 규칙을 컴퓨터가 찾습니다. 대신 데이터는 고정되어있습니다. 
<br><br></p>
<p>제가 존경하는 조지워싱턴 대학의 최재화교수님은 베이지안을 부모 찾기라고 비유하셨습니다. (<a href="https://www.youtube.com/watch?v=Gpi6Uuw6DJM">https://www.youtube.com/watch?v=Gpi6Uuw6DJM</a>) 이제 막 태어난 많은 아이들이 있고 이들의 부모를 찾아야할때, 빈도주의자들은 부모입장에서 맞는 아이들을 매칭시키는가 하면, 베이지안은 아이들의 입장에서 부모들을 매칭시키는 것을 떠올리시면 정확합니다. 두 사상 모두 결국에 하고자 하는 것은 같지만 관점이 정 반대입니다.
<br><br></p>
<h4 id="3.4.-베이즈-정리-속-각-부분의-의미">
<a class="anchor" href="#3.4.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC-%EC%86%8D-%EA%B0%81-%EB%B6%80%EB%B6%84%EC%9D%98-%EC%9D%98%EB%AF%B8" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4. 베이즈 정리 속 각 부분의 의미<a class="anchor-link" href="#3.4.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC-%EC%86%8D-%EA%B0%81-%EB%B6%80%EB%B6%84%EC%9D%98-%EC%9D%98%EB%AF%B8"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/92.jpg?raw=true" alt="">
<br><br></p>
<ul>
<li>Prior (사전확률) : $P(A)$</li>
</ul>
<p>Prior는 사전확률을 의미합니다. 사전확률은 데이터를 관측하기 전에 미리 예상하는 확률입니다. Steve의 예시에서는 미국의 농부 : 사서 = 20 : 1의 비율이 사전확률이였고, 발렌타인데이 예시는 이유 불충분의 원리로 사랑받음 : 사랑받지않음 = 50 : 50으로 설정한 것이 바로 사전확률입니다.
<br><br></p>
<ul>
<li>Likelihood (우도) : $P(B|A)$ </li>
</ul>
<p>베이즈 정리에서 가장 헷갈리는 영역인 Likelihood입니다. 그러나 어려울 것 하나도 없습니다. 예시를 생각해봅시다. Steve의 예시에서는 40%의 사서가 그러한 성격을 갖고 있다는 정보가 우도입니다. 발렌타인데이 예시에서는 보통 사랑하는 상태에서는 40%의 확률로 초콜릿을 준다는 정보가 Likelihood입니다. 우리가 구하고자 하는 확률은 $P(A|B)$입니다. 그러나 우도는 반대로 $P(B|A)$입니다. <br><br></p>
<p>이 것을 머신러닝의 문제로 확장해서 생각해봅시다. 분류 등의 문제를 풀 때, 우리가 알고 싶은 것은 $P(D|M)$입니다. 여기에서 D는 Data, M는 Model입니다. 우리가 머신러닝 모델을 사용, 테스트, 서비스 등을 할 때, 알고 싶은 것은 특정한 모델이 주어져서 고정되어 있을 때, 새로운 데이터를 잘 맞추는지에 대한 확률입니다. 이 것은 모델은 고정해둔 상태에서 데이터를 계속 변화시키는 빈도주의적 관점과 흡사하며, 이후에 말할 사후확률에 해당합니다.<br><br></p>
<p>그러나 우리가 <strong>학습</strong>할 때는 $P(M|D)$이 최대화되도록 학습시킵니다. (Maximum Likelihood Estimation) 우리는 모델을 학습시킬 때, 고정된 데이터셋을 사용합니다. 그리고 좋은 가설을 찾고자 합니다. Data가 고정되어 주어진 상태에서, 그 가설이 정답을 잘 맞추는 지에 대한 것이 Likelihood입니다. 이 것은 데이터를 고정한 상태에서 가설을 변화시키는 것이기 때문에 베이지안 관점과 흡사합니다. 머신러닝이 베이지안 그 자체라는 말이 어느정도는 느낌이 오시나요? 어디에서도 Likelihood를 이렇게 빈도주의와 베이지안의 철학 차이로 연결해서 설명하지 않습니다. 저는 이 내용을 깨닫는데 2년이 걸렸습니다. 여러분은 겨우 이틀만에 제 2년의 경험을 흡수하신 겁니다.<br><br></p>
<ul>
<li>Evidence : $P(B)$</li>
</ul>
<p>Evidence는 '증거'라고 불리는데, 사실 별로 와닿지 않아서 그냥 한국어 번역을 쓰지 않았습니다. Evidence는 $P(B)$입니다. 그러니까, 전체에서 $P(B)$라는 것은 필요 없는 것 다 지우고, 조건에 맞는 것만 보겠다라는 것입니다. 사실 표본공간을 $P(B)$로 제한하는 것이 베이지안 분석 (Bayesian Analysis)의 핵심 중 한가지인데, 머신러닝에서는 이 Evidence가 별로 중요하지 않습니다. 머신러닝에서는 $P(B)$는 $P(M)$에 해당하며, 현재 모델이 예측하려고 하는 모든 클래스를 이야기합니다. 만약 개와 고양이와 기린이을 분류하는 모델이 있다면 $P(M)$는 개 + 고양이 + 기린의 확률(=1)이며, $P(M|D)$(likelihood)는 데이터셋이 고정되어있을 때, 모델이 개 혹은 고양이 혹은 기린, 이들을 각각 맞출 확률을 이야기합니다.
<br><br></p>
<ul>
<li>Posterior (사후확률) : $P(B|A)$ </li>
</ul>
<p>사후확률은 위의 Likelihood 부분에서 말한대로 데이터를 모두 보고나서 평가되는 요소로 이전의 사전확률에 대비해 새롭게 업데이트 된 확률입니다. 머신러닝에서는 모델을 사용, 테스트, 서비스 등을 할 때를 이 사후확률의 개념이 정확도와 비슷하게 사용됩니다. 정확도는 학습을 마친 모델이 고정되어있을 때, 처음보는 새로운 유저의 데이터가 입력되었다면 그 데이터를 맞출 확률을 사후확률에 해당합니다. 개념이 확실히 잡히시나요?
<br><br></p>
<h4 id="3.5.-베이즈-정리의-유도">
<a class="anchor" href="#3.5.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%9C%A0%EB%8F%84" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.5. 베이즈 정리의 유도<a class="anchor-link" href="#3.5.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%9C%A0%EB%8F%84"> </a>
</h4>
<p>(1) 우리는 이전 확률시간에 조건부 확률에 대해 배웠습니다. 베이즈정리는 조건부 확률과 동일합니다. 아래와 같은 식을 이용해 베이즈 정리의 공식을 유도할 수 있습니다.</p>
<ul>
<li>B가 일어날 때, A가 일어날 사건에 대한 조건부 확률은 $P(A|B) = \frac{P(A \cap B)}{P(B)}$입니다. </li>
<li>A가 일어날 때, B가 일어날 사건에 대한 조건부 확률은 $P(B|A) = \frac{P(B \cap A)}{P(A)}$입니다. 
<br><br>
</li>
</ul>
<p>(2) 두 식의 분모를 없게 하기 위해 분모를 곱한다면 아래와 같아집니다.</p>
<ul>
<li>$P(A|B) \cdot P(B) = P(A \cap B)$</li>
<li>$P(B|A) \cdot P(A) = P(B \cap A)$
<br><br>
</li>
</ul>
<p>(3) 이 때, $P(A \cap B)$와 $P(B \cap A)$는 동일합니다. 따라서, 아래와 같은 식의 유도가 가능해집니다.</p>
<ul>
<li>$P(A|B) \cdot P(B) = P(B|A) \cdot P(A)$ 
<br><br>
</li>
</ul>
<p>(4) 양 변을 $P(B)$로 나누면 베이즈 정리가 완성됩니다.</p>
<ul>
<li>$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$</li>
</ul>
<p><br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Softmax와-Sigmoid-함수">
<a class="anchor" href="#4.-Softmax%EC%99%80-Sigmoid-%ED%95%A8%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Softmax와 Sigmoid 함수<a class="anchor-link" href="#4.-Softmax%EC%99%80-Sigmoid-%ED%95%A8%EC%88%98"> </a>
</h3>
<p>이제 앞으로 머신러닝 모델들을 쭉 배우면서 계속 마주칠 Softmax함수와 Sigmoid함수입니다. 이들은 위에서 설명한 베이즈 정리와 정확히 동일한 함수입니다. 이 부분의 내용은 상당히 어렵기 때문에 이해가 안가시면 그냥 패스하셔도 좋습니다. 이해가 되시는 분들만 따라오셔도 무방합니다. <strong>그러나 반드시 알아두셔야할 것은 Softmax와 Sigmoid라는 함수가 있고 앞으로 매번 나올 것인데, 이 함수들은 베이즈 정리와 동일하며, 확률값을 나타낸다는 것입니다.</strong> 예를 들어 각각 강아지, 고양이라는 2개의 클래스가 있다고 해봅시다. 그렇다면, 우리가 원하는 것(Accuracy)는 모델이 주어졌을 때 강아지를 잘 맞출 확률 $P(Dog|Model)$, 모델이 주어졌을 때 고양이를 잘 맞출 확률입니다.$P(Cat|Model)$입니다. <br><br></p>
<h4 id="4.1.-Softmax-함수">
<a class="anchor" href="#4.1.-Softmax-%ED%95%A8%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1. Softmax 함수<a class="anchor-link" href="#4.1.-Softmax-%ED%95%A8%EC%88%98"> </a>
</h4>
<p>강아지만 먼저 예시로 보겠습니다. 베이즈 정리로 풀어서 서술하면 아래와 같습니다.</p>
<p><br>
<br>

$$P(Dog|Model) = \frac{P(Model|Dog)P(Dog)}{P(Model)}$$

<br>
<br></p>
<p>이때, $P(Model)$은 모델이 현재 맞추려는 모든 클래스를 의미합니다. 모든 데이터셋은 겹치지 않기 때문에 (고양이이면서 강아지 일수 없음) 교집합이 없고, 그냥 덧셈으로 풀어서 쓸 수 있게 됩니다. 이를 전확률의 법칙이라고 합니다. 따라서 아래와 같이 식을 변경할 수 있습니다.</p>
<p><br>
<br>

$$P(Dog|Model) = \frac{P(Model|Dog)P(Dog)}{P(Model|Dog)P(Dog) + P(Model|Cat)P(Cat)}$$

<br>
<br></p>
<p>이 것을 함수로 만들고 싶은데, 확률은 0 ~ 1이기 때문에 만약 치역이 음수에서는 정의되지 않는 함수가 됩니다. 또한, Likelihood를 계산하려면 $P(Model|Dog_1) \times P(Model|Dog_2) \times P(Model|Dog_3)$...을 모두 곱해야 하기 때문에 연산량이 큽니다. 이를 Log로 바꾸게 되면 곱셈이 전부 덧셈으로 풀리게 되기 때문에 기능상 매우 편리합니다. 따라서 이들을 로그로 바꿔줍니다.</p>
<p><br>
<br>

$$P(Dog|Model) = \frac{\log P(Model|Dog)P(Dog)}{\log P(Model|Dog)P(Dog) + \log P(Model|Cat)P(Cat)}$$

<br>
<br></p>
<p>식이 너무 복잡하니 $\log P(Model|Dog)P(Dog) = z_{Dog}$으로 치환하고 $\log P(Model|Cat)P(Cat) = z_{Cat}$으로 치환하겠습니다. 그러면 아래와 같이 생각 할 수 있습니다.</p>
<p><br>
<br>

$$\log P(Model|Dog)P(Dog) = z_{Dog}$$


$$P(Model|Dog)P(Dog) = e^{z_{Dog}}$$

<br>

$$\log P(Model|Cat)P(Cat) = z_{Cat}$$


$$P(Model|Cat)P(Cat) = e^{z_{Cat}}$$

<br>
<br></p>
<p>기존의 확률값들을 자연상수 $e$의 지수꼴로 나타낼 수 있게 되었습니다. 따라서 이를 아래처럼 다시 정리합니다.</p>
<p><br>
<br>

$$P(Dog|Model) = \frac{e^{z_{Dog}}}{e^{z_{Dog}} + e^{z_{Cat}}}$$

<br>
<br></p>
<p>이를 시그마로 묶어서 더 깔끔하게 표현할 수 있고, 마찬가지로 고양이에 관한 사후확률도 표현이 가능합니다.</p>
<p><br>
<br>

$$P(Dog|Model) = \frac{e^{z_{Dog}}}{\sum_i e^{z_i}}$$

<br>

$$P(Cat|Model) = \frac{e^{z_{Cat}}}{\sum_i e^{z_i}}$$

<br>
<br></p>
<p>이러한 <strong>함수를 Softmax함수라고 부릅니다. Softmax함수는 보통 여러개의 클래스를 분류할 때 사용됩니다.</strong> 
<br><br></p>
<h4 id="4.2.-Sigmoid-함수">
<a class="anchor" href="#4.2.-Sigmoid-%ED%95%A8%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.2. Sigmoid 함수<a class="anchor-link" href="#4.2.-Sigmoid-%ED%95%A8%EC%88%98"> </a>
</h4>
<p>만약 클래스가 현재 예시처럼 2개 뿐이라면 한번 더 식을 정리 할 수 있습니다. 분모와 분자를 모두 $e^{z_{Dog}}$로 나누면 아래처럼 됩니다.</p>
<p><br>
<br>

$$P(Dog|Model) = \frac{e^{z_{Dog}}}{e^{z_{Dog}} + e^{z_{Cat}}}$$

<br>

$$P(Dog|Model) = \frac{\frac{e^{z_{Dog}}}{e^{z_{Dog}}}}{\frac{e^{z_{Dog}}}{e^{z_{Dog}}} + \frac{e^{z_{Cat}}}{e^{z_{Dog}}}}$$

<br>

$$P(Dog|Model) = \frac{1}{1 + \frac{e^{z_{Cat}}}{e^{z_{Dog}}}}$$

<br>

$$P(Dog|Model) = \frac{1}{1 + e^{z_{Cat} - z_{Dog}}}$$

<br>
<br></p>
<p>그리고 지수에 나온 뺄셈식($z_{Cat} - z_{Dog}$)을 풉니다.</p>
<p><br>
<br>

$$z_{Cat} - z_{Dog} = \log P(Model|Cat)P(Cat) - \log P(Model|Dog)P(Dog) $$

<br>

$$z_{Cat} - z_{Dog} = \log \frac{P(Model|Cat)P(Cat)}{P(Model|Dog)P(Dog)}$$

<br>

$$z_{Cat} - z_{Dog} = - \log \frac{P(Model|Dog)P(Dog)}{P(Model|Cat)P(Cat)}$$

<br>

$$z_{Cat} - z_{Dog} = - \log \frac{\frac{P(Model|Dog)P(Dog)}{P(Model)}}{\frac{P(Model|Cat)P(Cat)}{P(Model)}}$$

<br>

$$z_{Cat} - z_{Dog} = - \log \frac{P(Dog|Model)}{P(Cat|Model)}$$

<br>

$$z_{Cat} - z_{Dog} = - logit$$

<br>
<br></p>
<p>최종적으로 아래와 같은 식이 만들어집니다.</p>
<p><br>
<br>

$$P(Dog|Model) =  \frac{1}{1 + e^{z_{Cat} - z_{Dog}}} = \frac{1}{1 + e^{-logit}}$$

<br>
<br></p>
<p>이러한 <strong>함수를 Sigmoid함수라고 부릅니다. Sigmoid함수는 보통 2개의 클래스를 분류할 때 사용됩니다.</strong> 
<br><br></p>
<h4 id="4.3.-...-어렵나요?-그래도-기억해야할-것!">
<a class="anchor" href="#4.3.-...-%EC%96%B4%EB%A0%B5%EB%82%98%EC%9A%94?-%EA%B7%B8%EB%9E%98%EB%8F%84-%EA%B8%B0%EC%96%B5%ED%95%B4%EC%95%BC%ED%95%A0-%EA%B2%83!" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.3. ... 어렵나요? 그래도 기억해야할 것!<a class="anchor-link" href="#4.3.-...-%EC%96%B4%EB%A0%B5%EB%82%98%EC%9A%94?-%EA%B7%B8%EB%9E%98%EB%8F%84-%EA%B8%B0%EC%96%B5%ED%95%B4%EC%95%BC%ED%95%A0-%EA%B2%83!"> </a>
</h4>
</div>
</div>
</div>
</div>





    <br><br>
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = “https://gusdnd852.github.io/statics_theory“;
this.page.identifier = 03. 통계학 기초;
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://gusdnd852.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<a class="u-url" href="/bigdata-lecture/statics_theory" hidden></a>
</article>

<script>
	function run_exec(){
		if("colab" === "binder"){
			if(confirm("이 노트북은 CPU환경인 Binder에서 소스코드를 실행합니다.")){
				var binder_path = "https://mybinder.org/v2/gh/gusdnd852/bigdata-lecture/master?filepath=_notebooks%2F02_03_%ED%86%B5%EA%B3%84%ED%95%99_%EA%B8%B0%EC%B4%88.ipynb";
				alert("Binder는 Docker기반의 컨테이너를 직접 빌드하기 때문에 시간이 다소 소요됩니다. Build logs의 show버튼을 눌러서 진행상황을 확인하세요");
				window.open(binder_path, "_blank");
							
			}
			
		}else if("colab" === "colab"){
			if(confirm("이 노트북은 GPU환경인 Google Colab에서 소스코드를 실행합니다.")){
				var colab_path = "https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_03_통계학_기초.ipynb";
				alert("GPU 설정을 위해 Colab 상단 메뉴중, 런타임 → 런타임 유형변경에서 GPU를 선택해주세요. (미선택시 CPU에서 실행됨)");
				window.open(colab_path, "_blank");
			}			
		}else{
			alert("해당 실행환경은 구동이 불가능합니다. Binder와 Google Colab 중 한가지를 선택해주세요.");
		}
	}

    function code_listener() {
	var input_area = document.getElementsByClassName("input_area");

	for(var i = 0 ; i < input_area.length; i++){
		input_area[i].onclick = function(){
			run_exec();
    		}
	}

    } 

    code_listener();
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bigdata-lecture/"></data>

  <div class="wrapper">
      <div class="footer-col">
	<footer-elem class=px-2>
		<a href="https://github.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#github"></use>
			</svg>
			<span class="username">Github</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://youtube.com/channel/UCb7DJAuj1LulbY8WEr2EDUw">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#youtube"></use>
			</svg>
			<span class="username">Youtube</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://www.facebook.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#facebook"></use>
			</svg>
			<span class="username">Facebook</span>
		</a>
	</footer-elem>
	<br>
	<div class="copyright">
		© Copyright 2020 Hyunwoong Go. All rights reserved.
	</div>
      </div>
  </div>
</footer>
</body>
</html>
